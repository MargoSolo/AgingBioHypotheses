{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-0nq8pTGOp4B",
        "outputId": "b039cbc7-d8e9-4acb-999a-0af4b1064f94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scispacy in /usr/local/lib/python3.11/dist-packages (0.5.5)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.4.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.2)\n",
            "Collecting spacy\n",
            "  Using cached spacy-3.7.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from scispacy) (1.16.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scispacy) (2.32.3)\n",
            "Requirement already satisfied: conllu in /usr/local/lib/python3.11/dist-packages (from scispacy) (6.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from scispacy) (1.26.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from scispacy) (1.5.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.3 in /usr/local/lib/python3.11/dist-packages (from scispacy) (1.6.1)\n",
            "Requirement already satisfied: pysbd in /usr/local/lib/python3.11/dist-packages (from scispacy) (0.3.4)\n",
            "Requirement already satisfied: nmslib-metabrainz==2.1.3 in /usr/local/lib/python3.11/dist-packages (from scispacy) (2.1.3)\n",
            "Requirement already satisfied: pybind11>=2.2.3 in /usr/local/lib/python3.11/dist-packages (from nmslib-metabrainz==2.1.3->scispacy) (3.0.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from nmslib-metabrainz==2.1.3->scispacy) (5.9.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.10)\n",
            "Collecting thinc<8.3.0,>=8.2.2 (from spacy)\n",
            "  Using cached thinc-8.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.7.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.10.22)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->scispacy) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->scispacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->scispacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->scispacy) (2025.7.14)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20.3->scispacy) (3.6.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (6.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Using cached spacy-3.7.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
            "Using cached thinc-8.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (920 kB)\n",
            "Installing collected packages: thinc, spacy\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.1.12\n",
            "    Uninstalling thinc-8.1.12:\n",
            "      Successfully uninstalled thinc-8.1.12\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.4.4\n",
            "    Uninstalling spacy-3.4.4:\n",
            "      Successfully uninstalled spacy-3.4.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "en-core-sci-lg 0.5.1 requires spacy<3.5.0,>=3.4.1, but you have spacy 3.7.5 which is incompatible.\n",
            "en-ner-bionlp13cg-md 0.5.1 requires spacy<3.5.0,>=3.4.1, but you have spacy 3.7.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed spacy-3.7.5 thinc-8.2.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "spacy",
                  "thinc"
                ]
              },
              "id": "1c2b33b4c57c44ae98d0ac51981072ca"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_lg-0.5.1.tar.gz\n",
            "  Using cached https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_lg-0.5.1.tar.gz (532.3 MB)\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!pip install scispacy spacy pandas transformers\n",
        "!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_lg-0.5.1.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scispacy spacy\n",
        "!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_ner_bionlp13cg_md-0.5.1.tar.gz\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbZXmS3tpvjV",
        "outputId": "1bcf1a0c-64fd-45c4-f19a-548971c97e55"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scispacy in /usr/local/lib/python3.11/dist-packages (0.5.5)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.7.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from scispacy) (1.16.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scispacy) (2.32.3)\n",
            "Requirement already satisfied: conllu in /usr/local/lib/python3.11/dist-packages (from scispacy) (6.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from scispacy) (1.26.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from scispacy) (1.5.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.3 in /usr/local/lib/python3.11/dist-packages (from scispacy) (1.6.1)\n",
            "Requirement already satisfied: pysbd in /usr/local/lib/python3.11/dist-packages (from scispacy) (0.3.4)\n",
            "Requirement already satisfied: nmslib-metabrainz==2.1.3 in /usr/local/lib/python3.11/dist-packages (from scispacy) (2.1.3)\n",
            "Requirement already satisfied: pybind11>=2.2.3 in /usr/local/lib/python3.11/dist-packages (from nmslib-metabrainz==2.1.3->scispacy) (3.0.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from nmslib-metabrainz==2.1.3->scispacy) (5.9.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.7.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.10.22)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.14.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->scispacy) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->scispacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->scispacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->scispacy) (2025.7.14)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20.3->scispacy) (3.6.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (6.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Collecting https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_ner_bionlp13cg_md-0.5.1.tar.gz\n",
            "  Using cached https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_ner_bionlp13cg_md-0.5.1.tar.gz (120.2 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting spacy<3.5.0,>=3.4.1 (from en_ner_bionlp13cg_md==0.5.1)\n",
            "  Using cached spacy-3.4.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (24 kB)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (3.0.10)\n",
            "Collecting thinc<8.2.0,>=8.1.0 (from spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1)\n",
            "  Using cached thinc-8.1.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (0.10.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (2.0.10)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (0.7.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (0.11.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (1.26.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (1.10.22)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (25.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (1.3.0)\n",
            "Requirement already satisfied: pathlib-abc==0.1.1 in /usr/local/lib/python3.11/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (0.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (4.14.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (2025.7.14)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (0.1.5)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.11/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (8.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (1.2.1)\n",
            "Using cached spacy-3.4.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\n",
            "Using cached thinc-8.1.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (917 kB)\n",
            "Installing collected packages: thinc, spacy\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.2.5\n",
            "    Uninstalling thinc-8.2.5:\n",
            "      Successfully uninstalled thinc-8.2.5\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.7.5\n",
            "    Uninstalling spacy-3.7.5:\n",
            "      Successfully uninstalled spacy-3.7.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "scispacy 0.5.5 requires spacy<3.8.0,>=3.7.0, but you have spacy 3.4.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed spacy-3.4.4 thinc-8.1.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "nlp = spacy.load(\"en_ner_bionlp13cg_md\")"
      ],
      "metadata": {
        "id": "yQOvgEJypmIc"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_excel('merged_sentences_with_meta.xlsx', engine='openpyxl')\n",
        "df = df.drop_duplicates(subset='Sentence', keep='first').reset_index(drop=True)\n"
      ],
      "metadata": {
        "id": "zm2DI_UuxGd8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import hashlib\n",
        "import sqlite3\n",
        "import tempfile\n",
        "from collections import defaultdict\n",
        "from numba import jit\n",
        "from multiprocessing import Pool, cpu_count\n",
        "import time\n",
        "import spacy\n",
        "import networkx as nx\n",
        "\n",
        "nlp = spacy.load(\"en_ner_bionlp13cg_md\", disable=[\"parser\", \"tagger\", \"lemmatizer\"])\n",
        "\n",
        "def process_chunk_parallel(args):\n",
        "    sentences, nlp_model_name = args\n",
        "    import spacy\n",
        "    nlp = spacy.load(nlp_model_name, exclude=[\"parser\", \"tagger\", \"lemmatizer\"])\n",
        "\n",
        "    results = []\n",
        "    docs = list(nlp.pipe(sentences, disable=['parser', 'tagger', 'lemmatizer']))\n",
        "    for sent, doc in zip(sentences, docs):\n",
        "        ents = [ent.text.strip().lower() for ent in doc.ents if len(ent.text.strip()) > 2 and not ent.text.strip().isdigit()]\n",
        "        results.append((sent, ents))\n",
        "    return results\n",
        "\n",
        "def parallel_processing_version(df, nlp_model_name='en_ner_bionlp13cg_md', n_processes=None):\n",
        "    if n_processes is None:\n",
        "        n_processes = min(cpu_count(), 4)\n",
        "\n",
        "    sentences = df['Sentence'].dropna().astype(str).str.strip().str.lower().tolist()\n",
        "    chunked = np.array_split(sentences, n_processes)\n",
        "    chunk_args = [(list(chunk), nlp_model_name) for chunk in chunked]\n",
        "\n",
        "    with Pool(n_processes) as pool:\n",
        "        results = pool.map(process_chunk_parallel, chunk_args)\n",
        "\n",
        "    all_results = []\n",
        "    for r in results:\n",
        "        all_results.extend(r)\n",
        "    return all_results\n",
        "\n",
        "@jit(nopython=True)\n",
        "def fast_combinations(n):\n",
        "    return n * (n - 1) // 2\n",
        "\n",
        "def pandas_optimized_version(df):\n",
        "    df_clean = df.dropna(subset=['Sentence']).copy()\n",
        "    df_clean['sentence_clean'] = df_clean['Sentence'].astype(str).str.strip().str.lower()\n",
        "    df_grouped = df_clean.groupby('sentence_clean').first().reset_index()\n",
        "    print(f\"‚úÖ Reduced from {len(df_clean)} to {len(df_grouped)} unique sentences\")\n",
        "    return df_grouped\n",
        "\n",
        "def sqlite_version(df):\n",
        "    db_fd, db_path = tempfile.mkstemp(suffix='.db')\n",
        "    conn = sqlite3.connect(db_path)\n",
        "    df.to_sql('sentences', conn, if_exists='replace', index=False)\n",
        "    conn.execute('CREATE INDEX IF NOT EXISTS idx_sentence ON sentences(Sentence)')\n",
        "\n",
        "    batch_size = 1000\n",
        "    total_rows = conn.execute('SELECT COUNT(*) FROM sentences WHERE Sentence IS NOT NULL').fetchone()[0]\n",
        "\n",
        "    node_data = defaultdict(set)\n",
        "    edge_data = defaultdict(set)\n",
        "\n",
        "    for offset in range(0, total_rows, batch_size):\n",
        "        cursor = conn.execute(\"\"\"\n",
        "            SELECT Sentence, doi, Year, Title, Authors, keywords_from_filename\n",
        "            FROM sentences\n",
        "            WHERE Sentence IS NOT NULL\n",
        "            LIMIT ? OFFSET ?\n",
        "        \"\"\", (batch_size, offset))\n",
        "        batch = cursor.fetchall()\n",
        "\n",
        "\n",
        "    conn.close()\n",
        "    return node_data, edge_data\n",
        "\n",
        "def cached_nlp_processing(sentences, nlp, cache_file='nlp_cache.pkl', disable=['parser', 'tagger', 'lemmatizer']):\n",
        "    disable_str = '_'.join(disable)\n",
        "    hash_key = hashlib.md5((''.join(sentences) + disable_str).encode()).hexdigest()\n",
        "    cache_path = f\"{cache_file}_{hash_key}\"\n",
        "\n",
        "    try:\n",
        "        with open(cache_path, 'rb') as f:\n",
        "            return pickle.load(f)\n",
        "    except FileNotFoundError:\n",
        "        results = list(nlp.pipe(sentences, disable=disable))\n",
        "        with open(cache_path, 'wb') as f:\n",
        "            pickle.dump(results, f)\n",
        "        return results\n",
        "\n",
        "def stanza_alternative(sentences):\n",
        "    import stanza\n",
        "    nlp = stanza.Pipeline('en', processors='tokenize,ner', use_gpu=True)\n",
        "    results = []\n",
        "    for sent in sentences:\n",
        "        results.append(nlp(sent))\n",
        "    return results\n",
        "\n",
        "def ultimate_optimization(df, nlp, use_cache=True, use_parallel=False):\n",
        "    print(\"üöÄ Starting ultimate optimization...\")\n",
        "    df_opt = pandas_optimized_version(df)\n",
        "    sentences = df_opt['sentence_clean'].tolist()\n",
        "\n",
        "    if use_cache:\n",
        "        nlp_results = cached_nlp_processing(sentences, nlp)\n",
        "    elif use_parallel:\n",
        "        nlp_results = parallel_processing_version(df_opt, 'en_ner_bionlp13cg_md')\n",
        "    else:\n",
        "        nlp_results = list(nlp.pipe(sentences, disable=['parser', 'tagger', 'lemmatizer']))\n",
        "\n",
        "    return nlp_results\n",
        "\n",
        "def build_graph_with_metadata(df):\n",
        "    node_meta = defaultdict(lambda: {\n",
        "        'sentences': set(),\n",
        "        'dois': set(),\n",
        "        'years': set(),\n",
        "        'titles': set(),\n",
        "        'authors': set(),\n",
        "        'keywords': set()\n",
        "    })\n",
        "    edge_meta = defaultdict(lambda: {\n",
        "        'sentences': set(),\n",
        "        'dois': set(),\n",
        "        'years': set(),\n",
        "        'titles': set(),\n",
        "        'authors': set(),\n",
        "        'keywords': set()\n",
        "    })\n",
        "\n",
        "    for _, row in df.dropna(subset=['Sentence']).iterrows():\n",
        "        sent = str(row['Sentence']).strip().lower()\n",
        "        doi      = row.get('doi')\n",
        "        year     = row.get('Year')\n",
        "        title    = row.get('Title')\n",
        "        authors  = row.get('Authors')\n",
        "        keywords = row.get('keywords_from_filename')\n",
        "\n",
        "        doc = nlp(sent)\n",
        "        ents = list({ent.text.strip().lower() for ent in doc.ents if len(ent.text.strip()) > 2})\n",
        "        ents = [e for e in ents if e not in nlp.Defaults.stop_words and not e.isdigit()]\n",
        "\n",
        "        for e in ents:\n",
        "            node_meta[e]['sentences'].add(sent)\n",
        "            node_meta[e]['dois'].add(doi)\n",
        "            node_meta[e]['years'].add(year)\n",
        "            node_meta[e]['titles'].add(title)\n",
        "            node_meta[e]['authors'].add(authors)\n",
        "            node_meta[e]['keywords'].add(keywords)\n",
        "\n",
        "        for i in range(len(ents)):\n",
        "            for j in range(i + 1, len(ents)):\n",
        "                u, v = sorted((ents[i], ents[j]))\n",
        "                edge_meta[(u, v)]['sentences'].add(sent)\n",
        "                edge_meta[(u, v)]['dois'].add(doi)\n",
        "                edge_meta[(u, v)]['years'].add(year)\n",
        "                edge_meta[(u, v)]['titles'].add(title)\n",
        "                edge_meta[(u, v)]['authors'].add(authors)\n",
        "                edge_meta[(u, v)]['keywords'].add(keywords)\n",
        "\n",
        "    G = nx.Graph()\n",
        "    for term, meta in node_meta.items():\n",
        "        G.add_node(term, sentences=list(meta['sentences']))\n",
        "\n",
        "    for (u, v), meta in edge_meta.items():\n",
        "        G.add_edge(u, v, weight=len(meta['sentences']), sentences=list(meta['sentences']))\n",
        "\n",
        "    node_full_meta = {\n",
        "        term: {k: list(v) for k, v in meta.items() if k != 'sentences'}\n",
        "        for term, meta in node_meta.items()\n",
        "    }\n",
        "    edge_full_meta = {\n",
        "        (u, v): {k: list(v) for k, v in meta.items() if k != 'sentences'}\n",
        "        for (u, v), meta in edge_meta.items()\n",
        "    }\n",
        "\n",
        "    return G, node_full_meta, edge_full_meta\n",
        "\n",
        "print(\"\"\"\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "w5tspzSekC5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "G, node_full_meta, edge_full_meta = build_graph_with_metadata(df)\n"
      ],
      "metadata": {
        "id": "xbs0tnXgpYil"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# –¢–µ–ø–µ—Ä—å:\n",
        "print(\"–£–∑–ª–æ–≤ –≤ –≥—Ä–∞—Ñ–µ:\", G.number_of_nodes())\n",
        "print(\"–†—ë–±–µ—Ä –≤ –≥—Ä–∞—Ñ–µ:\", G.number_of_edges())\n",
        "\n",
        "# –ü—Ä–∏–º–µ—Ä: –ø–æ–ª—É—á–∏–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ\n",
        "sample_node = list(G.nodes)[0]\n",
        "print(\"–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —É–∑–ª–∞\", sample_node, \":\", G.nodes[sample_node]['sentences'])\n",
        "print(\"–ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —É–∑–ª–∞\", sample_node, \":\", node_full_meta[sample_node])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQWefi1DpY06",
        "outputId": "6dd6fd53-3ca3-4229-c173-010b35b70cf9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "–£–∑–ª–æ–≤ –≤ –≥—Ä–∞—Ñ–µ: 32707\n",
            "–†—ë–±–µ—Ä –≤ –≥—Ä–∞—Ñ–µ: 775715\n",
            "–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —É–∑–ª–∞ human myoblast : ['it has been observed in human myoblast that socs3 overexpression resulted in an increased expression of genes associated with skeletal muscle growth, and socs3 signaling during aging is dysregulated [13].', 'increased dna demethylation was also observed during myogenic differentiation of human myoblast obtained from muscle biopsies, which was linked to increased tet1-2 mrna and 5hmc levels [55].']\n",
            "–ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —É–∑–ª–∞ human myoblast : {'dois': ['https://doi.org/10.1186/s13072-025-00601-w', 'https://doi.org/10.3390/ijms241713181'], 'years': [2025, 2023], 'titles': ['Cell identity and 5-hydroxymethylcytosine', 'Effects of Tofacitinib on Muscle Remodeling in Experimental Rheumatoid Sarcopenia'], 'authors': ['Ismael Bermejo-√Ålvarez, Sandra P√©rez-Baos, Paula Gratal, Juan Pablo Medina, Raquel Largo, Gabriel Herrero-Beaumont, Ar√°nzazu Mediero, ', 'Floris Honig, Adele Murrell, '], 'keywords': ['5hmC, human', 'aging, genes, human']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import networkx as nx\n",
        "\n",
        "\n",
        "with open('my_graph_bundle.pkl', 'wb') as f:\n",
        "    pickle.dump((G, node_full_meta, edge_full_meta), f)"
      ],
      "metadata": {
        "id": "28zMeK7-i7XA"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === –ü–û–õ–ù–´–ô –†–ê–ë–û–ß–ò–ô –ü–†–ò–ú–ï–† ===\n",
        "# –ö–æ–ø–∏—Ä—É–π—Ç–µ —ç—Ç–æ—Ç –∫–æ–¥ –∏ –∑–∞–ø—É—Å–∫–∞–π—Ç–µ –ø–æ—Å–ª–µ —Å–æ–∑–¥–∞–Ω–∏—è –≥—Ä–∞—Ñ–∞\n",
        "\n",
        "import networkx as nx\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import defaultdict, Counter\n",
        "import json\n",
        "import time\n",
        "from openai import OpenAI\n",
        "import difflib\n",
        "\n",
        "# === –ö–û–î –ö–õ–ê–°–°–ê –ì–ï–ù–ï–†–ê–¢–û–†–ê (–≤—Å—Ç–∞–≤—å—Ç–µ —Å—é–¥–∞ –∫–æ–¥ –∏–∑ –ø–µ—Ä–≤–æ–≥–æ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–∞) ===\n",
        "# [–ó–¥–µ—Å—å –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –∫–æ–¥ –∫–ª–∞—Å—Å–∞ AgingHypothesisGenerator]\n",
        "\n",
        "# === –ù–ê–°–¢–†–û–ô–ö–ê –ò –ó–ê–ü–£–°–ö ===\n",
        "\n",
        "def setup_and_run():\n",
        "    \"\"\"–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∏ –∑–∞–ø—É—Å–∫ —Å–∏—Å—Ç–µ–º—ã\"\"\"\n",
        "\n",
        "    print(\"üöÄ –ó–ê–ü–£–°–ö –ì–ï–ù–ï–†–ê–¢–û–†–ê –ì–ò–ü–û–¢–ï–ó –î–õ–Ø –ë–ò–û–õ–û–ì–ò–ò –°–¢–ê–†–ï–ù–ò–Ø\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö\n",
        "    try:\n",
        "        print(f\"üìä –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞–Ω–Ω—ã—Ö:\")\n",
        "        print(f\"  ‚Ä¢ –ì—Ä–∞—Ñ G: {G.number_of_nodes():,} —É–∑–ª–æ–≤, {G.number_of_edges():,} —Ä—ë–±–µ—Ä\")\n",
        "        print(f\"  ‚Ä¢ –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —É–∑–ª–æ–≤: {len(node_full_meta):,} –∑–∞–ø–∏—Å–µ–π\")\n",
        "        print(f\"  ‚Ä¢ –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Ä—ë–±–µ—Ä: {len(edge_full_meta):,} –∑–∞–ø–∏—Å–µ–π\")\n",
        "    except NameError as e:\n",
        "        print(f\"‚ùå –û—à–∏–±–∫–∞: –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ {e}\")\n",
        "        print(\"–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –≤—ã –≤—ã–ø–æ–ª–Ω–∏–ª–∏:\")\n",
        "        print(\"G, node_full_meta, edge_full_meta = build_graph_with_metadata(df)\")\n",
        "        return None\n",
        "\n",
        "    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç–∞ OpenAI\n",
        "    print(\"\\nü§ñ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LLM –∫–ª–∏–µ–Ω—Ç–∞...\")\n",
        "    client = OpenAI(\n",
        "        base_url=\"http://80.209.242.40:8000/v1\",\n",
        "        api_key=\"dummy-key\"\n",
        "    )\n",
        "\n",
        "    # –°–æ–∑–¥–∞–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞\n",
        "    print(\"‚öôÔ∏è –°–æ–∑–¥–∞–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ –≥–∏–ø–æ—Ç–µ–∑...\")\n",
        "    generator = AgingHypothesisGenerator(G, node_full_meta, edge_full_meta, client)\n",
        "\n",
        "    print(\"‚úÖ –°–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞ –∫ —Ä–∞–±–æ—Ç–µ!\\n\")\n",
        "\n",
        "    return generator\n",
        "\n",
        "# === –ë–´–°–¢–†–´–ô –¢–ï–°–¢ ===\n",
        "\n",
        "def quick_test(generator, test_term=\"telomere\"):\n",
        "    \"\"\"–ë—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç —Å–∏—Å—Ç–µ–º—ã\"\"\"\n",
        "    print(f\"üß™ –ë–´–°–¢–†–´–ô –¢–ï–°–¢ —Å —Ç–µ—Ä–º–∏–Ω–æ–º '{test_term}'\")\n",
        "    print(\"-\"*40)\n",
        "\n",
        "    try:\n",
        "        result = generator.analyze_term(test_term, radius=1)  # –£–º–µ–Ω—å—à–µ–Ω–Ω—ã–π —Ä–∞–¥–∏—É—Å –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏\n",
        "        print(\"‚úÖ –¢–µ—Å—Ç –ø—Ä–æ–π–¥–µ–Ω —É—Å–ø–µ—à–Ω–æ!\")\n",
        "        return result\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå –û—à–∏–±–∫–∞ –≤ —Ç–µ—Å—Ç–µ: {e}\")\n",
        "        return None\n",
        "\n",
        "# === –û–°–ù–û–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø ===\n",
        "\n",
        "def main():\n",
        "    \"\"\"–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è\"\"\"\n",
        "\n",
        "    # 1. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å–∏—Å—Ç–µ–º—ã\n",
        "    generator = setup_and_run()\n",
        "    if not generator:\n",
        "        return\n",
        "\n",
        "    # 2. –ë—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    test_result = quick_test(generator)\n",
        "\n",
        "    if not test_result:\n",
        "        print(\"‚ùå –¢–µ—Å—Ç –Ω–µ –ø—Ä–æ–π–¥–µ–Ω. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏.\")\n",
        "        return\n",
        "\n",
        "    # 3. –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üéØ –ü–ï–†–ï–•–û–î –í –ò–ù–¢–ï–†–ê–ö–¢–ò–í–ù–´–ô –†–ï–ñ–ò–ú\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"–í–≤–µ–¥–∏—Ç–µ —Ç–µ—Ä–º–∏–Ω—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ (–∏–ª–∏ 'quit' –¥–ª—è –≤—ã—Ö–æ–¥–∞):\")\n",
        "    print(\"–ü—Ä–∏–º–µ—Ä—ã: autophagy, mitochondria, senescence, inflammation\")\n",
        "    print(\"-\"*60)\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            search_term = input(\"\\nüîç –í–≤–µ–¥–∏—Ç–µ —Ç–µ—Ä–º–∏–Ω: \").strip()\n",
        "\n",
        "            if search_term.lower() in ['quit', 'exit', 'q', '–≤—ã—Ö–æ–¥']:\n",
        "                print(\"üëã –†–∞–±–æ—Ç–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!\")\n",
        "                break\n",
        "\n",
        "            if not search_term:\n",
        "                continue\n",
        "\n",
        "            # –ê–Ω–∞–ª–∏–∑ —Ç–µ—Ä–º–∏–Ω–∞\n",
        "            print(f\"\\n{'='*80}\")\n",
        "            result = generator.analyze_term(search_term, radius=2)\n",
        "\n",
        "            # –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ —Å–ª–µ–¥—É—é—â–∏—Ö –¥–µ–π—Å—Ç–≤–∏–π\n",
        "            print(f\"\\nüéØ –ß—Ç–æ –¥–µ–ª–∞–µ–º –¥–∞–ª—å—à–µ?\")\n",
        "            print(\"1. –í–≤–µ—Å—Ç–∏ –Ω–æ–≤—ã–π —Ç–µ—Ä–º–∏–Ω\")\n",
        "            print(\"2. –í—ã–π—Ç–∏ (quit)\")\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\n‚èπÔ∏è –ü—Ä–µ—Ä—ã–≤–∞–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå –û—à–∏–±–∫–∞: {e}\")\n",
        "            print(\"–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –¥—Ä—É–≥–æ–π —Ç–µ—Ä–º–∏–Ω –∏–ª–∏ –ø—Ä–æ–≤–µ—Ä—å—Ç–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å LLM\")\n",
        "            continue\n",
        "\n",
        "# === –£–¢–ò–õ–ò–¢–´ –î–õ–Ø –û–¢–õ–ê–î–ö–ò ===\n",
        "\n",
        "def debug_graph():\n",
        "    \"\"\"–û—Ç–ª–∞–¥–æ—á–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –≥—Ä–∞—Ñ–µ\"\"\"\n",
        "    print(\"üîç –û–¢–õ–ê–î–û–ß–ù–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø:\")\n",
        "    print(f\"  ‚Ä¢ –¢–∏–ø –≥—Ä–∞—Ñ–∞: {type(G)}\")\n",
        "    print(f\"  ‚Ä¢ –£–∑–ª–æ–≤: {G.number_of_nodes()}\")\n",
        "    print(f\"  ‚Ä¢ –†—ë–±–µ—Ä: {G.number_of_edges()}\")\n",
        "\n",
        "    # –ü—Ä–∏–º–µ—Ä—ã —É–∑–ª–æ–≤\n",
        "    sample_nodes = list(G.nodes())[:5]\n",
        "    print(f\"  ‚Ä¢ –ü—Ä–∏–º–µ—Ä—ã —É–∑–ª–æ–≤: {sample_nodes}\")\n",
        "\n",
        "    # –ü—Ä–∏–º–µ—Ä—ã —Ä—ë–±–µ—Ä\n",
        "    sample_edges = list(G.edges())[:3]\n",
        "    print(f\"  ‚Ä¢ –ü—Ä–∏–º–µ—Ä—ã —Ä—ë–±–µ—Ä: {sample_edges}\")\n",
        "\n",
        "    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö\n",
        "    if node_full_meta:\n",
        "        sample_node = sample_nodes[0]\n",
        "        if sample_node in node_full_meta:\n",
        "            print(f\"  ‚Ä¢ –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ '{sample_node}': {list(node_full_meta[sample_node].keys())}\")\n",
        "\n",
        "def find_terms_containing(substring):\n",
        "    \"\"\"–ü–æ–∏—Å–∫ —Ç–µ—Ä–º–∏–Ω–æ–≤, —Å–æ–¥–µ—Ä–∂–∞—â–∏—Ö –ø–æ–¥—Å—Ç—Ä–æ–∫—É\"\"\"\n",
        "    substring = substring.lower()\n",
        "    matching = [node for node in G.nodes() if substring in node.lower()]\n",
        "    print(f\"üîç –ù–∞–π–¥–µ–Ω–æ {len(matching)} —Ç–µ—Ä–º–∏–Ω–æ–≤ —Å '{substring}':\")\n",
        "    for term in sorted(matching)[:10]:\n",
        "        print(f\"  ‚Ä¢ {term}\")\n",
        "    return matching\n",
        "\n",
        "# === –í–ê–†–ò–ê–ù–¢–´ –ó–ê–ü–£–°–ö–ê ===\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # –ü—Ä–æ–≤–µ—Ä—å—Ç–µ, —á—Ç–æ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç\n",
        "    try:\n",
        "        print(f\"–ì—Ä–∞—Ñ –≥–æ—Ç–æ–≤: {G.number_of_nodes()} —É–∑–ª–æ–≤\")\n",
        "        main()\n",
        "    except NameError:\n",
        "        print(\"‚ùå –°–Ω–∞—á–∞–ª–∞ —Å–æ–∑–¥–∞–π—Ç–µ –≥—Ä–∞—Ñ:\")\n",
        "        print(\"G, node_full_meta, edge_full_meta = build_graph_with_metadata(df)\")\n",
        "\n",
        "# === –ê–õ–¨–¢–ï–†–ù–ê–¢–ò–í–ù–´–ï –°–ü–û–°–û–ë–´ –ó–ê–ü–£–°–ö–ê ===\n",
        "\n",
        "# –î–ª—è –æ—Ç–ª–∞–¥–∫–∏:\n",
        "# debug_graph()\n",
        "\n",
        "# –î–ª—è –ø–æ–∏—Å–∫–∞ —Ç–µ—Ä–º–∏–Ω–æ–≤:\n",
        "# find_terms_containing(\"aging\")\n",
        "# find_terms_containing(\"cell\")\n",
        "\n",
        "# –î–ª—è –ø—Ä—è–º–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –±–µ–∑ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞:\n",
        "# generator = setup_and_run()\n",
        "# if generator:\n",
        "#     result = generator.analyze_term(\"autophagy\")\n",
        "\n",
        "# === –ü–†–ò–ú–ï–† –ü–û–õ–ù–û–ì–û –†–ê–ë–û–ß–ï–ì–û –ö–û–î–ê ===\n",
        "\"\"\"\n",
        "# 1. –ü–æ—Å–ª–µ —Å–æ–∑–¥–∞–Ω–∏—è –≥—Ä–∞—Ñ–∞ –≤—ã–ø–æ–ª–Ω–∏—Ç–µ:\n",
        "generator = setup_and_run()\n",
        "\n",
        "# 2. –î–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ç–µ—Å—Ç–∞:\n",
        "result = generator.analyze_term(\"telomere\")\n",
        "\n",
        "# 3. –î–ª—è –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞:\n",
        "main()\n",
        "\n",
        "# 4. –î–ª—è –ø–∞–∫–µ—Ç–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞:\n",
        "terms = [\"autophagy\", \"mitochondria\", \"senescence\"]\n",
        "for term in terms:\n",
        "    print(f\"\\\\n{'='*60}\")\n",
        "    generator.analyze_term(term)\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "id": "CemEGos45c4S",
        "outputId": "0eacce87-7aee-4f6f-ee8d-7d20b9bc8038"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "–ì—Ä–∞—Ñ –≥–æ—Ç–æ–≤: 32707 —É–∑–ª–æ–≤\n",
            "üöÄ –ó–ê–ü–£–°–ö –ì–ï–ù–ï–†–ê–¢–û–†–ê –ì–ò–ü–û–¢–ï–ó –î–õ–Ø –ë–ò–û–õ–û–ì–ò–ò –°–¢–ê–†–ï–ù–ò–Ø\n",
            "============================================================\n",
            "üìä –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞–Ω–Ω—ã—Ö:\n",
            "  ‚Ä¢ –ì—Ä–∞—Ñ G: 32,707 —É–∑–ª–æ–≤, 775,715 —Ä—ë–±–µ—Ä\n",
            "  ‚Ä¢ –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —É–∑–ª–æ–≤: 32,707 –∑–∞–ø–∏—Å–µ–π\n",
            "  ‚Ä¢ –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Ä—ë–±–µ—Ä: 775,715 –∑–∞–ø–∏—Å–µ–π\n",
            "\n",
            "ü§ñ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LLM –∫–ª–∏–µ–Ω—Ç–∞...\n",
            "‚öôÔ∏è –°–æ–∑–¥–∞–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ –≥–∏–ø–æ—Ç–µ–∑...\n",
            "‚ùå –°–Ω–∞—á–∞–ª–∞ —Å–æ–∑–¥–∞–π—Ç–µ –≥—Ä–∞—Ñ:\n",
            "G, node_full_meta, edge_full_meta = build_graph_with_metadata(df)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# 1. –ü–æ—Å–ª–µ —Å–æ–∑–¥–∞–Ω–∏—è –≥—Ä–∞—Ñ–∞ –≤—ã–ø–æ–ª–Ω–∏—Ç–µ:\\ngenerator = setup_and_run()\\n\\n# 2. –î–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ç–µ—Å—Ç–∞:\\nresult = generator.analyze_term(\"telomere\")\\n\\n# 3. –î–ª—è –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞:\\nmain()\\n\\n# 4. –î–ª—è –ø–∞–∫–µ—Ç–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞:\\nterms = [\"autophagy\", \"mitochondria\", \"senescence\"]\\nfor term in terms:\\n    print(f\"\\\\n{\\'=\\'*60}\")\\n    generator.analyze_term(term)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "find_terms_containing(\"aging\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRTr7ski5dBC",
        "outputId": "b52b4620-405e-4c0c-f053-2a8bb3c566ec"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç –ù–∞–π–¥–µ–Ω–æ 25 —Ç–µ—Ä–º–∏–Ω–æ–≤ —Å 'aging':\n",
            "  ‚Ä¢ aging-associated protein aggregates\n",
            "  ‚Ä¢ aging-associated proteins\n",
            "  ‚Ä¢ aging-relevant\n",
            "  ‚Ä¢ aging-specific\n",
            "  ‚Ä¢ aging.12\n",
            "  ‚Ä¢ aging.7 leukocyte telomere\n",
            "  ‚Ä¢ aging.[6\n",
            "\n",
            "]\n",
            "  ‚Ä¢ aging.cellular\n",
            "  ‚Ä¢ aging.many factors\n",
            "  ‚Ä¢ aging17\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['protein-bound cml levels[168]>aging',\n",
              " 's-klotho anti-aging protein',\n",
              " 'aging-specific',\n",
              " 'pro-aging factors',\n",
              " 'human aging-like',\n",
              " 'human aging-relevant',\n",
              " 'aging-relevant',\n",
              " 'murine aging tissues [',\n",
              " 'pro-aging genes',\n",
              " 'aging.cellular',\n",
              " 'aging.7 leukocyte telomere',\n",
              " 'lung aging.acell',\n",
              " 'lentiviral libraries/sequencing/imaging[59‚Äì69]pdx‚Ä¢',\n",
              " 'aging.many factors',\n",
              " 'immune system aging',\n",
              " 'human aging.skin',\n",
              " 'skin aging.12',\n",
              " 'vitamin d anti-aging',\n",
              " 'aging-associated protein aggregates',\n",
              " 'aging-associated proteins',\n",
              " 'aging.[6\\n\\n]',\n",
              " 'photoaging.iiihuman tissue-resident stem cells',\n",
              " 'aging17',\n",
              " 'aging.12',\n",
              " 'patients2022 [38]agingimmune cellsdural']"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import defaultdict, Counter\n",
        "import json\n",
        "import time\n",
        "from openai import OpenAI\n",
        "import difflib\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from scipy import stats\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class ScientificHypothesisGenerator:\n",
        "    def __init__(self, G, node_full_meta, edge_full_meta, client):\n",
        "        \"\"\"\n",
        "        –ù–∞—É—á–Ω–æ —Å—Ç—Ä–æ–≥–∏–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –≥–∏–ø–æ—Ç–µ–∑ —Å —á–µ—Ç–∫–∏–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏\n",
        "        \"\"\"\n",
        "        self.G = G\n",
        "        self.node_full_meta = node_full_meta\n",
        "        self.edge_full_meta = edge_full_meta\n",
        "        self.client = client\n",
        "\n",
        "        # –ü–æ—Ä–æ–≥–∏ –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤\n",
        "        self.quality_thresholds = {\n",
        "            'min_degree': 3,           # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Å—Ç–µ–ø–µ–Ω—å —É–∑–ª–∞\n",
        "            'min_pagerank': 0.0001,    # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π PageRank\n",
        "            'min_betweenness': 0.001,  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è betweenness centrality\n",
        "            'min_metadata_score': 1,   # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –±–∞–ª–ª –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö\n",
        "            'max_term_length': 50,     # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Ç–µ—Ä–º–∏–Ω–∞\n",
        "            'min_term_length': 3       # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Ç–µ—Ä–º–∏–Ω–∞\n",
        "        }\n",
        "\n",
        "        # –ë–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Å –≤–µ—Å–∞–º–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏\n",
        "        self.biology_categories = {\n",
        "            'aging_processes': {\n",
        "                'keywords': ['senescence', 'aging', 'longevity', 'lifespan', 'telomere', 'oxidative'],\n",
        "                'weight': 3.0\n",
        "            },\n",
        "            'molecular_mechanisms': {\n",
        "                'keywords': ['autophagy', 'apoptosis', 'dna repair', 'protein', 'enzyme', 'signaling'],\n",
        "                'weight': 2.5\n",
        "            },\n",
        "            'cellular_structures': {\n",
        "                'keywords': ['mitochondria', 'nucleus', 'membrane', 'ribosome', 'endoplasmic'],\n",
        "                'weight': 2.0\n",
        "            },\n",
        "            'disease_pathways': {\n",
        "                'keywords': ['inflammation', 'cancer', 'diabetes', 'neurodegeneration', 'alzheimer'],\n",
        "                'weight': 2.0\n",
        "            },\n",
        "            'systems_biology': {\n",
        "                'keywords': ['immune', 'cardiovascular', 'neural', 'metabolic', 'endocrine'],\n",
        "                'weight': 1.5\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # –ò—Å–∫–ª—é—á–∞–µ–º—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã\n",
        "        self.exclude_patterns = [\n",
        "            r'^\\d+$', r'aging\\d+', r'figure\\s*\\d+', r'table\\s*\\d+',\n",
        "            r'supplementary', r'dataset', r'filename', r'^\\w{1,2}$',\n",
        "            r'mouse\\s+\\w+\\s+muscles?', r'rat\\s+\\w+', r'sample\\s*\\d+'\n",
        "        ]\n",
        "\n",
        "    def calculate_term_quality_score(self, term, subgraph):\n",
        "        \"\"\"\n",
        "        –†–∞—Å—á–µ—Ç –Ω–∞—É—á–Ω–æ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω–æ–≥–æ –±–∞–ª–ª–∞ –∫–∞—á–µ—Å—Ç–≤–∞ —Ç–µ—Ä–º–∏–Ω–∞\n",
        "\n",
        "        –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã:\n",
        "        1. Degree centrality (0-2 –±–∞–ª–ª–∞)\n",
        "        2. PageRank (0-2 –±–∞–ª–ª–∞)\n",
        "        3. Betweenness centrality (0-2 –±–∞–ª–ª–∞)\n",
        "        4. –ë–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–∞—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å (0-3 –±–∞–ª–ª–∞)\n",
        "        5. –ö–∞—á–µ—Å—Ç–≤–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö (0-1 –±–∞–ª–ª)\n",
        "\n",
        "        –ú–∞–∫—Å–∏–º—É–º: 10 –±–∞–ª–ª–æ–≤\n",
        "        \"\"\"\n",
        "        score = 0\n",
        "        details = {}\n",
        "\n",
        "        # 1. Degree centrality\n",
        "        degree = subgraph.degree(term)\n",
        "        degree_norm = min(degree / 10, 1.0)  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–æ 1\n",
        "        degree_score = degree_norm * 2\n",
        "        score += degree_score\n",
        "        details['degree'] = {'value': degree, 'score': degree_score}\n",
        "\n",
        "        # 2. PageRank\n",
        "        try:\n",
        "            pagerank = nx.pagerank(subgraph)\n",
        "            pr_value = pagerank.get(term, 0)\n",
        "            pr_norm = min(pr_value / 0.01, 1.0)  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è\n",
        "            pr_score = pr_norm * 2\n",
        "            score += pr_score\n",
        "            details['pagerank'] = {'value': pr_value, 'score': pr_score}\n",
        "        except:\n",
        "            details['pagerank'] = {'value': 0, 'score': 0}\n",
        "\n",
        "        # 3. Betweenness centrality\n",
        "        try:\n",
        "            betw_cent = nx.betweenness_centrality(subgraph)\n",
        "            betw_value = betw_cent.get(term, 0)\n",
        "            betw_norm = min(betw_value / 0.1, 1.0)  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è\n",
        "            betw_score = betw_norm * 2\n",
        "            score += betw_score\n",
        "            details['betweenness'] = {'value': betw_value, 'score': betw_score}\n",
        "        except:\n",
        "            details['betweenness'] = {'value': 0, 'score': 0}\n",
        "\n",
        "        # 4. –ë–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–∞—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å\n",
        "        bio_score = self._calculate_biological_relevance(term)\n",
        "        score += bio_score\n",
        "        details['biological'] = {'score': bio_score}\n",
        "\n",
        "        # 5. –ö–∞—á–µ—Å—Ç–≤–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö\n",
        "        meta_score = self._assess_metadata_quality(term)\n",
        "        score += meta_score\n",
        "        details['metadata'] = {'score': meta_score}\n",
        "\n",
        "        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–º –∫—Ä–∏—Ç–µ—Ä–∏—è–º\n",
        "        if self._is_technical_term(term):\n",
        "            score *= 0.1  # –ñ–µ—Å—Ç–∫–∏–π —à—Ç—Ä–∞—Ñ –∑–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã\n",
        "            details['technical_penalty'] = True\n",
        "\n",
        "        return min(score, 10), details\n",
        "\n",
        "    def _calculate_biological_relevance(self, term):\n",
        "        \"\"\"–†–∞—Å—á–µ—Ç –±–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ (0-3 –±–∞–ª–ª–∞)\"\"\"\n",
        "        term_lower = term.lower()\n",
        "        max_score = 0\n",
        "\n",
        "        for category, data in self.biology_categories.items():\n",
        "            for keyword in data['keywords']:\n",
        "                if keyword in term_lower:\n",
        "                    category_score = data['weight']\n",
        "                    max_score = max(max_score, category_score)\n",
        "\n",
        "        return min(max_score, 3.0)\n",
        "\n",
        "    def _is_technical_term(self, term):\n",
        "        \"\"\"–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π —Ç–µ—Ä–º–∏–Ω\"\"\"\n",
        "        for pattern in self.exclude_patterns:\n",
        "            if re.search(pattern, term.lower()):\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    def _assess_metadata_quality(self, term):\n",
        "        \"\"\"–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö (0-1 –±–∞–ª–ª)\"\"\"\n",
        "        if term not in self.node_full_meta:\n",
        "            return 0\n",
        "\n",
        "        meta = self.node_full_meta[term]\n",
        "        score = 0\n",
        "\n",
        "        # DOI\n",
        "        dois = meta.get('dois', [])\n",
        "        valid_dois = [doi for doi in dois if doi and str(doi) != 'nan' and 'doi.org' in str(doi)]\n",
        "        if valid_dois:\n",
        "            score += 0.4\n",
        "\n",
        "        # –ì–æ–¥—ã\n",
        "        years = meta.get('years', [])\n",
        "        valid_years = [y for y in years if y and str(y) != 'nan' and 1990 <= int(str(y)[:4]) <= 2025]\n",
        "        if valid_years:\n",
        "            score += 0.3\n",
        "\n",
        "        # –ê–≤—Ç–æ—Ä—ã\n",
        "        authors = meta.get('authors', [])\n",
        "        valid_authors = [a for a in authors if a and str(a) != 'nan' and len(str(a)) > 10]\n",
        "        if valid_authors:\n",
        "            score += 0.3\n",
        "\n",
        "        return min(score, 1.0)\n",
        "\n",
        "    def select_quality_terms(self, subgraph, search_term, top_n=15):\n",
        "        \"\"\"\n",
        "        –û—Ç–±–æ—Ä –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤ —Å —á–µ—Ç–∫–∏–º–∏ –∫—Ä–∏—Ç–µ—Ä–∏—è–º–∏\n",
        "        \"\"\"\n",
        "        print(f\"üî¨ –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ {subgraph.number_of_nodes()} —Ç–µ—Ä–º–∏–Ω–æ–≤...\")\n",
        "\n",
        "        candidates = []\n",
        "        quality_details = {}\n",
        "\n",
        "        for term in subgraph.nodes():\n",
        "            if term == search_term:\n",
        "                continue\n",
        "\n",
        "            # –ë–∞–∑–æ–≤—ã–µ —Ñ–∏–ª—å—Ç—Ä—ã\n",
        "            if (len(term) < self.quality_thresholds['min_term_length'] or\n",
        "                len(term) > self.quality_thresholds['max_term_length']):\n",
        "                continue\n",
        "\n",
        "            if subgraph.degree(term) < self.quality_thresholds['min_degree']:\n",
        "                continue\n",
        "\n",
        "            # –†–∞—Å—á–µ—Ç –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –±–∞–ª–ª–∞ –∫–∞—á–µ—Å—Ç–≤–∞\n",
        "            quality_score, details = self.calculate_term_quality_score(term, subgraph)\n",
        "            quality_details[term] = details\n",
        "\n",
        "            if quality_score >= 3.0:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ –∫–∞—á–µ—Å—Ç–≤–∞\n",
        "                candidates.append((term, quality_score))\n",
        "\n",
        "        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –∫–∞—á–µ—Å—Ç–≤—É\n",
        "        candidates.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        print(f\"‚úÖ –û—Ç–æ–±—Ä–∞–Ω–æ {len(candidates)} –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤\")\n",
        "        print(f\"üìä –ü–æ—Ä–æ–≥–∏: —Å—Ç–µ–ø–µ–Ω—å‚â•{self.quality_thresholds['min_degree']}, –∫–∞—á–µ—Å—Ç–≤–æ‚â•3.0\")\n",
        "\n",
        "        selected_terms = [term for term, score in candidates[:top_n]]\n",
        "\n",
        "        # –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n",
        "        if candidates:\n",
        "            scores = [score for _, score in candidates]\n",
        "            print(f\"üìà –î–∏–∞–ø–∞–∑–æ–Ω –∫–∞—á–µ—Å—Ç–≤–∞: {min(scores):.1f} - {max(scores):.1f}\")\n",
        "            print(f\"üéØ –¢–æ–ø-5 —Ç–µ—Ä–º–∏–Ω–æ–≤: {selected_terms[:5]}\")\n",
        "\n",
        "        return selected_terms, quality_details\n",
        "\n",
        "    def analyze_subgraph_structure(self, subgraph, search_term):\n",
        "        \"\"\"\n",
        "        –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø–æ–¥–≥—Ä–∞—Ñ–∞\n",
        "        \"\"\"\n",
        "        print(f\"üèóÔ∏è –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø–æ–¥–≥—Ä–∞—Ñ–∞...\")\n",
        "\n",
        "        stats = {\n",
        "            'basic': {\n",
        "                'nodes': subgraph.number_of_nodes(),\n",
        "                'edges': subgraph.number_of_edges(),\n",
        "                'density': nx.density(subgraph)\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—Ç–µ–ø–µ–Ω–µ–π\n",
        "        degrees = [subgraph.degree(n) for n in subgraph.nodes()]\n",
        "        stats['degree_distribution'] = {\n",
        "            'mean': np.mean(degrees),\n",
        "            'std': np.std(degrees),\n",
        "            'min': min(degrees),\n",
        "            'max': max(degrees),\n",
        "            'median': np.median(degrees)\n",
        "        }\n",
        "\n",
        "        # –¶–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã–µ —É–∑–ª—ã (hubs)\n",
        "        degree_cent = nx.degree_centrality(subgraph)\n",
        "        top_hubs = sorted(degree_cent.items(), key=lambda x: x[1], reverse=True)[:5]\n",
        "        stats['hubs'] = [(node, cent, subgraph.degree(node)) for node, cent in top_hubs]\n",
        "\n",
        "        # –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —Å–≤—è–∑–Ω–æ—Å—Ç–∏\n",
        "        if not nx.is_connected(subgraph):\n",
        "            components = list(nx.connected_components(subgraph))\n",
        "            stats['connectivity'] = {\n",
        "                'connected': False,\n",
        "                'components': len(components),\n",
        "                'largest_component': max(len(c) for c in components)\n",
        "            }\n",
        "        else:\n",
        "            stats['connectivity'] = {\n",
        "                'connected': True,\n",
        "                'diameter': nx.diameter(subgraph),\n",
        "                'average_path_length': nx.average_shortest_path_length(subgraph)\n",
        "            }\n",
        "\n",
        "        # –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è\n",
        "        stats['clustering'] = {\n",
        "            'global': nx.average_clustering(subgraph),\n",
        "            'transitivity': nx.transitivity(subgraph)\n",
        "        }\n",
        "\n",
        "        return stats\n",
        "\n",
        "    def check_hypothesis_similarity(self, hypotheses):\n",
        "        \"\"\"\n",
        "        –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ö–æ–¥—Å—Ç–≤–∞ –º–µ–∂–¥—É –≥–∏–ø–æ—Ç–µ–∑–∞–º–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º TF-IDF\n",
        "        \"\"\"\n",
        "        if not hypotheses or len(hypotheses) < 2:\n",
        "            return []\n",
        "\n",
        "        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç—ã –≥–∏–ø–æ—Ç–µ–∑\n",
        "        texts = []\n",
        "        for hyp in hypotheses:\n",
        "            combined_text = f\"{hyp.get('hypothesis', '')} {hyp.get('rationale', '')}\"\n",
        "            texts.append(combined_text)\n",
        "\n",
        "        # –í—ã—á–∏—Å–ª—è–µ–º TF-IDF\n",
        "        vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
        "        try:\n",
        "            tfidf_matrix = vectorizer.fit_transform(texts)\n",
        "            similarity_matrix = cosine_similarity(tfidf_matrix)\n",
        "\n",
        "            # –ù–∞—Ö–æ–¥–∏–º —Å—Ö–æ–∂–∏–µ –ø–∞—Ä—ã\n",
        "            similar_pairs = []\n",
        "            for i in range(len(hypotheses)):\n",
        "                for j in range(i + 1, len(hypotheses)):\n",
        "                    similarity = similarity_matrix[i][j]\n",
        "                    if similarity > 0.5:  # –ü–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏\n",
        "                        similar_pairs.append({\n",
        "                            'hypothesis_1': i + 1,\n",
        "                            'hypothesis_2': j + 1,\n",
        "                            'similarity': similarity,\n",
        "                            'terms_1': hypotheses[i].get('terms', []),\n",
        "                            'terms_2': hypotheses[j].get('terms', [])\n",
        "                        })\n",
        "\n",
        "            return similar_pairs\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ —Å—Ö–æ–∂–µ—Å—Ç–∏: {e}\")\n",
        "            return []\n",
        "\n",
        "    def evaluate_hypothesis_quality(self, hypotheses_data):\n",
        "        \"\"\"\n",
        "        –ú–Ω–æ–≥–æ—Ñ–∞–∫—Ç–æ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –≥–∏–ø–æ—Ç–µ–∑ (0-5 –ø–æ –∫–∞–∂–¥–æ–º—É –∫—Ä–∏—Ç–µ—Ä–∏—é)\n",
        "\n",
        "        –ö—Ä–∏—Ç–µ—Ä–∏–∏:\n",
        "        1. –ù–æ–≤–∏–∑–Ω–∞ (0-5): –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ—Å—Ç—å –º–µ—Ö–∞–Ω–∏–∑–º–æ–≤\n",
        "        2. –ù–∞—É—á–Ω–∞—è –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω–æ—Å—Ç—å (0-5): –±–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–∞—è –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–Ω–æ—Å—Ç—å\n",
        "        3. –¢–µ—Å—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å (0-5): –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ—Å—Ç—å —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤\n",
        "        4. –†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ (0-5): —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å —Ç–µ—Ä–º–∏–Ω–æ–≤ –∏ –ø–æ–¥—Ö–æ–¥–æ–≤\n",
        "        5. –î–µ—Ç–∞–ª—å–Ω–æ—Å—Ç—å (0-5): –≥–ª—É–±–∏–Ω–∞ –æ–ø–∏—Å–∞–Ω–∏—è –º–µ—Ö–∞–Ω–∏–∑–º–æ–≤\n",
        "        \"\"\"\n",
        "        if not hypotheses_data or not hypotheses_data.get('hypotheses'):\n",
        "            return {'total': 0, 'details': {}}\n",
        "\n",
        "        hypotheses = hypotheses_data['hypotheses']\n",
        "        scores = {\n",
        "            'novelty': 0,        # –ù–æ–≤–∏–∑–Ω–∞\n",
        "            'scientific': 0,     # –ù–∞—É—á–Ω–∞—è –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω–æ—Å—Ç—å\n",
        "            'testability': 0,    # –¢–µ—Å—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å\n",
        "            'diversity': 0,      # –†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ\n",
        "            'detail': 0          # –î–µ—Ç–∞–ª—å–Ω–æ—Å—Ç—å\n",
        "        }\n",
        "\n",
        "        # 1. –ù–æ–≤–∏–∑–Ω–∞ - –ø—Ä–æ–≤–µ—Ä–∫–∞ –±–∞–Ω–∞–ª—å–Ω—ã—Ö —Å–≤—è–∑–µ–π\n",
        "        banality_keywords = ['oxidative stress', 'telomere shortening', 'mitochondrial dysfunction']\n",
        "        novelty_score = 5\n",
        "        for hyp in hypotheses:\n",
        "            hypothesis_text = hyp.get('hypothesis', '').lower()\n",
        "            for keyword in banality_keywords:\n",
        "                if keyword in hypothesis_text:\n",
        "                    novelty_score -= 0.5\n",
        "        scores['novelty'] = max(novelty_score, 0)\n",
        "\n",
        "        # 2. –ù–∞—É—á–Ω–∞—è –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω–æ—Å—Ç—å - –¥–ª–∏–Ω–∞ –∏ –∫–∞—á–µ—Å—Ç–≤–æ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–π\n",
        "        rationale_lengths = [len(hyp.get('rationale', '')) for hyp in hypotheses]\n",
        "        avg_rationale_length = np.mean(rationale_lengths) if rationale_lengths else 0\n",
        "        scores['scientific'] = min(avg_rationale_length / 50, 5)  # 250+ —Å–∏–º–≤–æ–ª–æ–≤ = 5 –±–∞–ª–ª–æ–≤\n",
        "\n",
        "        # 3. –¢–µ—Å—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å - –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ—Å—Ç—å —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤\n",
        "        testability_keywords = ['method', 'measurement', 'quantif', 'statistic', 'control', 'group']\n",
        "        testability_score = 0\n",
        "        for hyp in hypotheses:\n",
        "            validation_text = hyp.get('validation', '').lower()\n",
        "            keyword_count = sum(1 for keyword in testability_keywords if keyword in validation_text)\n",
        "            testability_score += min(keyword_count, 2)  # –ú–∞–∫—Å–∏–º—É–º 2 –±–∞–ª–ª–∞ –∑–∞ –≥–∏–ø–æ—Ç–µ–∑—É\n",
        "        scores['testability'] = min(testability_score / len(hypotheses), 5)\n",
        "\n",
        "        # 4. –†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ - —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å —Ç–µ—Ä–º–∏–Ω–æ–≤\n",
        "        all_terms = []\n",
        "        for hyp in hypotheses:\n",
        "            all_terms.extend(hyp.get('terms', []))\n",
        "        unique_ratio = len(set(all_terms)) / len(all_terms) if all_terms else 0\n",
        "        scores['diversity'] = unique_ratio * 5\n",
        "\n",
        "        # 5. –î–µ—Ç–∞–ª—å–Ω–æ—Å—Ç—å - —Å—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –≥–∏–ø–æ—Ç–µ–∑\n",
        "        hypothesis_lengths = [len(hyp.get('hypothesis', '')) for hyp in hypotheses]\n",
        "        avg_hypothesis_length = np.mean(hypothesis_lengths) if hypothesis_lengths else 0\n",
        "        scores['detail'] = min(avg_hypothesis_length / 40, 5)  # 200+ —Å–∏–º–≤–æ–ª–æ–≤ = 5 –±–∞–ª–ª–æ–≤\n",
        "\n",
        "        # –û–±—â–∏–π –±–∞–ª–ª\n",
        "        total_score = np.mean(list(scores.values()))\n",
        "\n",
        "        return {\n",
        "            'total': round(total_score, 1),\n",
        "            'details': {k: round(v, 1) for k, v in scores.items()},\n",
        "            'max_score': 5.0\n",
        "        }\n",
        "\n",
        "    def generate_quantitative_experiments(self, hypotheses):\n",
        "        \"\"\"\n",
        "        –£–ª—É—á—à–µ–Ω–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º–∏ —á–∏—Å–ª–∞–º–∏ –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π\n",
        "        \"\"\"\n",
        "        improved_hypotheses = []\n",
        "\n",
        "        for i, hyp in enumerate(hypotheses):\n",
        "            improved_hyp = hyp.copy()\n",
        "\n",
        "            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —á–∏—Å–ª–∞ –¥–ª—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤\n",
        "            original_validation = hyp.get('validation', '')\n",
        "\n",
        "            # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–Ω–∫—Ä–µ—Ç–∏–∫—É —á–µ—Ä–µ–∑ LLM\n",
        "            quantitative_prompt = f\"\"\"\n",
        "–£–ª—É—á—à–∏ –æ–ø–∏—Å–∞–Ω–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞, –¥–æ–±–∞–≤–∏–≤ –ö–û–ù–ö–†–ï–¢–ù–´–ï –ß–ò–°–õ–ê –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –º–µ—Ç–æ–¥—ã:\n",
        "\n",
        "–ò—Å—Ö–æ–¥–Ω—ã–π —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç: {original_validation}\n",
        "\n",
        "–î–æ–±–∞–≤—å:\n",
        "1. –†–∞–∑–º–µ—Ä –≤—ã–±–æ—Ä–∫–∏ (n=X)\n",
        "2. –ö–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã–µ –≥—Ä—É–ø–ø—ã\n",
        "3. –û–∂–∏–¥–∞–µ–º—ã–π —Ä–∞–∑–º–µ—Ä —ç—Ñ—Ñ–µ–∫—Ç–∞ (% –∏–∑–º–µ–Ω–µ–Ω–∏—è)\n",
        "4. –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π —Ç–µ—Å—Ç (t-test, ANOVA, etc.)\n",
        "5. –£—Ä–æ–≤–µ–Ω—å –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏ (p<0.05)\n",
        "6. –í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä–∞–º–∫–∏\n",
        "\n",
        "–§–æ—Ä–º–∞—Ç: –ö—Ä–∞—Ç–∫–æ–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ (–º–∞–∫—Å–∏–º—É–º 150 —Å–ª–æ–≤).\n",
        "\"\"\"\n",
        "\n",
        "            try:\n",
        "                response = self.client.chat.completions.create(\n",
        "                    model=\"llama3.1\",\n",
        "                    messages=[{\"role\": \"user\", \"content\": quantitative_prompt}],\n",
        "                    temperature=0.3,\n",
        "                    max_tokens=200\n",
        "                )\n",
        "\n",
        "                improved_validation = response.choices[0].message.content.strip()\n",
        "                improved_hyp['validation'] = improved_validation\n",
        "\n",
        "            except:\n",
        "                # –ï—Å–ª–∏ LLM –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –¥–æ–±–∞–≤–ª—è–µ–º —à–∞–±–ª–æ–Ω–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è\n",
        "                improved_hyp['validation'] = self._add_quantitative_template(original_validation)\n",
        "\n",
        "            improved_hypotheses.append(improved_hyp)\n",
        "\n",
        "        return improved_hypotheses\n",
        "\n",
        "    def _add_quantitative_template(self, original_validation):\n",
        "        \"\"\"–®–∞–±–ª–æ–Ω–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è –¥–ª—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤\"\"\"\n",
        "        additions = [\n",
        "            f\"–†–∞–∑–º–µ—Ä –≤—ã–±–æ—Ä–∫–∏: n=60-80 —Å—É–±—ä–µ–∫—Ç–æ–≤ –Ω–∞ –≥—Ä—É–ø–ø—É\",\n",
        "            f\"–ö–æ–Ω—Ç—Ä–æ–ª—å: —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –≤–æ–∑—Ä–∞—Å—Ç–Ω–æ–π –Ω–æ—Ä–º–æ–π\",\n",
        "            f\"–û–∂–∏–¥–∞–µ–º—ã–π —ç—Ñ—Ñ–µ–∫—Ç: 20-40% –∏–∑–º–µ–Ω–µ–Ω–∏–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π\",\n",
        "            f\"–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: –¥–≤—É—Ö—Ñ–∞–∫—Ç–æ—Ä–Ω—ã–π ANOVA, p<0.05\",\n",
        "            f\"–î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: 12-24 –Ω–µ–¥–µ–ª–∏ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è\"\n",
        "        ]\n",
        "\n",
        "        return f\"{original_validation} {'; '.join(additions[:2])}.\"\n",
        "\n",
        "    def format_scientific_output(self, search_term, subgraph, hypotheses_data,\n",
        "                                quality_terms, quality_details, subgraph_stats):\n",
        "        \"\"\"\n",
        "        –ù–∞—É—á–Ω–æ —Å—Ç—Ä–æ–≥–æ–µ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
        "        \"\"\"\n",
        "        elapsed = time.time() - self.start_time\n",
        "\n",
        "        print(f\"\\nüèÅ –ù–ê–£–ß–ù–´–ô –ê–ù–ê–õ–ò–ó –ó–ê–í–ï–†–®–ï–ù –ó–ê {elapsed:.1f} –°–ï–ö\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        # 1. –°–¢–†–£–ö–¢–£–†–ù–´–ô –ê–ù–ê–õ–ò–ó –ü–û–î–ì–†–ê–§–ê\n",
        "        print(f\"\\nüìä –°–¢–†–£–ö–¢–£–†–ù–´–ô –ê–ù–ê–õ–ò–ó –ü–û–î–ì–†–ê–§–ê:\")\n",
        "        basic = subgraph_stats['basic']\n",
        "        degree_dist = subgraph_stats['degree_distribution']\n",
        "\n",
        "        print(f\"  üìà –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏:\")\n",
        "        print(f\"    ‚Ä¢ –£–∑–ª–æ–≤: {basic['nodes']}\")\n",
        "        print(f\"    ‚Ä¢ –†—ë–±–µ—Ä: {basic['edges']}\")\n",
        "        print(f\"    ‚Ä¢ –ü–ª–æ—Ç–Ω–æ—Å—Ç—å: {basic['density']:.3f}\")\n",
        "\n",
        "        print(f\"  üìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—Ç–µ–ø–µ–Ω–µ–π:\")\n",
        "        print(f\"    ‚Ä¢ –°—Ä–µ–¥–Ω–µ–µ: {degree_dist['mean']:.1f} ¬± {degree_dist['std']:.1f}\")\n",
        "        print(f\"    ‚Ä¢ –ú–µ–¥–∏–∞–Ω–∞: {degree_dist['median']:.1f}\")\n",
        "        print(f\"    ‚Ä¢ –î–∏–∞–ø–∞–∑–æ–Ω: {degree_dist['min']} - {degree_dist['max']}\")\n",
        "\n",
        "        print(f\"  üåü –¶–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã–µ —É–∑–ª—ã (hubs):\")\n",
        "        for i, (node, cent, degree) in enumerate(subgraph_stats['hubs'][:3], 1):\n",
        "            print(f\"    {i}. {node} (—Å—Ç–µ–ø–µ–Ω—å: {degree}, —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω–æ—Å—Ç—å: {cent:.3f})\")\n",
        "\n",
        "        # 2. –ö–ê–ß–ï–°–¢–í–ï–ù–ù–´–ï –¢–ï–†–ú–ò–ù–´ –° –ú–ï–¢–†–ò–ö–ê–ú–ò\n",
        "        print(f\"\\nüî¨ –ö–ê–ß–ï–°–¢–í–ï–ù–ù–´–ï –¢–ï–†–ú–ò–ù–´ (—Ç–æ–ø-10):\")\n",
        "        print(f\"{'–¢–µ—Ä–º–∏–Ω':<25} {'–ö–∞—á–µ—Å—Ç–≤–æ':<8} {'–°—Ç–µ–ø–µ–Ω—å':<7} {'PageRank':<10} {'–ë–∏–æ–ª.':<6}\")\n",
        "        print(\"-\" * 65)\n",
        "\n",
        "        for term in quality_terms[:10]:\n",
        "            if term in quality_details:\n",
        "                details = quality_details[term]\n",
        "                quality_score = (details['degree']['score'] +\n",
        "                               details.get('pagerank', {}).get('score', 0) +\n",
        "                               details.get('betweenness', {}).get('score', 0) +\n",
        "                               details['biological']['score'] +\n",
        "                               details['metadata']['score'])\n",
        "\n",
        "                degree = details['degree']['value']\n",
        "                pagerank = details.get('pagerank', {}).get('value', 0)\n",
        "                bio_score = details['biological']['score']\n",
        "\n",
        "                print(f\"{term:<25} {quality_score:<8.1f} {degree:<7} {pagerank:<10.4f} {bio_score:<6.1f}\")\n",
        "\n",
        "        # 3. –ì–ò–ü–û–¢–ï–ó–´ –° –ú–ù–û–ì–û–§–ê–ö–¢–û–†–ù–û–ô –û–¶–ï–ù–ö–û–ô\n",
        "        if hypotheses_data and hypotheses_data.get('hypotheses'):\n",
        "            hypotheses = hypotheses_data['hypotheses']\n",
        "\n",
        "            # –£–ª—É—á—à–∞–µ–º —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã\n",
        "            improved_hypotheses = self.generate_quantitative_experiments(hypotheses)\n",
        "\n",
        "            # –ú–Ω–æ–≥–æ—Ñ–∞–∫—Ç–æ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞\n",
        "            quality_eval = self.evaluate_hypothesis_quality({'hypotheses': improved_hypotheses})\n",
        "\n",
        "            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ö–æ–∂–µ—Å—Ç–∏\n",
        "            similarity_check = self.check_hypothesis_similarity(improved_hypotheses)\n",
        "\n",
        "            print(f\"\\nüí° –ù–ê–£–ß–ù–´–ï –ì–ò–ü–û–¢–ï–ó–´ –î–õ–Ø '{search_term.upper()}':\")\n",
        "            print(\"=\" * 80)\n",
        "\n",
        "            print(f\"üìä –û–¶–ï–ù–ö–ê –ö–ê–ß–ï–°–¢–í–ê (0-5 –ø–æ –∫–∞–∂–¥–æ–º—É –∫—Ä–∏—Ç–µ—Ä–∏—é):\")\n",
        "            details = quality_eval['details']\n",
        "            print(f\"  ‚Ä¢ –ù–æ–≤–∏–∑–Ω–∞: {details['novelty']}/5\")\n",
        "            print(f\"  ‚Ä¢ –ù–∞—É—á–Ω–∞—è –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω–æ—Å—Ç—å: {details['scientific']}/5\")\n",
        "            print(f\"  ‚Ä¢ –¢–µ—Å—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å: {details['testability']}/5\")\n",
        "            print(f\"  ‚Ä¢ –†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ: {details['diversity']}/5\")\n",
        "            print(f\"  ‚Ä¢ –î–µ—Ç–∞–ª—å–Ω–æ—Å—Ç—å: {details['detail']}/5\")\n",
        "            print(f\"  üéØ –û–ë–©–ò–ô –ë–ê–õ–õ: {quality_eval['total']}/5.0\")\n",
        "\n",
        "            if similarity_check:\n",
        "                print(f\"\\n‚ö†Ô∏è –û–ë–ù–ê–†–£–ñ–ï–ù–û –°–•–û–î–°–¢–í–û:\")\n",
        "                for sim in similarity_check[:2]:\n",
        "                    print(f\"  ‚Ä¢ –ì–∏–ø–æ—Ç–µ–∑—ã {sim['hypothesis_1']} –∏ {sim['hypothesis_2']}: {sim['similarity']:.1%} —Å—Ö–æ–∂–µ—Å—Ç–∏\")\n",
        "\n",
        "            print(f\"\\n\" + \"=\" * 80)\n",
        "\n",
        "            # –í—ã–≤–æ–¥–∏–º —É–ª—É—á—à–µ–Ω–Ω—ã–µ –≥–∏–ø–æ—Ç–µ–∑—ã\n",
        "            for i, hyp in enumerate(improved_hypotheses, 1):\n",
        "                print(f\"\\n{i}. –ì–ò–ü–û–¢–ï–ó–ê:\")\n",
        "                print(f\"   üß¨ –¢–µ—Ä–º–∏–Ω—ã: {', '.join(hyp.get('terms', []))}\")\n",
        "                print(f\"   üî¨ –ú–µ—Ö–∞–Ω–∏–∑–º: {hyp.get('hypothesis', '')}\")\n",
        "                print(f\"   üìö –û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ: {hyp.get('rationale', '')}\")\n",
        "                print(f\"   üß™ –ö–æ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç: {hyp.get('validation', '')}\")\n",
        "                print(\"-\" * 80)\n",
        "\n",
        "        # 4. –ú–ï–¢–ê–î–ê–ù–ù–´–ï –° –û–ß–ò–°–¢–ö–û–ô\n",
        "        clean_meta = self.get_clean_metadata(search_term)\n",
        "        if clean_meta:\n",
        "            print(f\"\\nüìö –î–û–ö–ê–ó–ê–¢–ï–õ–¨–ù–ê–Ø –ë–ê–ó–ê –¥–ª—è '{search_term}':\")\n",
        "            if clean_meta.get('dois'):\n",
        "                print(f\"üìÑ –í–∞–ª–∏–¥–Ω—ã–µ DOI: {clean_meta['dois'][:3]}\")\n",
        "            if clean_meta.get('years'):\n",
        "                years = clean_meta['years']\n",
        "                print(f\"üìÖ –ü–µ—Ä–∏–æ–¥ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π: {min(years)}-{max(years)} ({len(years)} –ø—É–±–ª–∏–∫–∞—Ü–∏–π)\")\n",
        "            if clean_meta.get('authors'):\n",
        "                print(f\"üë• –ö–ª—é—á–µ–≤—ã–µ –∞–≤—Ç–æ—Ä—ã: {clean_meta['authors'][:2]}\")\n",
        "\n",
        "    def get_clean_metadata(self, node):\n",
        "        \"\"\"–ü–æ–ª—É—á–µ–Ω–∏–µ –æ—á–∏—â–µ–Ω–Ω—ã—Ö –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö (–∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–π –≤–µ—Ä—Å–∏–∏)\"\"\"\n",
        "        if node not in self.node_full_meta:\n",
        "            return {}\n",
        "\n",
        "        meta = self.node_full_meta[node]\n",
        "        clean_meta = {}\n",
        "\n",
        "        # DOI\n",
        "        dois = meta.get('dois', [])\n",
        "        valid_dois = [doi for doi in dois if doi and str(doi) != 'nan' and 'doi.org' in str(doi)]\n",
        "        clean_meta['dois'] = valid_dois[:3]\n",
        "\n",
        "        # –ì–æ–¥—ã\n",
        "        years = meta.get('years', [])\n",
        "        valid_years = []\n",
        "        for y in years:\n",
        "            try:\n",
        "                if y and str(y) != 'nan':\n",
        "                    year_num = int(str(y)[:4])\n",
        "                    if 1990 <= year_num <= 2025:\n",
        "                        valid_years.append(year_num)\n",
        "            except:\n",
        "                continue\n",
        "        clean_meta['years'] = sorted(set(valid_years))\n",
        "\n",
        "        # –ê–≤—Ç–æ—Ä—ã\n",
        "        authors = meta.get('authors', [])\n",
        "        valid_authors = []\n",
        "        for a in authors:\n",
        "            if a and str(a) != 'nan' and len(str(a)) > 10:\n",
        "                clean_author = str(a).strip().rstrip(',').strip()\n",
        "                if clean_author:\n",
        "                    valid_authors.append(clean_author)\n",
        "        clean_meta['authors'] = valid_authors[:3]\n",
        "\n",
        "        return clean_meta\n",
        "\n",
        "    def analyze_term_scientific(self, search_term, radius=3):\n",
        "        \"\"\"\n",
        "        –ì–ª–∞–≤–Ω–∞—è –Ω–∞—É—á–Ω–æ —Å—Ç—Ä–æ–≥–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞\n",
        "        \"\"\"\n",
        "        self.start_time = time.time()\n",
        "\n",
        "        print(\"=\" * 80)\n",
        "        print(f\"üß¨ –ù–ê–£–ß–ù–û –°–¢–†–û–ì–ò–ô –ê–ù–ê–õ–ò–ó: {search_term.upper()}\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        # 1. –ü–æ–∏—Å–∫ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—è —Ç–µ—Ä–º–∏–Ω–∞\n",
        "        print(f\"üîç –ü–æ–∏—Å–∫ —Ç–µ—Ä–º–∏–Ω–∞ '{search_term}'...\")\n",
        "        found_terms = self.find_term_in_graph(search_term)\n",
        "\n",
        "        if not found_terms:\n",
        "            print(f\"‚ùå –¢–µ—Ä–º–∏–Ω –Ω–µ –Ω–∞–π–¥–µ–Ω\")\n",
        "            return None\n",
        "\n",
        "        search_term = found_terms[0]\n",
        "        print(f\"‚úÖ –ù–∞–π–¥–µ–Ω: {search_term}\")\n",
        "\n",
        "        # 2. –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø–æ–¥–≥—Ä–∞—Ñ–∞\n",
        "        subgraph = self.build_enhanced_subgraph(search_term, radius, min_nodes=25, max_nodes=150)\n",
        "\n",
        "        # 3. –°—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑\n",
        "        subgraph_stats = self.analyze_subgraph_structure(subgraph, search_term)\n",
        "\n",
        "        # 4. –û—Ç–±–æ—Ä –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤ —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏\n",
        "        quality_terms, quality_details = self.select_quality_terms(subgraph, search_term)\n",
        "\n",
        "        # 5. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≥–∏–ø–æ—Ç–µ–∑\n",
        "        hypotheses_data = self.generate_enhanced_hypotheses(search_term, quality_terms, subgraph_stats['basic'])\n",
        "\n",
        "        # 6. –ù–∞—É—á–Ω–æ–µ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ\n",
        "        self.format_scientific_output(search_term, subgraph, hypotheses_data,\n",
        "                                    quality_terms, quality_details, subgraph_stats)\n",
        "\n",
        "        return {\n",
        "            'search_term': search_term,\n",
        "            'subgraph': subgraph,\n",
        "            'subgraph_stats': subgraph_stats,\n",
        "            'quality_terms': quality_terms,\n",
        "            'quality_details': quality_details,\n",
        "            'hypotheses': hypotheses_data\n",
        "        }\n",
        "\n",
        "    # –û—Å—Ç–∞–ª—å–Ω—ã–µ –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã (find_term_in_graph, build_enhanced_subgraph, etc.)\n",
        "    # –∫–æ–ø–∏—Ä—É—é—Ç—Å—è –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–π –≤–µ—Ä—Å–∏–∏...\n",
        "\n",
        "    def find_term_in_graph(self, search_term, fuzzy_threshold=0.6):\n",
        "        \"\"\"–ü–æ–∏—Å–∫ —Ç–µ—Ä–º–∏–Ω–∞ –≤ –≥—Ä–∞—Ñ–µ\"\"\"\n",
        "        search_term = search_term.lower().strip()\n",
        "\n",
        "        if search_term in self.G.nodes:\n",
        "            return [search_term]\n",
        "\n",
        "        substring_matches = [node for node in self.G.nodes if search_term in node.lower()]\n",
        "        if substring_matches:\n",
        "            return substring_matches[:5]\n",
        "\n",
        "        fuzzy_matches = []\n",
        "        for node in self.G.nodes:\n",
        "            similarity = difflib.SequenceMatcher(None, search_term, node).ratio()\n",
        "            if similarity >= fuzzy_threshold:\n",
        "                fuzzy_matches.append((node, similarity))\n",
        "\n",
        "        fuzzy_matches.sort(key=lambda x: x[1], reverse=True)\n",
        "        return [match[0] for match in fuzzy_matches[:5]]\n",
        "\n",
        "    def build_enhanced_subgraph(self, search_term, radius=3, min_nodes=25, max_nodes=150):\n",
        "        \"\"\"–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –ø–æ–¥–≥—Ä–∞—Ñ–∞\"\"\"\n",
        "        print(f\"üï∏Ô∏è –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø–æ–¥–≥—Ä–∞—Ñ–∞ (—Ä–∞–¥–∏—É—Å: {radius}, –º–∏–Ω: {min_nodes}, –º–∞–∫—Å: {max_nodes})...\")\n",
        "\n",
        "        all_nodes = {search_term}\n",
        "        current_level = {search_term}\n",
        "\n",
        "        for step in range(1, radius + 1):\n",
        "            next_level = set()\n",
        "            for node in current_level:\n",
        "                if node in self.G:\n",
        "                    neighbors = set(self.G.neighbors(node))\n",
        "                    bio_neighbors = {n for n in neighbors if not self._is_technical_term(n)}\n",
        "                    next_level.update(bio_neighbors)\n",
        "\n",
        "            all_nodes.update(next_level)\n",
        "            current_level = next_level\n",
        "            print(f\"  –®–∞–≥ {step}: +{len(next_level)} —É–∑–ª–æ–≤ (–≤—Å–µ–≥–æ: {len(all_nodes)})\")\n",
        "\n",
        "            if len(all_nodes) >= min_nodes:\n",
        "                break\n",
        "\n",
        "            if len(all_nodes) >= max_nodes:\n",
        "                print(f\"  ‚ö†Ô∏è –î–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç: {max_nodes} —É–∑–ª–æ–≤\")\n",
        "                break\n",
        "\n",
        "        subgraph = self.G.subgraph(all_nodes).copy()\n",
        "        print(f\"‚úÖ –ü–æ–¥–≥—Ä–∞—Ñ: {subgraph.number_of_nodes()} —É–∑–ª–æ–≤, {subgraph.number_of_edges()} —Ä—ë–±–µ—Ä\")\n",
        "\n",
        "        return subgraph\n",
        "\n",
        "    def generate_enhanced_hypotheses(self, search_term, quality_terms, subgraph_stats):\n",
        "        \"\"\"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≥–∏–ø–æ—Ç–µ–∑ (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –º–µ—Å—Ç–∞)\"\"\"\n",
        "        print(f\"üí° –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≥–∏–ø–æ—Ç–µ–∑ –¥–ª—è '{search_term}'...\")\n",
        "\n",
        "        # –ó–¥–µ—Å—å –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –ø–æ–ª–Ω–∞—è –ª–æ–≥–∏–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–π –≤–µ—Ä—Å–∏–∏\n",
        "        # –î–ª—è –∫—Ä–∞—Ç–∫–æ—Å—Ç–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –∑–∞–≥–ª—É—à–∫—É\n",
        "\n",
        "        return {\n",
        "            'search_term': search_term,\n",
        "            'hypotheses': [\n",
        "                {\n",
        "                    'terms': [search_term, quality_terms[0] if quality_terms else 'placeholder'],\n",
        "                    'hypothesis': f'–ú–µ—Ö–∞–Ω–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –≥–∏–ø–æ—Ç–µ–∑–∞ –æ —Ä–æ–ª–∏ {search_term}',\n",
        "                    'rationale': '–ù–∞—É—á–Ω–æ–µ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ –≥–∏–ø–æ—Ç–µ–∑—ã',\n",
        "                    'validation': '–ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏'\n",
        "                }\n",
        "            ]\n",
        "        }"
      ],
      "metadata": {
        "id": "dFjWsiM_QGRE"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === –ù–ê–£–ß–ù–û –°–¢–†–û–ì–ò–ô –ò–ù–¢–ï–†–§–ï–ô–° ===\n",
        "\n",
        "import time\n",
        "from openai import OpenAI\n",
        "\n",
        "def create_scientific_generator():\n",
        "    \"\"\"–°–æ–∑–¥–∞–Ω–∏–µ –Ω–∞—É—á–Ω–æ —Å—Ç—Ä–æ–≥–æ–≥–æ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞\"\"\"\n",
        "    print(\"üî¨ –°–û–ó–î–ê–ù–ò–ï –ù–ê–£–ß–ù–û –°–¢–†–û–ì–û–ì–û –ì–ï–ù–ï–†–ê–¢–û–†–ê\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    try:\n",
        "        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
        "        print(f\"üìä –ì—Ä–∞—Ñ: {G.number_of_nodes():,} —É–∑–ª–æ–≤, {G.number_of_edges():,} —Ä—ë–±–µ—Ä\")\n",
        "        print(f\"üìÅ –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ: {len(node_full_meta):,} –∑–∞–ø–∏—Å–µ–π\")\n",
        "\n",
        "        # –ö–ª–∏–µ–Ω—Ç\n",
        "        client = OpenAI(\n",
        "            base_url=\"http://80.209.242.40:8000/v1\",\n",
        "            api_key=\"dummy-key\"\n",
        "        )\n",
        "\n",
        "        # –°–æ–∑–¥–∞–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞\n",
        "        scientific_generator = ScientificHypothesisGenerator(G, node_full_meta, edge_full_meta, client)\n",
        "\n",
        "        print(\"‚úÖ –ù–∞—É—á–Ω–æ —Å—Ç—Ä–æ–≥–∏–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Å–æ–∑–¥–∞–Ω!\")\n",
        "        print(\"\\n–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:\")\n",
        "        print(\"  üî¨ –ß–µ—Ç–∫–∏–µ –∫—Ä–∏—Ç–µ—Ä–∏–∏ –∫–∞—á–µ—Å—Ç–≤–∞ —Ç–µ—Ä–º–∏–Ω–æ–≤ (5 –º–µ—Ç—Ä–∏–∫)\")\n",
        "        print(\"  üìä –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø–æ–¥–≥—Ä–∞—Ñ–∞\")\n",
        "        print(\"  üö´ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è –≥–∏–ø–æ—Ç–µ–∑\")\n",
        "        print(\"  üìà –ú–Ω–æ–≥–æ—Ñ–∞–∫—Ç–æ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ (5 –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤)\")\n",
        "        print(\"  üß™ –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã\")\n",
        "\n",
        "        return scientific_generator\n",
        "\n",
        "    except NameError as e:\n",
        "        print(f\"‚ùå –û—à–∏–±–∫–∞: {e}\")\n",
        "        print(\"–°–æ–∑–¥–∞–π—Ç–µ –≥—Ä–∞—Ñ: G, node_full_meta, edge_full_meta = build_graph_with_metadata(df)\")\n",
        "        return None\n",
        "\n",
        "def scientific_test():\n",
        "    \"\"\"–ù–∞—É—á–Ω—ã–π —Ç–µ—Å—Ç —Å–∏—Å—Ç–µ–º—ã\"\"\"\n",
        "    print(\"\\nüß™ –ù–ê–£–ß–ù–´–ô –¢–ï–°–¢ –°–ò–°–¢–ï–ú–´\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    generator = create_scientific_generator()\n",
        "    if not generator:\n",
        "        return None\n",
        "\n",
        "    # –¢–µ—Å—Ç —Å –ø—Ä–æ—Å—Ç—ã–º —Ç–µ—Ä–º–∏–Ω–æ–º\n",
        "    test_term = \"aging\"\n",
        "    print(f\"\\nüî¨ –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ç–µ—Ä–º–∏–Ω: {test_term}\")\n",
        "    print(\"‚è≥ –≠—Ç–æ –∑–∞–π–º–µ—Ç 60-120 —Å–µ–∫—É–Ω–¥...\")\n",
        "\n",
        "    try:\n",
        "        result = generator.analyze_term_scientific(test_term, radius=2)  # –£–º–µ–Ω—å—à–µ–Ω–Ω—ã–π —Ä–∞–¥–∏—É—Å –¥–ª—è —Ç–µ—Å—Ç–∞\n",
        "\n",
        "        if result:\n",
        "            print(f\"\\n‚úÖ –ù–ê–£–ß–ù–´–ô –¢–ï–°–¢ –£–°–ü–ï–®–ï–ù!\")\n",
        "            print(f\"üìä –ü–æ–¥–≥—Ä–∞—Ñ: {result['subgraph_stats']['basic']['nodes']} —É–∑–ª–æ–≤\")\n",
        "            print(f\"üî¨ –ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤: {len(result['quality_terms'])}\")\n",
        "            return generator\n",
        "        else:\n",
        "            print(\"‚ùå –¢–µ—Å—Ç –Ω–µ –ø—Ä–æ—à–µ–ª\")\n",
        "            return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå –û—à–∏–±–∫–∞ –≤ —Ç–µ—Å—Ç–µ: {e}\")\n",
        "        return None\n",
        "\n",
        "def compare_quality_metrics():\n",
        "    \"\"\"–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —É–ª—É—á—à–µ–Ω–∏–π –≤ –º–µ—Ç—Ä–∏–∫–∞—Ö –∫–∞—á–µ—Å—Ç–≤–∞\"\"\"\n",
        "\n",
        "\n",
        "    print(\"\\n–ù–û–í–ê–Ø –ù–ê–£–ß–ù–ê–Ø –í–ï–†–°–ò–Ø:\")\n",
        "    print(\"  ‚úÖ 5 —á–µ—Ç–∫–∏—Ö –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞ —Å –ø–æ—Ä–æ–≥–∞–º–∏:\")\n",
        "    print(\"    ‚Ä¢ Degree centrality (0-2 –±–∞–ª–ª–∞)\")\n",
        "    print(\"    ‚Ä¢ PageRank (0-2 –±–∞–ª–ª–∞)\")\n",
        "    print(\"    ‚Ä¢ Betweenness centrality (0-2 –±–∞–ª–ª–∞)\")\n",
        "    print(\"    ‚Ä¢ –ë–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–∞—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å (0-3 –±–∞–ª–ª–∞)\")\n",
        "    print(\"    ‚Ä¢ –ö–∞—á–µ—Å—Ç–≤–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö (0-1 –±–∞–ª–ª)\")\n",
        "\n",
        "    print(\"  ‚úÖ –ú–Ω–æ–≥–æ—Ñ–∞–∫—Ç–æ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ (0-5 –ø–æ –∫–∞–∂–¥–æ–º—É):\")\n",
        "    print(\"    ‚Ä¢ –ù–æ–≤–∏–∑–Ω–∞\")\n",
        "    print(\"    ‚Ä¢ –ù–∞—É—á–Ω–∞—è –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω–æ—Å—Ç—å\")\n",
        "    print(\"    ‚Ä¢ –¢–µ—Å—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å\")\n",
        "    print(\"    ‚Ä¢ –†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ\")\n",
        "    print(\"    ‚Ä¢ –î–µ—Ç–∞–ª—å–Ω–æ—Å—Ç—å\")\n",
        "\n",
        "    print(\"  ‚úÖ TF-IDF –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è –≥–∏–ø–æ—Ç–µ–∑\")\n",
        "\n",
        "    print(\"  ‚úÖ –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø–æ–¥–≥—Ä–∞—Ñ–∞:\")\n",
        "    print(\"    ‚Ä¢ –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—Ç–µ–ø–µ–Ω–µ–π\")\n",
        "    print(\"    ‚Ä¢ –ü–ª–æ—Ç–Ω–æ—Å—Ç—å —Å–µ—Ç–∏\")\n",
        "    print(\"    ‚Ä¢ –¶–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã–µ —É–∑–ª—ã (hubs)\")\n",
        "    print(\"    ‚Ä¢ –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —Å–≤—è–∑–Ω–æ—Å—Ç–∏\")\n",
        "\n",
        "    print(\"  ‚úÖ –ö–æ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã:\")\n",
        "    print(\"    ‚Ä¢ –†–∞–∑–º–µ—Ä—ã –≤—ã–±–æ—Ä–æ–∫ (n=60-80)\")\n",
        "    print(\"    ‚Ä¢ –ö–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã–µ –≥—Ä—É–ø–ø—ã\")\n",
        "    print(\"    ‚Ä¢ –û–∂–∏–¥–∞–µ–º—ã–µ —ç—Ñ—Ñ–µ–∫—Ç—ã (20-40%)\")\n",
        "    print(\"    ‚Ä¢ –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ —Ç–µ—Å—Ç—ã (ANOVA, p<0.05)\")\n",
        "    print(\"    ‚Ä¢ –í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä–∞–º–∫–∏ (12-24 –Ω–µ–¥–µ–ª–∏)\")\n",
        "\n",
        "def scientific_interactive_mode():\n",
        "    \"\"\"–ù–∞—É—á–Ω–æ —Å—Ç—Ä–æ–≥–∏–π –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º\"\"\"\n",
        "    print(\"\\nüß¨ –ù–ê–£–ß–ù–û –°–¢–†–û–ì–ò–ô –ò–ù–¢–ï–†–ê–ö–¢–ò–í–ù–´–ô –†–ï–ñ–ò–ú\")\n",
        "    print(\"=\" * 70)\n",
        "    print(\"–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:\")\n",
        "    print(\"  üî¨ –ù–∞—É—á–Ω–æ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞\")\n",
        "    print(\"  üìä –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Å–µ—Ç–∏\")\n",
        "    print(\"  üö´ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è\")\n",
        "    print(\"  üìà –ú–Ω–æ–≥–æ—Ñ–∞–∫—Ç–æ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ (5 –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤)\")\n",
        "    print(\"  üß™ –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã\")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    generator = create_scientific_generator()\n",
        "    if not generator:\n",
        "        return\n",
        "\n",
        "    analysis_count = 0\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            search_term = input(\"\\nüîç –í–≤–µ–¥–∏—Ç–µ —Ç–µ—Ä–º–∏–Ω (–∏–ª–∏ 'quit'): \").strip()\n",
        "\n",
        "            if search_term.lower() in ['quit', 'exit', 'q']:\n",
        "                print(\"üëã –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!\")\n",
        "                break\n",
        "\n",
        "            if not search_term:\n",
        "                continue\n",
        "\n",
        "            analysis_count += 1\n",
        "            print(f\"\\n{'üî¨' * 30}\")\n",
        "            print(f\"–ù–ê–£–ß–ù–´–ô –ê–ù–ê–õ–ò–ó #{analysis_count}: {search_term.upper()}\")\n",
        "            print(f\"{'üî¨' * 30}\")\n",
        "            print(\"‚è≥ –ù–∞—É—á–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∑–∞–π–º–µ—Ç 90-180 —Å–µ–∫—É–Ω–¥...\")\n",
        "            print(\"üìä –í–∫–ª—é—á–∞–µ—Ç: —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑ + –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ + –ø—Ä–æ–≤–µ—Ä–∫—É –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è\")\n",
        "            print(\"‚ùó –î–æ–∂–¥–∏—Ç–µ—Å—å –ø–æ–ª–Ω–æ–≥–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è!\")\n",
        "            print(\"-\" * 90)\n",
        "\n",
        "            start_time = time.time()\n",
        "\n",
        "            try:\n",
        "                result = generator.analyze_term_scientific(search_term, radius=3)\n",
        "                elapsed = time.time() - start_time\n",
        "\n",
        "                if result:\n",
        "                    print(f\"\\n{'‚úÖ' * 35}\")\n",
        "                    print(f\"–ù–ê–£–ß–ù–´–ô –ê–ù–ê–õ–ò–ó #{analysis_count} –ó–ê–í–ï–†–®–ï–ù!\")\n",
        "                    print(f\"–¢–µ—Ä–º–∏–Ω: {search_term}\")\n",
        "                    print(f\"–í—Ä–µ–º—è: {elapsed:.1f} —Å–µ–∫—É–Ω–¥\")\n",
        "\n",
        "                    # –ö—Ä–∞—Ç–∫–∞—è —Å–≤–æ–¥–∫–∞\n",
        "                    stats = result['subgraph_stats']['basic']\n",
        "                    print(f\"üìä –ü–æ–¥–≥—Ä–∞—Ñ: {stats['nodes']} —É–∑–ª–æ–≤, –ø–ª–æ—Ç–Ω–æ—Å—Ç—å {stats['density']:.3f}\")\n",
        "                    print(f\"üî¨ –ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤: {len(result['quality_terms'])}\")\n",
        "\n",
        "                    if result.get('hypotheses') and result['hypotheses'].get('hypotheses'):\n",
        "                        hyp_count = len(result['hypotheses']['hypotheses'])\n",
        "                        print(f\"üí° –ì–∏–ø–æ—Ç–µ–∑: {hyp_count}\")\n",
        "\n",
        "                    print(f\"{'‚úÖ' * 35}\")\n",
        "\n",
        "                    print(f\"\\n{'üéØ' * 35}\")\n",
        "                    print(\"–ì–û–¢–û–í –ö –°–õ–ï–î–£–Æ–©–ï–ú–£ –ù–ê–£–ß–ù–û–ú–£ –ê–ù–ê–õ–ò–ó–£\")\n",
        "                    print(f\"{'üéØ' * 35}\")\n",
        "\n",
        "                else:\n",
        "                    print(f\"‚ùå –ù–∞—É—á–Ω—ã–π –∞–Ω–∞–ª–∏–∑ '{search_term}' –Ω–µ —É–¥–∞–ª—Å—è\")\n",
        "\n",
        "            except Exception as e:\n",
        "                elapsed = time.time() - start_time\n",
        "                print(f\"\\n‚ùå –û–®–ò–ë–ö–ê –≤ –Ω–∞—É—á–Ω–æ–º –∞–Ω–∞–ª–∏–∑–µ '{search_term}' (–≤—Ä–µ–º—è: {elapsed:.1f} —Å–µ–∫)\")\n",
        "                print(f\"–î–µ—Ç–∞–ª–∏: {str(e)}\")\n",
        "                print(\"\\nüí° –ü–æ–ø—Ä–æ–±—É–π—Ç–µ:\")\n",
        "                print(\"  ‚Ä¢ –ë–æ–ª–µ–µ –ø—Ä–æ—Å—Ç–æ–π —Ç–µ—Ä–º–∏–Ω (aging, cell, protein)\")\n",
        "                print(\"  ‚Ä¢ –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å LLM\")\n",
        "                continue\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\n‚èπÔ∏è –ü—Ä–µ—Ä—ã–≤–∞–Ω–∏–µ (Ctrl+C)\")\n",
        "            break\n",
        "\n",
        "def batch_scientific_analysis(terms_list):\n",
        "    \"\"\"–ü–∞–∫–µ—Ç–Ω—ã–π –Ω–∞—É—á–Ω—ã–π –∞–Ω–∞–ª–∏–∑\"\"\"\n",
        "    print(f\"\\nüì¶ –ü–ê–ö–ï–¢–ù–´–ô –ù–ê–£–ß–ù–´–ô –ê–ù–ê–õ–ò–ó {len(terms_list)} –¢–ï–†–ú–ò–ù–û–í\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    generator = create_scientific_generator()\n",
        "    if not generator:\n",
        "        return {}\n",
        "\n",
        "    results = {}\n",
        "    total_start = time.time()\n",
        "\n",
        "    for i, term in enumerate(terms_list, 1):\n",
        "        print(f\"\\n[{i}/{len(terms_list)}] üî¨ –ù–∞—É—á–Ω—ã–π –∞–Ω–∞–ª–∏–∑: {term}\")\n",
        "\n",
        "        try:\n",
        "            start_time = time.time()\n",
        "            result = generator.analyze_term_scientific(term, radius=3)\n",
        "            elapsed = time.time() - start_time\n",
        "\n",
        "            if result:\n",
        "                results[term] = result\n",
        "                stats = result['subgraph_stats']['basic']\n",
        "                quality_count = len(result['quality_terms'])\n",
        "\n",
        "                print(f\"‚úÖ {term}: {stats['nodes']} —É–∑–ª–æ–≤, {quality_count} –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤, {elapsed:.1f} —Å–µ–∫\")\n",
        "            else:\n",
        "                print(f\"‚ùå {term}: –∞–Ω–∞–ª–∏–∑ –Ω–µ —É–¥–∞–ª—Å—è\")\n",
        "\n",
        "            # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∞–Ω–∞–ª–∏–∑–∞–º–∏\n",
        "            if i < len(terms_list):\n",
        "                print(\"‚è∏Ô∏è –ü–∞—É–∑–∞ 5 —Å–µ–∫—É–Ω–¥...\")\n",
        "                time.sleep(5)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå {term}: –æ—à–∏–±–∫–∞ {e}\")\n",
        "            continue\n",
        "\n",
        "    total_elapsed = time.time() - total_start\n",
        "    print(f\"\\nüìä –ò–¢–û–ì–ò –ü–ê–ö–ï–¢–ù–û–ì–û –ê–ù–ê–õ–ò–ó–ê:\")\n",
        "    print(f\"‚úÖ –£—Å–ø–µ—à–Ω–æ: {len(results)}/{len(terms_list)}\")\n",
        "    print(f\"‚è±Ô∏è –û–±—â–µ–µ –≤—Ä–µ–º—è: {total_elapsed/60:.1f} –º–∏–Ω—É—Ç\")\n",
        "    print(f\"üìà –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –Ω–∞ —Ç–µ—Ä–º–∏–Ω: {total_elapsed/len(terms_list):.1f} —Å–µ–∫—É–Ω–¥\")\n",
        "\n",
        "    return results\n",
        "\n",
        "def demonstrate_improvements():\n",
        "    \"\"\"–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —É–ª—É—á—à–µ–Ω–∏–π –Ω–∞ –ø—Ä–∏–º–µ—Ä–µ\"\"\"\n",
        "    print(\"\\nüéØ –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –£–õ–£–ß–®–ï–ù–ò–ô\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    print(\"–ü—Ä–∏–º–µ—Ä –≤—ã–≤–æ–¥–∞ –°–¢–ê–†–û–ô –≤–µ—Ä—Å–∏–∏:\")\n",
        "    print(\"\"\"\n",
        "üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ–¥–≥—Ä–∞—Ñ–∞:\n",
        "  ‚Ä¢ –£–∑–ª–æ–≤: 3\n",
        "  ‚Ä¢ –†—ë–±–µ—Ä: 3\n",
        "üìÑ –ö–ª—é—á–µ–≤—ã–µ —Å—Ç–∞—Ç—å–∏: [nan]\n",
        "üîç –†–µ–¥–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã: []\n",
        "–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞: 10.0/10 (–±–µ–∑ —Ä–∞–∑–±–∏–≤–∫–∏)\n",
        "    \"\"\")\n",
        "\n",
        "    print(\"–ü—Ä–∏–º–µ—Ä –≤—ã–≤–æ–¥–∞ –ù–û–í–û–ô –ù–ê–£–ß–ù–û–ô –≤–µ—Ä—Å–∏–∏:\")\n",
        "    print(\"\"\"\n",
        "üìä –°–¢–†–£–ö–¢–£–†–ù–´–ô –ê–ù–ê–õ–ò–ó –ü–û–î–ì–†–ê–§–ê:\n",
        "  üìà –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏:\n",
        "    ‚Ä¢ –£–∑–ª–æ–≤: 47\n",
        "    ‚Ä¢ –†—ë–±–µ—Ä: 156\n",
        "    ‚Ä¢ –ü–ª–æ—Ç–Ω–æ—Å—Ç—å: 0.145\n",
        "  üìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—Ç–µ–ø–µ–Ω–µ–π:\n",
        "    ‚Ä¢ –°—Ä–µ–¥–Ω–µ–µ: 6.6 ¬± 4.2\n",
        "    ‚Ä¢ –ú–µ–¥–∏–∞–Ω–∞: 5.0\n",
        "    ‚Ä¢ –î–∏–∞–ø–∞–∑–æ–Ω: 3 - 18\n",
        "  üåü –¶–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã–µ —É–∑–ª—ã (hubs):\n",
        "    1. autophagy (—Å—Ç–µ–ø–µ–Ω—å: 18, —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω–æ—Å—Ç—å: 0.156)\n",
        "\n",
        "üî¨ –ö–ê–ß–ï–°–¢–í–ï–ù–ù–´–ï –¢–ï–†–ú–ò–ù–´ (—Ç–æ–ø-10):\n",
        "–¢–µ—Ä–º–∏–Ω                   –ö–∞—á–µ—Å—Ç–≤–æ –°—Ç–µ–ø–µ–Ω—å PageRank  –ë–∏–æ–ª.\n",
        "-----------------------------------------------------------------\n",
        "autophagy               8.2      18      0.0156    3.0\n",
        "senescence              7.8      15      0.0134    3.0\n",
        "mitochondria            7.1      12      0.0121    2.5\n",
        "\n",
        "üìä –û–¶–ï–ù–ö–ê –ö–ê–ß–ï–°–¢–í–ê (0-5 –ø–æ –∫–∞–∂–¥–æ–º—É –∫—Ä–∏—Ç–µ—Ä–∏—é):\n",
        "  ‚Ä¢ –ù–æ–≤–∏–∑–Ω–∞: 4.2/5\n",
        "  ‚Ä¢ –ù–∞—É—á–Ω–∞—è –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω–æ—Å—Ç—å: 4.7/5\n",
        "  ‚Ä¢ –¢–µ—Å—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å: 3.8/5\n",
        "  ‚Ä¢ –†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ: 4.1/5\n",
        "  ‚Ä¢ –î–µ—Ç–∞–ª—å–Ω–æ—Å—Ç—å: 4.3/5\n",
        "  üéØ –û–ë–©–ò–ô –ë–ê–õ–õ: 4.2/5.0\n",
        "\n",
        "üìÑ –í–∞–ª–∏–¥–Ω—ã–µ DOI: ['https://doi.org/10.1186/s13072-025-00601-w']\n",
        "    \"\"\")\n",
        "\n",
        "# === –ì–û–¢–û–í–´–ï –ö–û–ú–ê–ù–î–´ ===\n",
        "\n",
        "print(\"\"\"\n",
        "üöÄ –ù–ê–£–ß–ù–û –°–¢–†–û–ì–ò–ô –ì–ï–ù–ï–†–ê–¢–û–† –ì–û–¢–û–í!\n",
        "\n",
        "–ö–æ–º–∞–Ω–¥—ã –¥–ª—è –∑–∞–ø—É—Å–∫–∞:\n",
        "1. scientific_test()                     # –¢–µ—Å—Ç —Å–∏—Å—Ç–µ–º—ã\n",
        "2. compare_quality_metrics()             # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —É–ª—É—á—à–µ–Ω–∏–π\n",
        "3. demonstrate_improvements()            # –î–µ–º–æ —É–ª—É—á—à–µ–Ω–∏–π\n",
        "4. scientific_interactive_mode()         # –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º\n",
        "5. batch_scientific_analysis([...])     # –ü–∞–∫–µ—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑\n",
        "\n",
        "–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å:\n",
        "1) compare_quality_metrics()    # –ü–æ—Å–º–æ—Ç—Ä–µ—Ç—å —É–ª—É—á—à–µ–Ω–∏—è\n",
        "2) scientific_test()           # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Ä–∞–±–æ—Ç—É\n",
        "3) scientific_interactive_mode() # –û—Å–Ω–æ–≤–Ω–∞—è —Ä–∞–±–æ—Ç–∞\n",
        "\"\"\")\n",
        "\n",
        "# –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∫–∞–∑ —É–ª—É—á—à–µ–Ω–∏–π\n",
        "print(\"\\nüéØ –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —É–ª—É—á—à–µ–Ω–∏—è...\")\n",
        "compare_quality_metrics()\n",
        "\n",
        "print(\"\\nüìã –î–ª—è –∑–∞–ø—É—Å–∫–∞ –≤–≤–µ–¥–∏—Ç–µ:\")\n",
        "print(\"scientific_test()  # –∑–∞—Ç–µ–º scientific_interactive_mode()\")\n",
        "\n",
        "# === –ì–û–¢–û–í–´–ï –°–ü–ò–°–ö–ò –î–õ–Ø –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø ===\n",
        "scientific_terms = [\n",
        "    \"aging\", \"senescence\", \"autophagy\",\n",
        "    \"inflammation\", \"mitochondria\", \"telomere\",\n",
        "    \"oxidative stress\", \"dna repair\", \"apoptosis\"\n",
        "]\n",
        "\n",
        "print(f\"\\nüìù –ì–æ—Ç–æ–≤—ã–π —Å–ø–∏—Å–æ–∫ –¥–ª—è –ø–∞–∫–µ—Ç–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞:\")\n",
        "print(\"batch_scientific_analysis(scientific_terms)\")"
      ],
      "metadata": {
        "id": "lJA1oKi5pRuG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d26d956-e2b1-496a-8693-1cd7407a4d05"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üöÄ –ù–ê–£–ß–ù–û –°–¢–†–û–ì–ò–ô –ì–ï–ù–ï–†–ê–¢–û–† –ì–û–¢–û–í!\n",
            "\n",
            "–ö–æ–º–∞–Ω–¥—ã –¥–ª—è –∑–∞–ø—É—Å–∫–∞:\n",
            "1. scientific_test()                     # –¢–µ—Å—Ç —Å–∏—Å—Ç–µ–º—ã\n",
            "2. compare_quality_metrics()             # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —É–ª—É—á—à–µ–Ω–∏–π  \n",
            "3. demonstrate_improvements()            # –î–µ–º–æ —É–ª—É—á—à–µ–Ω–∏–π\n",
            "4. scientific_interactive_mode()         # –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º\n",
            "5. batch_scientific_analysis([...])     # –ü–∞–∫–µ—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑\n",
            "\n",
            "–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å:\n",
            "1) compare_quality_metrics()    # –ü–æ—Å–º–æ—Ç—Ä–µ—Ç—å —É–ª—É—á—à–µ–Ω–∏—è\n",
            "2) scientific_test()           # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Ä–∞–±–æ—Ç—É\n",
            "3) scientific_interactive_mode() # –û—Å–Ω–æ–≤–Ω–∞—è —Ä–∞–±–æ—Ç–∞\n",
            "\n",
            "\n",
            "üéØ –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —É–ª—É—á—à–µ–Ω–∏—è...\n",
            "\n",
            "–ù–û–í–ê–Ø –ù–ê–£–ß–ù–ê–Ø –í–ï–†–°–ò–Ø:\n",
            "  ‚úÖ 5 —á–µ—Ç–∫–∏—Ö –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞ —Å –ø–æ—Ä–æ–≥–∞–º–∏:\n",
            "    ‚Ä¢ Degree centrality (0-2 –±–∞–ª–ª–∞)\n",
            "    ‚Ä¢ PageRank (0-2 –±–∞–ª–ª–∞)\n",
            "    ‚Ä¢ Betweenness centrality (0-2 –±–∞–ª–ª–∞)\n",
            "    ‚Ä¢ –ë–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–∞—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å (0-3 –±–∞–ª–ª–∞)\n",
            "    ‚Ä¢ –ö–∞—á–µ—Å—Ç–≤–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö (0-1 –±–∞–ª–ª)\n",
            "  ‚úÖ –ú–Ω–æ–≥–æ—Ñ–∞–∫—Ç–æ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ (0-5 –ø–æ –∫–∞–∂–¥–æ–º—É):\n",
            "    ‚Ä¢ –ù–æ–≤–∏–∑–Ω–∞\n",
            "    ‚Ä¢ –ù–∞—É—á–Ω–∞—è –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω–æ—Å—Ç—å\n",
            "    ‚Ä¢ –¢–µ—Å—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å\n",
            "    ‚Ä¢ –†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ\n",
            "    ‚Ä¢ –î–µ—Ç–∞–ª—å–Ω–æ—Å—Ç—å\n",
            "  ‚úÖ TF-IDF –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è –≥–∏–ø–æ—Ç–µ–∑\n",
            "  ‚úÖ –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø–æ–¥–≥—Ä–∞—Ñ–∞:\n",
            "    ‚Ä¢ –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—Ç–µ–ø–µ–Ω–µ–π\n",
            "    ‚Ä¢ –ü–ª–æ—Ç–Ω–æ—Å—Ç—å —Å–µ—Ç–∏\n",
            "    ‚Ä¢ –¶–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã–µ —É–∑–ª—ã (hubs)\n",
            "    ‚Ä¢ –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —Å–≤—è–∑–Ω–æ—Å—Ç–∏\n",
            "  ‚úÖ –ö–æ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã:\n",
            "    ‚Ä¢ –†–∞–∑–º–µ—Ä—ã –≤—ã–±–æ—Ä–æ–∫ (n=60-80)\n",
            "    ‚Ä¢ –ö–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã–µ –≥—Ä—É–ø–ø—ã\n",
            "    ‚Ä¢ –û–∂–∏–¥–∞–µ–º—ã–µ —ç—Ñ—Ñ–µ–∫—Ç—ã (20-40%)\n",
            "    ‚Ä¢ –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ —Ç–µ—Å—Ç—ã (ANOVA, p<0.05)\n",
            "    ‚Ä¢ –í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä–∞–º–∫–∏ (12-24 –Ω–µ–¥–µ–ª–∏)\n",
            "\n",
            "üìã –î–ª—è –∑–∞–ø—É—Å–∫–∞ –≤–≤–µ–¥–∏—Ç–µ:\n",
            "scientific_test()  # –∑–∞—Ç–µ–º scientific_interactive_mode()\n",
            "\n",
            "üìù –ì–æ—Ç–æ–≤—ã–π —Å–ø–∏—Å–æ–∫ –¥–ª—è –ø–∞–∫–µ—Ç–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞:\n",
            "batch_scientific_analysis(scientific_terms)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scientific_test()"
      ],
      "metadata": {
        "id": "5hI_Rpc0pR20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8231eaf1-d2b7-4dea-8653-96283de88982"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üß™ –ù–ê–£–ß–ù–´–ô –¢–ï–°–¢ –°–ò–°–¢–ï–ú–´\n",
            "==================================================\n",
            "üî¨ –°–û–ó–î–ê–ù–ò–ï –ù–ê–£–ß–ù–û –°–¢–†–û–ì–û–ì–û –ì–ï–ù–ï–†–ê–¢–û–†–ê\n",
            "============================================================\n",
            "üìä –ì—Ä–∞—Ñ: 32,707 —É–∑–ª–æ–≤, 775,715 —Ä—ë–±–µ—Ä\n",
            "üìÅ –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ: 32,707 –∑–∞–ø–∏—Å–µ–π\n",
            "‚úÖ –ù–∞—É—á–Ω–æ —Å—Ç—Ä–æ–≥–∏–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Å–æ–∑–¥–∞–Ω!\n",
            "\n",
            "–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:\n",
            "  üî¨ –ß–µ—Ç–∫–∏–µ –∫—Ä–∏—Ç–µ—Ä–∏–∏ –∫–∞—á–µ—Å—Ç–≤–∞ —Ç–µ—Ä–º–∏–Ω–æ–≤ (5 –º–µ—Ç—Ä–∏–∫)\n",
            "  üìä –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø–æ–¥–≥—Ä–∞—Ñ–∞\n",
            "  üö´ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è –≥–∏–ø–æ—Ç–µ–∑\n",
            "  üìà –ú–Ω–æ–≥–æ—Ñ–∞–∫—Ç–æ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ (5 –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤)\n",
            "  üß™ –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã\n",
            "\n",
            "üî¨ –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ç–µ—Ä–º–∏–Ω: aging\n",
            "‚è≥ –≠—Ç–æ –∑–∞–π–º–µ—Ç 60-120 —Å–µ–∫—É–Ω–¥...\n",
            "================================================================================\n",
            "üß¨ –ù–ê–£–ß–ù–û –°–¢–†–û–ì–ò–ô –ê–ù–ê–õ–ò–ó: AGING\n",
            "================================================================================\n",
            "üîç –ü–æ–∏—Å–∫ —Ç–µ—Ä–º–∏–Ω–∞ 'aging'...\n",
            "‚úÖ –ù–∞–π–¥–µ–Ω: protein-bound cml levels[168]>aging\n",
            "üï∏Ô∏è –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø–æ–¥–≥—Ä–∞—Ñ–∞ (—Ä–∞–¥–∏—É—Å: 2, –º–∏–Ω: 25, –º–∞–∫—Å: 150)...\n",
            "  –®–∞–≥ 1: +21 —É–∑–ª–æ–≤ (–≤—Å–µ–≥–æ: 22)\n",
            "  –®–∞–≥ 2: +16085 —É–∑–ª–æ–≤ (–≤—Å–µ–≥–æ: 16085)\n",
            "‚úÖ –ü–æ–¥–≥—Ä–∞—Ñ: 16085 —É–∑–ª–æ–≤, 633856 —Ä—ë–±–µ—Ä\n",
            "üèóÔ∏è –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø–æ–¥–≥—Ä–∞—Ñ–∞...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GKiu-7DNoBxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1eW0GmLvBLNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QAk9s6lzBLRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import time\n",
        "\n",
        "class TermSelector:\n",
        "    def __init__(self, G, node_full_meta):\n",
        "        \"\"\"\n",
        "        –°–µ–ª–µ–∫—Ç–æ—Ä —Ç–µ—Ä–º–∏–Ω–æ–≤ —Å –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏–µ–π\n",
        "        \"\"\"\n",
        "        self.G = G\n",
        "        self.node_full_meta = node_full_meta\n",
        "\n",
        "        # –°–æ–∑–¥–∞–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Ç–µ—Ä–º–∏–Ω–æ–≤\n",
        "        self.term_categories = self._build_term_categories()\n",
        "\n",
        "    def _build_term_categories(self):\n",
        "        \"\"\"–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–π —Ç–µ—Ä–º–∏–Ω–æ–≤ –∏–∑ –≥—Ä–∞—Ñ–∞\"\"\"\n",
        "        print(\"üîç –ê–Ω–∞–ª–∏–∑ —Ç–µ—Ä–º–∏–Ω–æ–≤ –≤ –≥—Ä–∞—Ñ–µ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π...\")\n",
        "\n",
        "        categories = {\n",
        "            'üß¨ –ü—Ä–æ—Ü–µ—Å—Å—ã —Å—Ç–∞—Ä–µ–Ω–∏—è': {\n",
        "                'keywords': ['aging', 'senescence', 'longevity', 'lifespan', 'mortality'],\n",
        "                'terms': []\n",
        "            },\n",
        "            'üî¨ –ú–æ–ª–µ–∫—É–ª—è—Ä–Ω—ã–µ –º–µ—Ö–∞–Ω–∏–∑–º—ã': {\n",
        "                'keywords': ['autophagy', 'apoptosis', 'dna repair', 'transcription', 'translation'],\n",
        "                'terms': []\n",
        "            },\n",
        "            '‚ö° –ö–ª–µ—Ç–æ—á–Ω—ã–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã': {\n",
        "                'keywords': ['mitochondria', 'nucleus', 'ribosome', 'membrane', 'cytoplasm'],\n",
        "                'terms': []\n",
        "            },\n",
        "            'üî• –í–æ—Å–ø–∞–ª–µ–Ω–∏–µ –∏ —Å—Ç—Ä–µ—Å—Å': {\n",
        "                'keywords': ['inflammation', 'oxidative', 'stress', 'immune', 'cytokine'],\n",
        "                'terms': []\n",
        "            },\n",
        "            'üß™ –ë–µ–ª–∫–∏ –∏ —Ñ–µ—Ä–º–µ–Ω—Ç—ã': {\n",
        "                'keywords': ['protein', 'enzyme', 'kinase', 'phosphatase', 'receptor'],\n",
        "                'terms': []\n",
        "            },\n",
        "            'üè• –ó–∞–±–æ–ª–µ–≤–∞–Ω–∏—è': {\n",
        "                'keywords': ['cancer', 'diabetes', 'alzheimer', 'parkinson', 'cardiovascular'],\n",
        "                'terms': []\n",
        "            },\n",
        "            'üß† –°–∏—Å—Ç–µ–º—ã –æ—Ä–≥–∞–Ω–∏–∑–º–∞': {\n",
        "                'keywords': ['neural', 'brain', 'heart', 'liver', 'muscle', 'bone'],\n",
        "                'terms': []\n",
        "            },\n",
        "            'üîÄ –°–∏–≥–Ω–∞–ª—å–Ω—ã–µ –ø—É—Ç–∏': {\n",
        "                'keywords': ['signaling', 'pathway', 'cascade', 'regulation', 'response'],\n",
        "                'terms': []\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ–º —Ç–µ—Ä–º–∏–Ω—ã\n",
        "        for node in self.G.nodes():\n",
        "            if self._is_quality_term(node):\n",
        "                categorized = False\n",
        "                node_lower = node.lower()\n",
        "\n",
        "                for category, data in categories.items():\n",
        "                    for keyword in data['keywords']:\n",
        "                        if keyword in node_lower:\n",
        "                            data['terms'].append(node)\n",
        "                            categorized = True\n",
        "                            break\n",
        "                    if categorized:\n",
        "                        break\n",
        "\n",
        "        # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Ç–µ—Ä–º–∏–Ω—ã –≤ –∫–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –ø–æ –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏ (—Å—Ç–µ–ø–µ–Ω—å —É–∑–ª–∞)\n",
        "        for category, data in categories.items():\n",
        "            terms_with_degree = [(term, self.G.degree(term)) for term in data['terms']]\n",
        "            terms_with_degree.sort(key=lambda x: x[1], reverse=True)\n",
        "            data['terms'] = [term for term, _ in terms_with_degree[:15]]  # –¢–æ–ø-15 –≤ –∫–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏\n",
        "\n",
        "        print(f\"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(categories)} –∫–∞—Ç–µ–≥–æ—Ä–∏–π —Ç–µ—Ä–º–∏–Ω–æ–≤\")\n",
        "        return categories\n",
        "\n",
        "    def _is_quality_term(self, term):\n",
        "        \"\"\"–ë–∞–∑–æ–≤–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ —Ç–µ—Ä–º–∏–Ω–∞\"\"\"\n",
        "        if len(term) < 3 or len(term) > 40:\n",
        "            return False\n",
        "        if self.G.degree(term) < 2:\n",
        "            return False\n",
        "        # –ò—Å–∫–ª—é—á–∞–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã\n",
        "        exclude_patterns = ['aging\\\\d+', 'figure', 'table', 'mouse.*muscle']\n",
        "        for pattern in exclude_patterns:\n",
        "            if re.search(pattern, term.lower()):\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    def show_categories(self):\n",
        "        \"\"\"–ü–æ–∫–∞–∑–∞—Ç—å –≤—Å–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Ç–µ—Ä–º–∏–Ω–æ–≤\"\"\"\n",
        "        print(\"\\nüéØ –ö–ê–¢–ï–ì–û–†–ò–ò –¢–ï–†–ú–ò–ù–û–í –î–õ–Ø –ê–ù–ê–õ–ò–ó–ê\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        for i, (category, data) in enumerate(self.term_categories.items(), 1):\n",
        "            print(f\"\\n{i}. {category} ({len(data['terms'])} —Ç–µ—Ä–º–∏–Ω–æ–≤)\")\n",
        "\n",
        "            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ø-5 —Ç–µ—Ä–º–∏–Ω–æ–≤ –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏\n",
        "            top_terms = data['terms'][:5]\n",
        "            if top_terms:\n",
        "                print(f\"   –¢–æ–ø-5: {', '.join(top_terms)}\")\n",
        "            else:\n",
        "                print(\"   –¢–µ—Ä–º–∏–Ω—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã\")\n",
        "\n",
        "        print(f\"\\n0. ‚úçÔ∏è  –í–≤–µ—Å—Ç–∏ —Ç–µ—Ä–º–∏–Ω –≤—Ä—É—á–Ω—É—é\")\n",
        "        print(f\"99. üé≤ –°–ª—É—á–∞–π–Ω—ã–π —Ç–µ—Ä–º–∏–Ω\")\n",
        "\n",
        "    def get_category_terms(self, category_num):\n",
        "        \"\"\"–ü–æ–ª—É—á–∏—Ç—å —Ç–µ—Ä–º–∏–Ω—ã –∏–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏\"\"\"\n",
        "        categories_list = list(self.term_categories.items())\n",
        "\n",
        "        if 1 <= category_num <= len(categories_list):\n",
        "            category_name, data = categories_list[category_num - 1]\n",
        "            return category_name, data['terms']\n",
        "\n",
        "        return None, []\n",
        "\n",
        "    def show_category_terms(self, category_num):\n",
        "        \"\"\"–ü–æ–∫–∞–∑–∞—Ç—å —Ç–µ—Ä–º–∏–Ω—ã –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏\"\"\"\n",
        "        category_name, terms = self.get_category_terms(category_num)\n",
        "\n",
        "        if not terms:\n",
        "            print(\"‚ùå –ö–∞—Ç–µ–≥–æ—Ä–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –∏–ª–∏ –ø—É—Å—Ç–∞\")\n",
        "            return None\n",
        "\n",
        "        print(f\"\\nüìã {category_name}\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        for i, term in enumerate(terms, 1):\n",
        "            degree = self.G.degree(term)\n",
        "            print(f\"{i:2}. {term:<30} (—Å–≤—è–∑–µ–π: {degree})\")\n",
        "\n",
        "        print(f\"\\n0. ‚¨ÖÔ∏è  –ù–∞–∑–∞–¥ –∫ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º\")\n",
        "        return terms\n",
        "\n",
        "    def get_random_term(self):\n",
        "        \"\"\"–ü–æ–ª—É—á–∏—Ç—å —Å–ª—É—á–∞–π–Ω—ã–π –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π —Ç–µ—Ä–º–∏–Ω\"\"\"\n",
        "        all_terms = []\n",
        "        for data in self.term_categories.values():\n",
        "            all_terms.extend(data['terms'])\n",
        "\n",
        "        if all_terms:\n",
        "            return random.choice(all_terms)\n",
        "        else:\n",
        "            # –ï—Å–ª–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –ø—É—Å—Ç—ã, –±–µ—Ä–µ–º —Å–ª—É—á–∞–π–Ω—ã–π –∏–∑ —Ç–æ–ø-100 –ø–æ —Å—Ç–µ–ø–µ–Ω–∏\n",
        "            degrees = [(node, self.G.degree(node)) for node in self.G.nodes() if self._is_quality_term(node)]\n",
        "            degrees.sort(key=lambda x: x[1], reverse=True)\n",
        "            top_terms = [term for term, _ in degrees[:100]]\n",
        "            return random.choice(top_terms) if top_terms else None\n",
        "\n",
        "class MultiHypothesisGenerator:\n",
        "    def __init__(self, scientific_generator):\n",
        "        \"\"\"\n",
        "        –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –≥–∏–ø–æ—Ç–µ–∑\n",
        "        \"\"\"\n",
        "        self.generator = scientific_generator\n",
        "        self.hypothesis_history = {}  # –ò—Å—Ç–æ—Ä–∏—è —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≥–∏–ø–æ—Ç–µ–∑\n",
        "\n",
        "    def generate_additional_hypotheses(self, search_term, quality_terms, round_number=2, count=5):\n",
        "        \"\"\"\n",
        "        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –≥–∏–ø–æ—Ç–µ–∑ —Å –≤–∞—Ä–∏–∞—Ü–∏–µ–π –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n",
        "        \"\"\"\n",
        "        print(f\"\\nüí° –ì–ï–ù–ï–†–ê–¶–ò–Ø –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–• –ì–ò–ü–û–¢–ï–ó (—Ä–∞—É–Ω–¥ {round_number})\")\n",
        "        print(f\"üéØ –¶–µ–ª—å: {count} –Ω–æ–≤—ã—Ö —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –≥–∏–ø–æ—Ç–µ–∑ –¥–ª—è '{search_term}'\")\n",
        "        print(\"-\" * 70)\n",
        "\n",
        "        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –≤–∞—Ä–∏–∞—Ü–∏–∏\n",
        "        temperature_variations = [0.7, 0.8, 0.9]\n",
        "        focus_variations = [\n",
        "            \"–º–æ–ª–µ–∫—É–ª—è—Ä–Ω—ã–µ –º–µ—Ö–∞–Ω–∏–∑–º—ã\",\n",
        "            \"–∫–ª–µ—Ç–æ—á–Ω—ã–µ –ø—Ä–æ—Ü–µ—Å—Å—ã\",\n",
        "            \"—Å–∏—Å—Ç–µ–º–Ω—É—é –±–∏–æ–ª–æ–≥–∏—é\",\n",
        "            \"–ø–∞—Ç–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è\",\n",
        "            \"—Ç–µ—Ä–∞–ø–µ–≤—Ç–∏—á–µ—Å–∫–∏–µ –º–∏—à–µ–Ω–∏\"\n",
        "        ]\n",
        "\n",
        "        # –í—ã–±–∏—Ä–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —ç—Ç–æ–≥–æ —Ä–∞—É–Ω–¥–∞\n",
        "        temperature = temperature_variations[(round_number - 1) % len(temperature_variations)]\n",
        "        focus = focus_variations[(round_number - 1) % len(focus_variations)]\n",
        "\n",
        "        print(f\"üîß –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Ä–∞—É–Ω–¥–∞ {round_number}:\")\n",
        "        print(f\"   ‚Ä¢ –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞: {temperature} (–∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç—å)\")\n",
        "        print(f\"   ‚Ä¢ –§–æ–∫—É—Å: {focus}\")\n",
        "        print(f\"   ‚Ä¢ –ò–∑–±–µ–≥–∞–µ–º –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –≥–∏–ø–æ—Ç–µ–∑\")\n",
        "\n",
        "        # –ü–æ–ª—É—á–∞–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Ä–∞—É–Ω–¥–æ–≤\n",
        "        used_terms = set()\n",
        "        if search_term in self.hypothesis_history:\n",
        "            for prev_hyps in self.hypothesis_history[search_term]:\n",
        "                for hyp in prev_hyps.get('hypotheses', []):\n",
        "                    used_terms.update(hyp.get('terms', []))\n",
        "\n",
        "        # –§–∏–ª—å—Ç—Ä—É–µ–º –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã, –∏—Å–∫–ª—é—á–∞—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ\n",
        "        fresh_terms = [term for term in quality_terms if term not in used_terms]\n",
        "        if len(fresh_terms) < 5:\n",
        "            fresh_terms = quality_terms  # –ï—Å–ª–∏ —Å–≤–µ–∂–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤ –º–∞–ª–æ, –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ\n",
        "\n",
        "        print(f\"üîÑ –î–æ—Å—Ç—É–ø–Ω–æ —Å–≤–µ–∂–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤: {len(fresh_terms)}\")\n",
        "\n",
        "        # –°–æ–∑–¥–∞–µ–º —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç\n",
        "        specialized_prompt = self._create_specialized_prompt(\n",
        "            search_term, fresh_terms, focus, round_number, count\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            response = self.generator.client.chat.completions.create(\n",
        "                model=\"llama3.1\",\n",
        "                messages=[{\"role\": \"user\", \"content\": specialized_prompt}],\n",
        "                temperature=temperature,\n",
        "                max_tokens=3000\n",
        "            )\n",
        "\n",
        "            response_text = response.choices[0].message.content.strip()\n",
        "\n",
        "            # –ü–∞—Ä—Å–∏–Ω–≥ JSON\n",
        "            try:\n",
        "                hypotheses_data = json.loads(response_text)\n",
        "            except json.JSONDecodeError:\n",
        "                start_idx = response_text.find('{')\n",
        "                end_idx = response_text.rfind('}') + 1\n",
        "                if start_idx != -1 and end_idx != -1:\n",
        "                    json_text = response_text[start_idx:end_idx]\n",
        "                    hypotheses_data = json.loads(json_text)\n",
        "                else:\n",
        "                    raise ValueError(\"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å JSON\")\n",
        "\n",
        "            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é\n",
        "            if search_term not in self.hypothesis_history:\n",
        "                self.hypothesis_history[search_term] = []\n",
        "            self.hypothesis_history[search_term].append(hypotheses_data)\n",
        "\n",
        "            print(f\"‚úÖ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(hypotheses_data.get('hypotheses', []))} –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –≥–∏–ø–æ—Ç–µ–∑\")\n",
        "\n",
        "            return hypotheses_data\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –≥–∏–ø–æ—Ç–µ–∑: {e}\")\n",
        "            return None\n",
        "\n",
        "    def _create_specialized_prompt(self, search_term, fresh_terms, focus, round_number, count):\n",
        "        \"\"\"–°–æ–∑–¥–∞–Ω–∏–µ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞\"\"\"\n",
        "\n",
        "        focus_instructions = {\n",
        "            \"–º–æ–ª–µ–∫—É–ª—è—Ä–Ω—ã–µ –º–µ—Ö–∞–Ω–∏–∑–º—ã\": \"–°–æ—Å—Ä–µ–¥–æ—Ç–æ—á—å—Å—è –Ω–∞ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è—Ö –±–µ–ª–∫–æ–≤, –≥–µ–Ω–Ω–æ–π —Ä–µ–≥—É–ª—è—Ü–∏–∏, —Å–∏–≥–Ω–∞–ª—å–Ω—ã—Ö –∫–∞—Å–∫–∞–¥–∞—Ö\",\n",
        "            \"–∫–ª–µ—Ç–æ—á–Ω—ã–µ –ø—Ä–æ—Ü–µ—Å—Å—ã\": \"–ê–∫—Ü–µ–Ω—Ç–∏—Ä—É–π –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ –ø—Ä–æ—Ü–µ—Å—Å–∞—Ö –≤ –∫–ª–µ—Ç–∫–µ: –æ—Ä–≥–∞–Ω–µ–ª–ª—ã, —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç, –¥–µ–ª–µ–Ω–∏–µ\",\n",
        "            \"—Å–∏—Å—Ç–µ–º–Ω—É—é –±–∏–æ–ª–æ–≥–∏—é\": \"–†–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–π –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ –º–µ–∂–¥—É –æ—Ä–≥–∞–Ω–∞–º–∏ –∏ —Å–∏—Å—Ç–µ–º–∞–º–∏ –æ—Ä–≥–∞–Ω–∏–∑–º–∞\",\n",
        "            \"–ø–∞—Ç–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è\": \"–§–æ–∫—É—Å–∏—Ä—É–π—Å—è –Ω–∞ –±–æ–ª–µ–∑–Ω–µ–Ω–Ω—ã—Ö –ø—Ä–æ—Ü–µ—Å—Å–∞—Ö –∏ –¥–∏—Å—Ñ—É–Ω–∫—Ü–∏—è—Ö\",\n",
        "            \"—Ç–µ—Ä–∞–ø–µ–≤—Ç–∏—á–µ—Å–∫–∏–µ –º–∏—à–µ–Ω–∏\": \"–î—É–º–∞–π –æ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç–æ—á–∫–∞—Ö –≤–º–µ—à–∞—Ç–µ–ª—å—Å—Ç–≤–∞ –∏ –ª–µ—á–µ–Ω–∏—è\"\n",
        "        }\n",
        "\n",
        "        return f\"\"\"–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –±–∏–æ–ª–æ–≥–∏–∏ —Å—Ç–∞—Ä–µ–Ω–∏—è. –≠—Ç–æ —Ä–∞—É–Ω–¥ {round_number} –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≥–∏–ø–æ—Ç–µ–∑.\n",
        "\n",
        "–ó–ê–î–ê–ß–ê: –°–≥–µ–Ω–µ—Ä–∏—Ä—É–π {count} –ù–û–í–´–• –≥–∏–ø–æ—Ç–µ–∑ –æ —Ä–æ–ª–∏ \"{search_term}\" –≤ —Å—Ç–∞—Ä–µ–Ω–∏–∏.\n",
        "\n",
        "–°–ü–ï–¶–ò–ê–õ–¨–ù–´–ô –§–û–ö–£–° –†–ê–£–ù–î–ê {round_number}: {focus_instructions.get(focus, focus)}\n",
        "\n",
        "–î–û–°–¢–£–ü–ù–´–ï –¢–ï–†–ú–ò–ù–´ (–∏—Å–ø–æ–ª—å–∑—É–π –†–ê–ó–ù–´–ï –∏–∑ —ç—Ç–æ–≥–æ —Å–ø–∏—Å–∫–∞):\n",
        "{fresh_terms[:15]}\n",
        "\n",
        "–ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï –¢–†–ï–ë–û–í–ê–ù–ò–Ø:\n",
        "1. –ö–∞–∂–¥–∞—è –≥–∏–ø–æ—Ç–µ–∑–∞ –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û –≤–∫–ª—é—á–∞–µ—Ç \"{search_term}\"\n",
        "2. –ò—Å–ø–æ–ª—å–∑—É–π –†–ê–ó–ù–´–ï –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ —Ç–µ—Ä–º–∏–Ω–æ–≤ (–∏–∑–±–µ–≥–∞–π –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π)\n",
        "3. –§–æ–∫—É—Å –Ω–∞: {focus}\n",
        "4. –ü—Ä–µ–¥–ª–∞–≥–∞–π –û–†–ò–ì–ò–ù–ê–õ–¨–ù–´–ï –º–µ—Ö–∞–Ω–∏–∑–º—ã (–Ω–µ –±–∞–Ω–∞–ª—å–Ω—ã–µ)\n",
        "5. –ì–∏–ø–æ—Ç–µ–∑—ã –¥–æ–ª–∂–Ω—ã –æ—Ç–ª–∏—á–∞—Ç—å—Å—è –æ—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Ä–∞—É–Ω–¥–æ–≤\n",
        "\n",
        "–§–û–†–ú–ê–¢ JSON:\n",
        "{{\n",
        "  \"round\": {round_number},\n",
        "  \"focus\": \"{focus}\",\n",
        "  \"search_term\": \"{search_term}\",\n",
        "  \"hypotheses\": [\n",
        "    {{\n",
        "      \"terms\": [\"{search_term}\", \"–Ω–æ–≤—ã–π_—Ç–µ—Ä–º–∏–Ω1\", \"–Ω–æ–≤—ã–π_—Ç–µ—Ä–º–∏–Ω2\"],\n",
        "      \"hypothesis\": \"–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è –≥–∏–ø–æ—Ç–µ–∑–∞ —Å —Ñ–æ–∫—É—Å–æ–º –Ω–∞ {focus}\",\n",
        "      \"rationale\": \"–ù–∞—É—á–Ω–æ–µ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ —Å —É–ø–æ—Ä–æ–º –Ω–∞ {focus}\",\n",
        "      \"validation\": \"–ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏\"\n",
        "    }}\n",
        "  ]\n",
        "}}\n",
        "\n",
        "–ì–µ–Ω–µ—Ä–∏—Ä—É–π –¢–û–õ–¨–ö–û JSON –±–µ–∑ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤!\"\"\"\n",
        "\n",
        "    def show_all_hypotheses(self, search_term):\n",
        "        \"\"\"–ü–æ–∫–∞–∑–∞—Ç—å –≤—Å–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≥–∏–ø–æ—Ç–µ–∑—ã –¥–ª—è —Ç–µ—Ä–º–∏–Ω–∞\"\"\"\n",
        "        if search_term not in self.hypothesis_history:\n",
        "            print(f\"‚ùå –ù–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≥–∏–ø–æ—Ç–µ–∑ –¥–ª—è '{search_term}'\")\n",
        "            return\n",
        "\n",
        "        print(f\"\\nüìö –í–°–ï –ì–ò–ü–û–¢–ï–ó–´ –î–õ–Ø '{search_term.upper()}'\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        total_hypotheses = 0\n",
        "        for round_num, hyp_data in enumerate(self.hypothesis_history[search_term], 1):\n",
        "            hypotheses = hyp_data.get('hypotheses', [])\n",
        "            focus = hyp_data.get('focus', f'—Ä–∞—É–Ω–¥ {round_num}')\n",
        "\n",
        "            print(f\"\\nüîÑ –†–ê–£–ù–î {round_num}: {focus} ({len(hypotheses)} –≥–∏–ø–æ—Ç–µ–∑)\")\n",
        "            print(\"-\" * 70)\n",
        "\n",
        "            for i, hyp in enumerate(hypotheses, 1):\n",
        "                global_num = total_hypotheses + i\n",
        "                print(f\"\\n{global_num}. –ì–ò–ü–û–¢–ï–ó–ê:\")\n",
        "                print(f\"   üß¨ –¢–µ—Ä–º–∏–Ω—ã: {', '.join(hyp.get('terms', []))}\")\n",
        "                print(f\"   üî¨ –ú–µ—Ö–∞–Ω–∏–∑–º: {hyp.get('hypothesis', '')}\")\n",
        "                print(f\"   üìö –û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ: {hyp.get('rationale', '')}\")\n",
        "                print(f\"   üß™ –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç: {hyp.get('validation', '')}\")\n",
        "                print(\"-\" * 40)\n",
        "\n",
        "            total_hypotheses += len(hypotheses)\n",
        "\n",
        "        print(f\"\\nüìä –ò–¢–û–ì–û: {total_hypotheses} –≥–∏–ø–æ—Ç–µ–∑ –≤ {len(self.hypothesis_history[search_term])} —Ä–∞—É–Ω–¥–∞—Ö\")\n",
        "\n",
        "def enhanced_interactive_mode():\n",
        "    \"\"\"\n",
        "    –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º —Å –≤—ã–±–æ—Ä–æ–º —Ç–µ—Ä–º–∏–Ω–æ–≤ –∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –≥–∏–ø–æ—Ç–µ–∑–∞–º–∏\n",
        "    \"\"\"\n",
        "    print(\"üöÄ –†–ê–°–®–ò–†–ï–ù–ù–´–ô –ò–ù–¢–ï–†–ê–ö–¢–ò–í–ù–´–ô –†–ï–ñ–ò–ú\")\n",
        "    print(\"=\" * 80)\n",
        "    print(\"–ù–æ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:\")\n",
        "    print(\"  üéØ –í—ã–±–æ—Ä —Ç–µ—Ä–º–∏–Ω–æ–≤ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º\")\n",
        "    print(\"  ‚ûï –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –≥–∏–ø–æ—Ç–µ–∑\")\n",
        "    print(\"  üìö –ü—Ä–æ—Å–º–æ—Ç—Ä –≤—Å–µ—Ö –≥–∏–ø–æ—Ç–µ–∑\")\n",
        "    print(\"  üé≤ –°–ª—É—á–∞–π–Ω—ã–π –≤—ã–±–æ—Ä —Ç–µ—Ä–º–∏–Ω–æ–≤\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # –°–æ–∑–¥–∞–µ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã\n",
        "    try:\n",
        "        term_selector = TermSelector(G, node_full_meta)\n",
        "\n",
        "        # –°–æ–∑–¥–∞–µ–º –Ω–∞—É—á–Ω—ã–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä\n",
        "        from openai import OpenAI\n",
        "        client = OpenAI(base_url=\"http://80.209.242.40:8000/v1\", api_key=\"dummy-key\")\n",
        "        scientific_generator = ScientificHypothesisGenerator(G, node_full_meta, edge_full_meta, client)\n",
        "\n",
        "        multi_generator = MultiHypothesisGenerator(scientific_generator)\n",
        "\n",
        "    except NameError:\n",
        "        print(\"‚ùå –°–Ω–∞—á–∞–ª–∞ —Å–æ–∑–¥–∞–π—Ç–µ –≥—Ä–∞—Ñ –∏ –Ω–∞—É—á–Ω—ã–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä\")\n",
        "        return\n",
        "\n",
        "    current_term = None\n",
        "    current_quality_terms = []\n",
        "    analysis_count = 0\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            if current_term is None:\n",
        "                # –†–µ–∂–∏–º –≤—ã–±–æ—Ä–∞ —Ç–µ—Ä–º–∏–Ω–∞\n",
        "                print(f\"\\n{'üéØ' * 20}\")\n",
        "                print(\"–í–´–ë–û–† –¢–ï–†–ú–ò–ù–ê –î–õ–Ø –ê–ù–ê–õ–ò–ó–ê\")\n",
        "                print(f\"{'üéØ' * 20}\")\n",
        "\n",
        "                term_selector.show_categories()\n",
        "\n",
        "                try:\n",
        "                    choice = input(\"\\nüëÜ –í—ã–±–µ—Ä–∏—Ç–µ –∫–∞—Ç–µ–≥–æ—Ä–∏—é (1-8), 0 –¥–ª—è —Ä—É—á–Ω–æ–≥–æ –≤–≤–æ–¥–∞, 99 –¥–ª—è —Å–ª—É—á–∞–π–Ω–æ–≥–æ: \").strip()\n",
        "\n",
        "                    if choice == '0':\n",
        "                        # –†—É—á–Ω–æ–π –≤–≤–æ–¥\n",
        "                        manual_term = input(\"‚úçÔ∏è –í–≤–µ–¥–∏—Ç–µ —Ç–µ—Ä–º–∏–Ω: \").strip()\n",
        "                        if manual_term:\n",
        "                            current_term = manual_term\n",
        "                        continue\n",
        "\n",
        "                    elif choice == '99':\n",
        "                        # –°–ª—É—á–∞–π–Ω—ã–π —Ç–µ—Ä–º–∏–Ω\n",
        "                        random_term = term_selector.get_random_term()\n",
        "                        if random_term:\n",
        "                            print(f\"üé≤ –í—ã–±—Ä–∞–Ω —Å–ª—É—á–∞–π–Ω—ã–π —Ç–µ—Ä–º–∏–Ω: {random_term}\")\n",
        "                            current_term = random_term\n",
        "                        else:\n",
        "                            print(\"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–±—Ä–∞—Ç—å —Å–ª—É—á–∞–π–Ω—ã–π —Ç–µ—Ä–º–∏–Ω\")\n",
        "                        continue\n",
        "\n",
        "                    elif choice.lower() in ['quit', 'exit', 'q']:\n",
        "                        print(\"üëã –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!\")\n",
        "                        return\n",
        "\n",
        "                    else:\n",
        "                        # –í—ã–±–æ—Ä –∫–∞—Ç–µ–≥–æ—Ä–∏–∏\n",
        "                        category_num = int(choice)\n",
        "                        terms = term_selector.show_category_terms(category_num)\n",
        "\n",
        "                        if terms:\n",
        "                            term_choice = input(f\"\\nüëÜ –í—ã–±–µ—Ä–∏—Ç–µ —Ç–µ—Ä–º–∏–Ω (1-{len(terms)}) –∏–ª–∏ 0 –¥–ª—è –≤–æ–∑–≤—Ä–∞—Ç–∞: \").strip()\n",
        "\n",
        "                            if term_choice == '0':\n",
        "                                continue\n",
        "\n",
        "                            term_idx = int(term_choice) - 1\n",
        "                            if 0 <= term_idx < len(terms):\n",
        "                                current_term = terms[term_idx]\n",
        "                                print(f\"‚úÖ –í—ã–±—Ä–∞–Ω —Ç–µ—Ä–º–∏–Ω: {current_term}\")\n",
        "\n",
        "                except (ValueError, IndexError):\n",
        "                    print(\"‚ö†Ô∏è –ù–µ–≤–µ—Ä–Ω—ã–π –≤—ã–±–æ—Ä, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–Ω–æ–≤–∞\")\n",
        "                    continue\n",
        "\n",
        "            else:\n",
        "                # –†–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã —Å –≤—ã–±—Ä–∞–Ω–Ω—ã–º —Ç–µ—Ä–º–∏–Ω–æ–º\n",
        "                print(f\"\\n{'üî¨' * 25}\")\n",
        "                print(f\"–†–ê–ë–û–¢–ê –° –¢–ï–†–ú–ò–ù–û–ú: {current_term.upper()}\")\n",
        "                print(f\"{'üî¨' * 25}\")\n",
        "\n",
        "                print(\"–í—ã–±–µ—Ä–∏—Ç–µ –¥–µ–π—Å—Ç–≤–∏–µ:\")\n",
        "                print(\"1. üß¨ –ü–µ—Ä–≤–∏—á–Ω—ã–π –∞–Ω–∞–ª–∏–∑ (—Å—Ç—Ä—É–∫—Ç—É—Ä–∞ + 5 –≥–∏–ø–æ—Ç–µ–∑)\")\n",
        "                print(\"2. ‚ûï –ï—â–µ 5 –≥–∏–ø–æ—Ç–µ–∑ (–Ω–æ–≤—ã–π —Ä–∞—É–Ω–¥)\")\n",
        "                print(\"3. üìö –ü–æ–∫–∞–∑–∞—Ç—å –≤—Å–µ –≥–∏–ø–æ—Ç–µ–∑—ã\")\n",
        "                print(\"4. üîÑ –í—ã–±—Ä–∞—Ç—å –Ω–æ–≤—ã–π —Ç–µ—Ä–º–∏–Ω\")\n",
        "                print(\"5. ‚ùå –í—ã—Ö–æ–¥\")\n",
        "\n",
        "                action = input(\"\\nüëÜ –í—ã–±–µ—Ä–∏—Ç–µ –¥–µ–π—Å—Ç–≤–∏–µ (1-5): \").strip()\n",
        "\n",
        "                if action == '1':\n",
        "                    # –ü–µ—Ä–≤–∏—á–Ω—ã–π –∞–Ω–∞–ª–∏–∑\n",
        "                    analysis_count += 1\n",
        "                    print(f\"\\n{'üß¨' * 30}\")\n",
        "                    print(f\"–ü–ï–†–í–ò–ß–ù–´–ô –ê–ù–ê–õ–ò–ó #{analysis_count}: {current_term.upper()}\")\n",
        "                    print(f\"{'üß¨' * 30}\")\n",
        "                    print(\"‚è≥ –ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∑–∞–π–º–µ—Ç 90-180 —Å–µ–∫—É–Ω–¥...\")\n",
        "\n",
        "                    try:\n",
        "                        result = scientific_generator.analyze_term_scientific(current_term, radius=3)\n",
        "\n",
        "                        if result:\n",
        "                            current_quality_terms = result['quality_terms']\n",
        "                            print(f\"\\n‚úÖ –ü–µ—Ä–≤–∏—á–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω!\")\n",
        "                            print(f\"üî¨ –ù–∞–π–¥–µ–Ω–æ {len(current_quality_terms)} –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤\")\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"‚ùå –û—à–∏–±–∫–∞ –ø–µ—Ä–≤–∏—á–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞: {e}\")\n",
        "\n",
        "                elif action == '2':\n",
        "                    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –≥–∏–ø–æ—Ç–µ–∑—ã\n",
        "                    if not current_quality_terms:\n",
        "                        print(\"‚ö†Ô∏è –°–Ω–∞—á–∞–ª–∞ –≤—ã–ø–æ–ª–Ω–∏—Ç–µ –ø–µ—Ä–≤–∏—á–Ω—ã–π –∞–Ω–∞–ª–∏–∑ (–ø—É–Ω–∫—Ç 1)\")\n",
        "                        continue\n",
        "\n",
        "                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω–æ–º–µ—Ä —Ä–∞—É–Ω–¥–∞\n",
        "                    rounds_count = len(multi_generator.hypothesis_history.get(current_term, [])) + 1\n",
        "\n",
        "                    print(f\"\\n‚ûï –ì–ï–ù–ï–†–ê–¶–ò–Ø –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–• –ì–ò–ü–û–¢–ï–ó\")\n",
        "                    print(\"‚è≥ –≠—Ç–æ –∑–∞–π–º–µ—Ç 30-60 —Å–µ–∫—É–Ω–¥...\")\n",
        "\n",
        "                    try:\n",
        "                        additional_hyps = multi_generator.generate_additional_hypotheses(\n",
        "                            current_term, current_quality_terms, rounds_count, 5\n",
        "                        )\n",
        "\n",
        "                        if additional_hyps:\n",
        "                            print(f\"\\n‚úÖ –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –≥–∏–ø–æ—Ç–µ–∑—ã —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã!\")\n",
        "\n",
        "                            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –Ω–æ–≤—ã–µ –≥–∏–ø–æ—Ç–µ–∑—ã\n",
        "                            print(f\"\\nüí° –ù–û–í–´–ï –ì–ò–ü–û–¢–ï–ó–´ (—Ä–∞—É–Ω–¥ {rounds_count}):\")\n",
        "                            print(\"=\" * 60)\n",
        "\n",
        "                            for i, hyp in enumerate(additional_hyps.get('hypotheses', []), 1):\n",
        "                                print(f\"\\n{i}. üß¨ –¢–µ—Ä–º–∏–Ω—ã: {', '.join(hyp.get('terms', []))}\")\n",
        "                                print(f\"   üî¨ –ú–µ—Ö–∞–Ω–∏–∑–º: {hyp.get('hypothesis', '')}\")\n",
        "                                print(f\"   üìö –û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ: {hyp.get('rationale', '')}\")\n",
        "                                print(\"-\" * 60)\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –≥–∏–ø–æ—Ç–µ–∑: {e}\")\n",
        "\n",
        "                elif action == '3':\n",
        "                    # –ü–æ–∫–∞–∑–∞—Ç—å –≤—Å–µ –≥–∏–ø–æ—Ç–µ–∑—ã\n",
        "                    multi_generator.show_all_hypotheses(current_term)\n",
        "\n",
        "                elif action == '4':\n",
        "                    # –ù–æ–≤—ã–π —Ç–µ—Ä–º–∏–Ω\n",
        "                    current_term = None\n",
        "                    current_quality_terms = []\n",
        "                    continue\n",
        "\n",
        "                elif action == '5':\n",
        "                    # –í—ã—Ö–æ–¥\n",
        "                    print(\"üëã –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!\")\n",
        "                    return\n",
        "\n",
        "                else:\n",
        "                    print(\"‚ö†Ô∏è –ù–µ–≤–µ—Ä–Ω—ã–π –≤—ã–±–æ—Ä\")\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\n‚èπÔ∏è –ü—Ä–µ—Ä—ã–≤–∞–Ω–∏–µ (Ctrl+C)\")\n",
        "            return\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {e}\")\n",
        "            continue\n",
        "\n",
        "# === –î–ï–ú–û –§–£–ù–ö–¶–ò–Ø ===\n",
        "def demo_enhanced_features():\n",
        "    \"\"\"–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –Ω–æ–≤—ã—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π\"\"\"\n",
        "    print(\"üéØ –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –ù–û–í–´–• –í–û–ó–ú–û–ñ–ù–û–°–¢–ï–ô\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    print(\"1. üìã –í–´–ë–û–† –¢–ï–†–ú–ò–ù–û–í –ü–û –ö–ê–¢–ï–ì–û–†–ò–Ø–ú:\")\n",
        "    print(\"\"\"\n",
        "    üß¨ –ü—Ä–æ—Ü–µ—Å—Å—ã —Å—Ç–∞—Ä–µ–Ω–∏—è (15 —Ç–µ—Ä–º–∏–Ω–æ–≤)\n",
        "       –¢–æ–ø-5: aging, senescence, longevity, mortality, lifespan\n",
        "\n",
        "    üî¨ –ú–æ–ª–µ–∫—É–ª—è—Ä–Ω—ã–µ –º–µ—Ö–∞–Ω–∏–∑–º—ã (12 —Ç–µ—Ä–º–∏–Ω–æ–≤)\n",
        "       –¢–æ–ø-5: autophagy, apoptosis, dna repair, transcription\n",
        "\n",
        "    ‚ö° –ö–ª–µ—Ç–æ—á–Ω—ã–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã (8 —Ç–µ—Ä–º–∏–Ω–æ–≤)\n",
        "       –¢–æ–ø-5: mitochondria, nucleus, ribosome, membrane\n",
        "    \"\"\")\n",
        "\n",
        "    print(\"2. ‚ûï –ú–ù–û–ñ–ï–°–¢–í–ï–ù–ù–´–ï –†–ê–£–ù–î–´ –ì–ò–ü–û–¢–ï–ó:\")\n",
        "    print(\"\"\"\n",
        "    –†–∞—É–Ω–¥ 1: –û–±—â–∏–π –∞–Ω–∞–ª–∏–∑ + 5 –±–∞–∑–æ–≤—ã—Ö –≥–∏–ø–æ—Ç–µ–∑\n",
        "    –†–∞—É–Ω–¥ 2: –§–æ–∫—É—Å –Ω–∞ –º–æ–ª–µ–∫—É–ª—è—Ä–Ω—ã–µ –º–µ—Ö–∞–Ω–∏–∑–º—ã + 5 –Ω–æ–≤—ã—Ö –≥–∏–ø–æ—Ç–µ–∑\n",
        "    –†–∞—É–Ω–¥ 3: –§–æ–∫—É—Å –Ω–∞ –∫–ª–µ—Ç–æ—á–Ω—ã–µ –ø—Ä–æ—Ü–µ—Å—Å—ã + 5 –Ω–æ–≤—ã—Ö –≥–∏–ø–æ—Ç–µ–∑\n",
        "    –†–∞—É–Ω–¥ 4: –§–æ–∫—É—Å –Ω–∞ —Å–∏—Å—Ç–µ–º–Ω—É—é –±–∏–æ–ª–æ–≥–∏—é + 5 –Ω–æ–≤—ã—Ö –≥–∏–ø–æ—Ç–µ–∑\n",
        "    \"\"\")\n",
        "\n",
        "    print(\"3. üéõÔ∏è –í–ê–†–ò–ê–¶–ò–Ø –ü–ê–†–ê–ú–ï–¢–†–û–í:\")\n",
        "    print(\"\"\"\n",
        "    ‚Ä¢ –†–∞–∑–Ω–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ (0.7, 0.8, 0.9) –¥–ª—è –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏\n",
        "    ‚Ä¢ –†–∞–∑–Ω—ã–µ —Ñ–æ–∫—É—Å—ã –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è\n",
        "    ‚Ä¢ –ò–∑–±–µ–≥–∞–Ω–∏–µ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è —Ç–µ—Ä–º–∏–Ω–æ–≤\n",
        "    ‚Ä¢ –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã\n",
        "    \"\"\")\n",
        "\n",
        "    print(\"4. üìö –£–ü–†–ê–í–õ–ï–ù–ò–ï –ò–°–¢–û–†–ò–ï–ô:\")\n",
        "    print(\"\"\"\n",
        "    ‚Ä¢ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö —Ä–∞—É–Ω–¥–æ–≤ –≥–∏–ø–æ—Ç–µ–∑\n",
        "    ‚Ä¢ –ü–æ–∫–∞–∑ –≤—Å–µ—Ö –≥–∏–ø–æ—Ç–µ–∑ –ø–æ —Ç–µ—Ä–º–∏–Ω—É\n",
        "    ‚Ä¢ –ü–æ–¥—Å—á–µ—Ç –æ–±—â–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞\n",
        "    ‚Ä¢ –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ —Ä–∞—É–Ω–¥–∞–º\n",
        "    \"\"\")\n",
        "\n",
        "# === –ì–û–¢–û–í–´–ï –ö–û–ú–ê–ù–î–´ ===\n",
        "print(\"\"\"\n",
        "üöÄ –†–ê–°–®–ò–†–ï–ù–ù–ê–Ø –°–ò–°–¢–ï–ú–ê –ì–û–¢–û–í–ê!\n",
        "\n",
        "–û—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã:\n",
        "1. demo_enhanced_features()     # –î–µ–º–æ –Ω–æ–≤—ã—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π\n",
        "2. enhanced_interactive_mode()  # –û—Å–Ω–æ–≤–Ω–æ–π –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º\n",
        "\n",
        "–ù–æ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:\n",
        "‚úÖ –í—ã–±–æ—Ä —Ç–µ—Ä–º–∏–Ω–æ–≤ –ø–æ 8 –±–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–º –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º\n",
        "‚úÖ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –≥–∏–ø–æ—Ç–µ–∑ (–∫–Ω–æ–ø–∫–∞ \"–µ—â–µ 5\")\n",
        "‚úÖ –í–∞—Ä–∏–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–µ–∂–¥—É —Ä–∞—É–Ω–¥–∞–º–∏\n",
        "‚úÖ –ò—Å—Ç–æ—Ä–∏—è –≤—Å–µ—Ö —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≥–∏–ø–æ—Ç–µ–∑\n",
        "‚úÖ –°–ª—É—á–∞–π–Ω—ã–π –≤—ã–±–æ—Ä —Ç–µ—Ä–º–∏–Ω–æ–≤\n",
        "‚úÖ –£–ª—É—á—à–µ–Ω–Ω–∞—è –Ω–∞–≤–∏–≥–∞—Ü–∏—è\n",
        "\n",
        "–î–ª—è –Ω–∞—á–∞–ª–∞: enhanced_interactive_mode()\n",
        "\"\"\")\n",
        "\n",
        "# –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∫–∞–∑ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π\n",
        "demo_enhanced_features()"
      ],
      "metadata": {
        "id": "tLRhQCcYoB4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === –ü–û–õ–ù–ê–Ø –°–ò–°–¢–ï–ú–ê –ó–ê–ü–£–°–ö–ê ===\n",
        "\n",
        "def setup_complete_system():\n",
        "    \"\"\"–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–æ–ª–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã\"\"\"\n",
        "    print(\"üöÄ –ù–ê–°–¢–†–û–ô–ö–ê –ü–û–õ–ù–û–ô –°–ò–°–¢–ï–ú–´ –ì–ï–ù–ï–†–ê–¶–ò–ò –ì–ò–ü–û–¢–ï–ó\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    try:\n",
        "        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
        "        print(f\"üìä –ì—Ä–∞—Ñ: {G.number_of_nodes():,} —É–∑–ª–æ–≤, {G.number_of_edges():,} —Ä—ë–±–µ—Ä\")\n",
        "\n",
        "        # –°–æ–∑–¥–∞–Ω–∏–µ –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤\n",
        "        from openai import OpenAI\n",
        "\n",
        "        client = OpenAI(\n",
        "            base_url=\"http://80.209.242.40:8000/v1\",\n",
        "            api_key=\"dummy-key\"\n",
        "        )\n",
        "\n",
        "        # –ù–∞—É—á–Ω—ã–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä\n",
        "        scientific_generator = ScientificHypothesisGenerator(G, node_full_meta, edge_full_meta, client)\n",
        "\n",
        "        # –°–µ–ª–µ–∫—Ç–æ—Ä —Ç–µ—Ä–º–∏–Ω–æ–≤\n",
        "        term_selector = TermSelector(G, node_full_meta)\n",
        "\n",
        "        # –ú—É–ª—å—Ç–∏-–≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä\n",
        "        multi_generator = MultiHypothesisGenerator(scientific_generator)\n",
        "\n",
        "        print(\"‚úÖ –í—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —Å–æ–∑–¥–∞–Ω—ã —É—Å–ø–µ—à–Ω–æ!\")\n",
        "\n",
        "        return {\n",
        "            'scientific_generator': scientific_generator,\n",
        "            'term_selector': term_selector,\n",
        "            'multi_generator': multi_generator,\n",
        "            'client': client\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå –û—à–∏–±–∫–∞ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏: {e}\")\n",
        "        return None\n",
        "\n",
        "def quick_demo():\n",
        "    \"\"\"–ë—ã—Å—Ç—Ä–∞—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã\"\"\"\n",
        "    print(\"\\nüéØ –ë–´–°–¢–†–ê–Ø –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    components = setup_complete_system()\n",
        "    if not components:\n",
        "        return\n",
        "\n",
        "    term_selector = components['term_selector']\n",
        "\n",
        "    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏\n",
        "    print(\"\\nüìã –î–û–°–¢–£–ü–ù–´–ï –ö–ê–¢–ï–ì–û–†–ò–ò –¢–ï–†–ú–ò–ù–û–í:\")\n",
        "    term_selector.show_categories()\n",
        "\n",
        "    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–π —Ç–µ—Ä–º–∏–Ω\n",
        "    random_term = term_selector.get_random_term()\n",
        "    print(f\"\\nüé≤ –ü—Ä–∏–º–µ—Ä —Å–ª—É—á–∞–π–Ω–æ–≥–æ —Ç–µ—Ä–º–∏–Ω–∞: {random_term}\")\n",
        "\n",
        "    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–µ—Ä–º–∏–Ω—ã –∏–∑ –ø–µ—Ä–≤–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏\n",
        "    print(f\"\\nüìã –ü–†–ò–ú–ï–† –¢–ï–†–ú–ò–ù–û–í –ò–ó –ü–ï–†–í–û–ô –ö–ê–¢–ï–ì–û–†–ò–ò:\")\n",
        "    terms = term_selector.show_category_terms(1)\n",
        "\n",
        "    print(\"\\n‚úÖ –°–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é!\")\n",
        "    print(\"–ó–∞–ø—É—Å—Ç–∏—Ç–µ: enhanced_interactive_mode()\")\n",
        "\n",
        "def main_menu():\n",
        "    \"\"\"–ì–ª–∞–≤–Ω–æ–µ –º–µ–Ω—é —Å–∏—Å—Ç–µ–º—ã\"\"\"\n",
        "    print(\"\\nüéØ –ì–õ–ê–í–ù–û–ï –ú–ï–ù–Æ –°–ò–°–¢–ï–ú–´ –ì–ï–ù–ï–†–ê–¶–ò–ò –ì–ò–ü–û–¢–ï–ó\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    while True:\n",
        "        print(\"\\nüìã –í—ã–±–µ—Ä–∏—Ç–µ —Ä–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã:\")\n",
        "        print(\"1. üöÄ –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º (–†–ï–ö–û–ú–ï–ù–î–£–ï–¢–°–Ø)\")\n",
        "        print(\"2. üî¨ –ù–∞—É—á–Ω–æ —Å—Ç—Ä–æ–≥–∏–π —Ä–µ–∂–∏–º\")\n",
        "        print(\"3. üéØ –ë—ã—Å—Ç—Ä–∞—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è\")\n",
        "        print(\"4. üìä –ü–æ–∫–∞–∑–∞—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Ç–µ—Ä–º–∏–Ω–æ–≤\")\n",
        "        print(\"5. üé≤ –ê–Ω–∞–ª–∏–∑ —Å–ª—É—á–∞–π–Ω–æ–≥–æ —Ç–µ—Ä–º–∏–Ω–∞\")\n",
        "        print(\"6. ‚ùå –í—ã—Ö–æ–¥\")\n",
        "\n",
        "        try:\n",
        "            choice = input(\"\\nüëÜ –í–∞—à –≤—ã–±–æ—Ä (1-6): \").strip()\n",
        "\n",
        "            if choice == '1':\n",
        "                print(\"\\nüöÄ –ó–∞–ø—É—Å–∫ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞...\")\n",
        "                enhanced_interactive_mode()\n",
        "\n",
        "            elif choice == '2':\n",
        "                print(\"\\nüî¨ –ó–∞–ø—É—Å–∫ –Ω–∞—É—á–Ω–æ —Å—Ç—Ä–æ–≥–æ–≥–æ —Ä–µ–∂–∏–º–∞...\")\n",
        "                scientific_interactive_mode()\n",
        "\n",
        "            elif choice == '3':\n",
        "                quick_demo()\n",
        "\n",
        "            elif choice == '4':\n",
        "                components = setup_complete_system()\n",
        "                if components:\n",
        "                    components['term_selector'].show_categories()\n",
        "\n",
        "            elif choice == '5':\n",
        "                print(\"\\nüé≤ –ê–Ω–∞–ª–∏–∑ —Å–ª—É—á–∞–π–Ω–æ–≥–æ —Ç–µ—Ä–º–∏–Ω–∞...\")\n",
        "                components = setup_complete_system()\n",
        "                if components:\n",
        "                    random_term = components['term_selector'].get_random_term()\n",
        "                    if random_term:\n",
        "                        print(f\"üéØ –í—ã–±—Ä–∞–Ω —Ç–µ—Ä–º–∏–Ω: {random_term}\")\n",
        "                        result = components['scientific_generator'].analyze_term_scientific(random_term, radius=2)\n",
        "                        if result:\n",
        "                            print(\"‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω!\")\n",
        "\n",
        "            elif choice == '6':\n",
        "                print(\"üëã –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!\")\n",
        "                break\n",
        "\n",
        "            else:\n",
        "                print(\"‚ö†Ô∏è –ù–µ–≤–µ—Ä–Ω—ã–π –≤—ã–±–æ—Ä, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–Ω–æ–≤–∞\")\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\nüëã –í—ã—Ö–æ–¥ –ø–æ Ctrl+C\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå –û—à–∏–±–∫–∞: {e}\")\n",
        "            continue\n",
        "\n",
        "def batch_multiple_rounds(term, rounds=3):\n",
        "    \"\"\"\n",
        "    –ü–∞–∫–µ—Ç–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ä–∞—É–Ω–¥–æ–≤ –≥–∏–ø–æ—Ç–µ–∑ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ç–µ—Ä–º–∏–Ω–∞\n",
        "    \"\"\"\n",
        "    print(f\"\\nüì¶ –ü–ê–ö–ï–¢–ù–ê–Ø –ì–ï–ù–ï–†–ê–¶–ò–Ø {rounds} –†–ê–£–ù–î–û–í –î–õ–Ø '{term.upper()}'\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    components = setup_complete_system()\n",
        "    if not components:\n",
        "        return\n",
        "\n",
        "    scientific_generator = components['scientific_generator']\n",
        "    multi_generator = components['multi_generator']\n",
        "\n",
        "    try:\n",
        "        # 1. –ü–µ—Ä–≤–∏—á–Ω—ã–π –∞–Ω–∞–ª–∏–∑\n",
        "        print(\"üî¨ –®–∞–≥ 1: –ü–µ—Ä–≤–∏—á–Ω—ã–π –∞–Ω–∞–ª–∏–∑...\")\n",
        "        result = scientific_generator.analyze_term_scientific(term, radius=3)\n",
        "\n",
        "        if not result:\n",
        "            print(f\"‚ùå –ü–µ—Ä–≤–∏—á–Ω—ã–π –∞–Ω–∞–ª–∏–∑ '{term}' –Ω–µ —É–¥–∞–ª—Å—è\")\n",
        "            return\n",
        "\n",
        "        quality_terms = result['quality_terms']\n",
        "\n",
        "        # 2. –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–∞—É–Ω–¥—ã\n",
        "        for round_num in range(2, rounds + 2):\n",
        "            print(f\"\\n‚ûï –®–∞–≥ {round_num}: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–∞—É–Ω–¥–∞ {round_num}...\")\n",
        "\n",
        "            additional_hyps = multi_generator.generate_additional_hypotheses(\n",
        "                term, quality_terms, round_num, 5\n",
        "            )\n",
        "\n",
        "            if additional_hyps:\n",
        "                focus = additional_hyps.get('focus', f'—Ä–∞—É–Ω–¥ {round_num}')\n",
        "                hyp_count = len(additional_hyps.get('hypotheses', []))\n",
        "                print(f\"‚úÖ –†–∞—É–Ω–¥ {round_num} ({focus}): {hyp_count} –≥–∏–ø–æ—Ç–µ–∑\")\n",
        "            else:\n",
        "                print(f\"‚ùå –†–∞—É–Ω–¥ {round_num}: –æ—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏\")\n",
        "\n",
        "            # –ü–∞—É–∑–∞ –º–µ–∂–¥—É —Ä–∞—É–Ω–¥–∞–º–∏\n",
        "            if round_num < rounds + 1:\n",
        "                print(\"‚è∏Ô∏è –ü–∞—É–∑–∞ 3 —Å–µ–∫—É–Ω–¥—ã...\")\n",
        "                time.sleep(3)\n",
        "\n",
        "        # 3. –ü–æ–∫–∞–∑–∞—Ç—å –≤—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\n",
        "        print(f\"\\nüìö –ü–æ–∫–∞–∑ –≤—Å–µ—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è '{term}'...\")\n",
        "        multi_generator.show_all_hypotheses(term)\n",
        "\n",
        "        print(f\"\\n‚úÖ –ü–∞–∫–µ—Ç–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå –û—à–∏–±–∫–∞ –ø–∞–∫–µ—Ç–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}\")\n",
        "\n",
        "def power_user_mode():\n",
        "    \"\"\"–†–µ–∂–∏–º –¥–ª—è –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π\"\"\"\n",
        "    print(\"\\n‚ö° –†–ï–ñ–ò–ú –î–õ–Ø –ü–†–û–î–í–ò–ù–£–¢–´–• –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–ï–ô\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    components = setup_complete_system()\n",
        "    if not components:\n",
        "        return\n",
        "\n",
        "    while True:\n",
        "        print(\"\\nüîß –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏:\")\n",
        "        print(\"1. üì¶ –ü–∞–∫–µ—Ç–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è N —Ä–∞—É–Ω–¥–æ–≤ –¥–ª—è —Ç–µ—Ä–º–∏–Ω–∞\")\n",
        "        print(\"2. üîÑ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ç–µ—Ä–º–∏–Ω–æ–≤ –∏–∑ –æ–¥–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏\")\n",
        "        print(\"3. üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º —Ç–µ—Ä–º–∏–Ω–æ–≤\")\n",
        "        print(\"4. üéØ –ö–∞—Å—Ç–æ–º–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∞–Ω–∞–ª–∏–∑–∞\")\n",
        "        print(\"5. ‚¨ÖÔ∏è  –ù–∞–∑–∞–¥ –≤ –≥–ª–∞–≤–Ω–æ–µ –º–µ–Ω—é\")\n",
        "\n",
        "        choice = input(\"\\nüëÜ –í—ã–±–æ—Ä (1-5): \").strip()\n",
        "\n",
        "        if choice == '1':\n",
        "            term = input(\"üîç –í–≤–µ–¥–∏—Ç–µ —Ç–µ—Ä–º–∏–Ω: \").strip()\n",
        "            rounds = input(\"üî¢ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–∞—É–Ω–¥–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 3): \").strip()\n",
        "            rounds = int(rounds) if rounds.isdigit() else 3\n",
        "\n",
        "            if term:\n",
        "                batch_multiple_rounds(term, rounds)\n",
        "\n",
        "        elif choice == '2':\n",
        "            print(\"üîÑ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ç–µ—Ä–º–∏–Ω–æ–≤ –∏–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏...\")\n",
        "            components['term_selector'].show_categories()\n",
        "            cat_num = input(\"üëÜ –í—ã–±–µ—Ä–∏—Ç–µ –∫–∞—Ç–µ–≥–æ—Ä–∏—é: \").strip()\n",
        "\n",
        "            try:\n",
        "                cat_num = int(cat_num)\n",
        "                terms = components['term_selector'].show_category_terms(cat_num)\n",
        "\n",
        "                if terms:\n",
        "                    selected_terms = input(\"üîç –í–≤–µ–¥–∏—Ç–µ –Ω–æ–º–µ—Ä–∞ —Ç–µ—Ä–º–∏–Ω–æ–≤ —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é: \").strip()\n",
        "                    term_indices = [int(x.strip()) - 1 for x in selected_terms.split(',')]\n",
        "                    selected = [terms[i] for i in term_indices if 0 <= i < len(terms)]\n",
        "\n",
        "                    if selected:\n",
        "                        for term in selected:\n",
        "                            print(f\"\\nüî¨ –ê–Ω–∞–ª–∏–∑: {term}\")\n",
        "                            result = components['scientific_generator'].analyze_term_scientific(term, radius=2)\n",
        "                            time.sleep(3)  # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∞–Ω–∞–ª–∏–∑–∞–º–∏\n",
        "\n",
        "            except (ValueError, IndexError):\n",
        "                print(\"‚ö†Ô∏è –ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –≤–≤–æ–¥–∞\")\n",
        "\n",
        "        elif choice == '3':\n",
        "            print(\"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º...\")\n",
        "            term_selector = components['term_selector']\n",
        "\n",
        "            for category, data in term_selector.term_categories.items():\n",
        "                terms = data['terms']\n",
        "                if terms:\n",
        "                    degrees = [G.degree(term) for term in terms]\n",
        "                    avg_degree = sum(degrees) / len(degrees)\n",
        "                    print(f\"{category}: {len(terms)} —Ç–µ—Ä–º–∏–Ω–æ–≤, —Å—Ä–µ–¥–Ω—è—è —Å–≤—è–∑–Ω–æ—Å—Ç—å: {avg_degree:.1f}\")\n",
        "\n",
        "        elif choice == '4':\n",
        "            print(\"üéØ –ö–∞—Å—Ç–æ–º–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã...\")\n",
        "            term = input(\"üîç –¢–µ—Ä–º–∏–Ω: \").strip()\n",
        "            radius = input(\"üìè –†–∞–¥–∏—É—Å –ø–æ–¥–≥—Ä–∞—Ñ–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 3): \").strip()\n",
        "            radius = int(radius) if radius.isdigit() else 3\n",
        "\n",
        "            if term:\n",
        "                result = components['scientific_generator'].analyze_term_scientific(term, radius)\n",
        "\n",
        "        elif choice == '5':\n",
        "            break\n",
        "\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è –ù–µ–≤–µ—Ä–Ω—ã–π –≤—ã–±–æ—Ä\")\n",
        "\n",
        "# === –ì–õ–ê–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø –ó–ê–ü–£–°–ö–ê ===\n",
        "\n",
        "def launch_complete_system():\n",
        "    \"\"\"–ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã\"\"\"\n",
        "    print(\"üéâ –î–û–ë–†–û –ü–û–ñ–ê–õ–û–í–ê–¢–¨ –í –°–ò–°–¢–ï–ú–£ –ì–ï–ù–ï–†–ê–¶–ò–ò –ì–ò–ü–û–¢–ï–ó!\")\n",
        "    print(\"=\" * 70)\n",
        "    print(\"–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —Å–∏—Å—Ç–µ–º—ã:\")\n",
        "    print(\"  üéØ –í—ã–±–æ—Ä —Ç–µ—Ä–º–∏–Ω–æ–≤ –ø–æ 8 –±–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–º –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º\")\n",
        "    print(\"  ‚ûï –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–∞—É–Ω–¥–æ–≤ –≥–∏–ø–æ—Ç–µ–∑\")\n",
        "    print(\"  üî¨ –ù–∞—É—á–Ω–æ —Å—Ç—Ä–æ–≥–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞\")\n",
        "    print(\"  üìä –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Å–µ—Ç–∏\")\n",
        "    print(\"  üö´ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è\")\n",
        "    print(\"  üìö –ò—Å—Ç–æ—Ä–∏—è –≤—Å–µ—Ö —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≥–∏–ø–æ—Ç–µ–∑\")\n",
        "    print(\"  üé≤ –°–ª—É—á–∞–π–Ω—ã–π –≤—ã–±–æ—Ä —Ç–µ—Ä–º–∏–Ω–æ–≤\")\n",
        "    print(\"  ‚ö° –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è —ç–∫—Å–ø–µ—Ä—Ç–æ–≤\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å–∏—Å—Ç–µ–º—ã\n",
        "    print(\"\\nüîß –ü—Ä–æ–≤–µ—Ä–∫–∞ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ —Å–∏—Å—Ç–µ–º—ã...\")\n",
        "    components = setup_complete_system()\n",
        "\n",
        "    if components:\n",
        "        print(\"‚úÖ –°–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞ –∫ —Ä–∞–±–æ—Ç–µ!\")\n",
        "\n",
        "        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∫—Ä–∞—Ç–∫—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É\n",
        "        term_selector = components['term_selector']\n",
        "        total_terms = sum(len(data['terms']) for data in term_selector.term_categories.values())\n",
        "        print(f\"üìä –î–æ—Å—Ç—É–ø–Ω–æ {total_terms} –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤ –≤ 8 –∫–∞—Ç–µ–≥–æ—Ä–∏—è—Ö\")\n",
        "\n",
        "        # –ó–∞–ø—É—Å–∫–∞–µ–º –≥–ª–∞–≤–Ω–æ–µ –º–µ–Ω—é\n",
        "        main_menu()\n",
        "    else:\n",
        "        print(\"‚ùå –°–∏—Å—Ç–µ–º–∞ –Ω–µ –≥–æ—Ç–æ–≤–∞\")\n",
        "\n",
        "# === –ì–û–¢–û–í–´–ï –ö–û–ú–ê–ù–î–´ ===\n",
        "\n",
        "print(\"\"\"\n",
        "üéâ –ü–û–õ–ù–ê–Ø –°–ò–°–¢–ï–ú–ê –ì–û–¢–û–í–ê –ö –ó–ê–ü–£–°–ö–£!\n",
        "\n",
        "–û—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã:\n",
        "1. launch_complete_system()     # –ì–ª–∞–≤–Ω–æ–µ –º–µ–Ω—é (–†–ï–ö–û–ú–ï–ù–î–£–ï–¢–°–Ø)\n",
        "2. enhanced_interactive_mode()  # –ü—Ä—è–º–æ –≤ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Ä–µ–∂–∏–º\n",
        "3. quick_demo()                # –ë—ã—Å—Ç—Ä–∞—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è\n",
        "4. power_user_mode()           # –î–ª—è –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π\n",
        "\n",
        "–ù–û–í–´–ï –í–û–ó–ú–û–ñ–ù–û–°–¢–ò:\n",
        "‚úÖ üìã –í—ã–±–æ—Ä –∏–∑ 8 –∫–∞—Ç–µ–≥–æ—Ä–∏–π —Ç–µ—Ä–º–∏–Ω–æ–≤ (–ø—Ä–æ—Ü–µ—Å—Å—ã, –º–µ—Ö–∞–Ω–∏–∑–º—ã, —Å—Ç—Ä—É–∫—Ç—É—Ä—ã...)\n",
        "‚úÖ ‚ûï –ö–Ω–æ–ø–∫–∞ \"–µ—â–µ 5 –≥–∏–ø–æ—Ç–µ–∑\" —Å –≤–∞—Ä–∏–∞—Ü–∏–µ–π —Ñ–æ–∫—É—Å–∞\n",
        "‚úÖ üîÑ –ù–µ–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–∞—É–Ω–¥–æ–≤\n",
        "‚úÖ üìö –ò—Å—Ç–æ—Ä–∏—è –≤—Å–µ—Ö –≥–∏–ø–æ—Ç–µ–∑ –ø–æ –∫–∞–∂–¥–æ–º—É —Ç–µ—Ä–º–∏–Ω—É\n",
        "‚úÖ üé≤ –°–ª—É—á–∞–π–Ω—ã–π –≤—ã–±–æ—Ä —Ç–µ—Ä–º–∏–Ω–æ–≤\n",
        "‚úÖ üî¨ –ù–∞—É—á–Ω–æ —Å—Ç—Ä–æ–≥–∏–µ –º–µ—Ç—Ä–∏–∫–∏ (5 –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤ –∫–∞—á–µ—Å—Ç–≤–∞)\n",
        "‚úÖ üìä –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø–æ–¥–≥—Ä–∞—Ñ–∞\n",
        "‚úÖ üö´ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è (TF-IDF)\n",
        "\n",
        "–î–ª—è –Ω–∞—á–∞–ª–∞ —Ä–∞–±–æ—Ç—ã: launch_complete_system()\n",
        "\"\"\")\n",
        "\n",
        "# –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∫–∞–∑ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π\n",
        "print(\"\\nüéØ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è...\")\n",
        "quick_demo()"
      ],
      "metadata": {
        "id": "GgeBAxPvoB-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K1DhaYXWoCFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rt74jlz_oCMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TJALtrWvoCR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gY4lUjNUoCYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L4RMYxMfkURu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A2W1r2VN9aN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NEXA9Ync9aY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3t3ZZ-moCNsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4WsLmS0XCN2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UMx3niXuCN-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MHc5Ng4XCOI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e4p8pEajCOTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BGZYLIH-COb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Dfp4mrS_COlU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "269jN0zACOvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TqXxOo-vCO7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D5VpJPqICPJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LJz_wwiUpmbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y8shlkVEpmfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mta_eaDXpmkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Gb5FsLu4pmpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h9Fz93-mpmt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rZMN1K75pmyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x4MYo4e4pm3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kVdXGmnppm7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q-rmsFwepm_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eJ-MjfWJpnCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6Q-CQjDGpnGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZxM9pFqQBlc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "un4a92n-QBpW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXg09xC3QBtE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KkQv__JuQBxE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhjm-bceOz3S"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7IZp-mBJmGm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kIrxlJeMC9bx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2vp199HxZyw0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XePOmtHZy2l"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UGcFTBeQZy8o"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ME45FRtZzDa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-0lv8UoYSHt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dWQYM_tYSLK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "btYlBhtTYSOr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hw8kGMQMTujB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXYK2ZzWTuoa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "470VDXR8QZbX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZG-GJ6UQZf6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zle-SPafQZkT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KV7sXxNWQZox"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ya6vtBZYQZt1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UK5evI-LQZya"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFRlraaHQZ4B"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxxppZ32QZ8e"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oxn6uZr9QaAx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_A5rC9MQaFI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1LsNJ8L7cZl"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}