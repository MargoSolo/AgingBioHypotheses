{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-0nq8pTGOp4B",
        "outputId": "a884231c-434b-4846-f350-d9751013cc44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scispacy\n",
            "  Downloading scispacy-0.5.5-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.2)\n",
            "Collecting spacy\n",
            "  Downloading spacy-3.7.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from scispacy) (1.16.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scispacy) (2.32.3)\n",
            "Collecting conllu (from scispacy)\n",
            "  Downloading conllu-6.0.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from scispacy) (2.0.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from scispacy) (1.5.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.3 in /usr/local/lib/python3.11/dist-packages (from scispacy) (1.6.1)\n",
            "Collecting pysbd (from scispacy)\n",
            "  Downloading pysbd-0.3.4-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting nmslib-metabrainz==2.1.3 (from scispacy)\n",
            "  Downloading nmslib_metabrainz-2.1.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (956 bytes)\n",
            "Collecting pybind11>=2.2.3 (from nmslib-metabrainz==2.1.3->scispacy)\n",
            "  Downloading pybind11-3.0.0-py3-none-any.whl.metadata (10.0 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from nmslib-metabrainz==2.1.3->scispacy) (5.9.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.10)\n",
            "Collecting thinc<8.3.0,>=8.2.2 (from spacy)\n",
            "  Downloading thinc-8.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.16.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->scispacy) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->scispacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->scispacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->scispacy) (2025.7.14)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20.3->scispacy) (3.6.0)\n",
            "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.3.0,>=8.2.2->spacy)\n",
            "  Downloading blis-0.7.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Collecting numpy (from scispacy)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.0.post1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Downloading scispacy-0.5.5-py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.2/46.2 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nmslib_metabrainz-2.1.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m65.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading spacy-3.7.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m52.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading thinc-8.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (920 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m920.2/920.2 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading conllu-6.0.0-py3-none-any.whl (16 kB)\n",
            "Downloading pysbd-0.3.4-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading blis-0.7.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybind11-3.0.0-py3-none-any.whl (292 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m292.1/292.1 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pysbd, pybind11, numpy, conllu, nmslib-metabrainz, blis, thinc, spacy, scispacy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: blis\n",
            "    Found existing installation: blis 1.3.0\n",
            "    Uninstalling blis-1.3.0:\n",
            "      Successfully uninstalled blis-1.3.0\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.3.6\n",
            "    Uninstalling thinc-8.3.6:\n",
            "      Successfully uninstalled thinc-8.3.6\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.8.7\n",
            "    Uninstalling spacy-3.8.7:\n",
            "      Successfully uninstalled spacy-3.8.7\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed blis-0.7.11 conllu-6.0.0 nmslib-metabrainz-2.1.3 numpy-1.26.4 pybind11-3.0.0 pysbd-0.3.4 scispacy-0.5.5 spacy-3.7.5 thinc-8.2.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "27d7b47db2ee411fb6db8e6cf0d35528"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_lg-0.5.1.tar.gz\n",
            "  Downloading https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_lg-0.5.1.tar.gz (532.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m532.3/532.3 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m^C\n"
          ]
        }
      ],
      "source": [
        "!pip install scispacy spacy pandas transformers\n",
        "!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_lg-0.5.1.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scispacy spacy\n",
        "!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_ner_bionlp13cg_md-0.5.1.tar.gz\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbZXmS3tpvjV",
        "outputId": "d54ab00a-a4cb-4f44-d79d-e2887d74d16e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scispacy in /usr/local/lib/python3.11/dist-packages (0.5.5)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.7.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from scispacy) (1.16.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scispacy) (2.32.3)\n",
            "Requirement already satisfied: conllu in /usr/local/lib/python3.11/dist-packages (from scispacy) (6.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from scispacy) (1.26.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from scispacy) (1.5.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.3 in /usr/local/lib/python3.11/dist-packages (from scispacy) (1.6.1)\n",
            "Requirement already satisfied: pysbd in /usr/local/lib/python3.11/dist-packages (from scispacy) (0.3.4)\n",
            "Requirement already satisfied: nmslib-metabrainz==2.1.3 in /usr/local/lib/python3.11/dist-packages (from scispacy) (2.1.3)\n",
            "Requirement already satisfied: pybind11>=2.2.3 in /usr/local/lib/python3.11/dist-packages (from nmslib-metabrainz==2.1.3->scispacy) (3.0.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from nmslib-metabrainz==2.1.3->scispacy) (5.9.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.16.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.14.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->scispacy) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->scispacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->scispacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->scispacy) (2025.7.14)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20.3->scispacy) (3.6.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.0.post1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_ner_bionlp13cg_md-0.5.1.tar.gz\n",
            "  Downloading https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_ner_bionlp13cg_md-0.5.1.tar.gz (120.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.2/120.2 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting spacy<3.5.0,>=3.4.1 (from en_ner_bionlp13cg_md==0.5.1)\n",
            "  Downloading spacy-3.4.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (24 kB)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (3.0.10)\n",
            "Collecting thinc<8.2.0,>=8.1.0 (from spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1)\n",
            "  Downloading thinc-8.1.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
            "Collecting wasabi<1.1.0,>=0.9.1 (from spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1)\n",
            "  Downloading wasabi-0.10.1-py3-none-any.whl.metadata (28 kB)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (2.0.10)\n",
            "Collecting typer<0.8.0,>=0.3.0 (from spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1)\n",
            "  Downloading typer-0.7.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting pathy>=0.3.5 (from spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1)\n",
            "  Downloading pathy-0.11.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting smart-open<7.0.0,>=5.2.1 (from spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1)\n",
            "  Downloading smart_open-6.4.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (1.26.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (2.32.3)\n",
            "Collecting pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 (from spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1)\n",
            "  Downloading pydantic-1.10.22-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.1/154.1 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (25.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (1.3.0)\n",
            "Collecting pathlib-abc==0.1.1 (from pathy>=0.3.5->spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1)\n",
            "  Downloading pathlib_abc-0.1.1-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (4.14.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (2025.7.14)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (0.1.5)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.11/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (8.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (1.2.1)\n",
            "Downloading spacy-3.4.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pathy-0.11.0-py3-none-any.whl (47 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.3/47.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pathlib_abc-0.1.1-py3-none-any.whl (23 kB)\n",
            "Downloading pydantic-1.10.22-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m84.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smart_open-6.4.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading thinc-8.1.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (917 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m917.4/917.4 kB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typer-0.7.0-py3-none-any.whl (38 kB)\n",
            "Downloading wasabi-0.10.1-py3-none-any.whl (26 kB)\n",
            "Building wheels for collected packages: en_ner_bionlp13cg_md\n",
            "  Building wheel for en_ner_bionlp13cg_md (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en_ner_bionlp13cg_md: filename=en_ner_bionlp13cg_md-0.5.1-py3-none-any.whl size=120241137 sha256=6a7df33e50a3d82cc264c3d7b82a6b559c1bce5646c130307483adeb0dc6b544\n",
            "  Stored in directory: /root/.cache/pip/wheels/1b/48/06/cb86feaf8cf8bb0d00a9465e788e3f19cc81e931c6a69b9859\n",
            "Successfully built en_ner_bionlp13cg_md\n",
            "Installing collected packages: wasabi, typer, smart-open, pydantic, pathlib-abc, pathy, thinc, spacy, en_ner_bionlp13cg_md\n",
            "  Attempting uninstall: wasabi\n",
            "    Found existing installation: wasabi 1.1.3\n",
            "    Uninstalling wasabi-1.1.3:\n",
            "      Successfully uninstalled wasabi-1.1.3\n",
            "  Attempting uninstall: typer\n",
            "    Found existing installation: typer 0.16.0\n",
            "    Uninstalling typer-0.16.0:\n",
            "      Successfully uninstalled typer-0.16.0\n",
            "  Attempting uninstall: smart-open\n",
            "    Found existing installation: smart_open 7.3.0.post1\n",
            "    Uninstalling smart_open-7.3.0.post1:\n",
            "      Successfully uninstalled smart_open-7.3.0.post1\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.11.7\n",
            "    Uninstalling pydantic-2.11.7:\n",
            "      Successfully uninstalled pydantic-2.11.7\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.2.5\n",
            "    Uninstalling thinc-8.2.5:\n",
            "      Successfully uninstalled thinc-8.2.5\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.7.5\n",
            "    Uninstalling spacy-3.7.5:\n",
            "      Successfully uninstalled spacy-3.7.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "scispacy 0.5.5 requires spacy<3.8.0,>=3.7.0, but you have spacy 3.4.4 which is incompatible.\n",
            "gradio 5.38.0 requires pydantic<2.12,>=2.0, but you have pydantic 1.10.22 which is incompatible.\n",
            "gradio 5.38.0 requires typer<1.0,>=0.12; sys_platform != \"emscripten\", but you have typer 0.7.0 which is incompatible.\n",
            "langchain-core 0.3.70 requires pydantic>=2.7.4, but you have pydantic 1.10.22 which is incompatible.\n",
            "google-genai 1.26.0 requires pydantic<3.0.0,>=2.0.0, but you have pydantic 1.10.22 which is incompatible.\n",
            "albumentations 2.0.8 requires pydantic>=2.9.2, but you have pydantic 1.10.22 which is incompatible.\n",
            "langchain 0.3.26 requires pydantic<3.0.0,>=2.7.4, but you have pydantic 1.10.22 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed en_ner_bionlp13cg_md-0.5.1 pathlib-abc-0.1.1 pathy-0.11.0 pydantic-1.10.22 smart-open-6.4.0 spacy-3.4.4 thinc-8.1.12 typer-0.7.0 wasabi-0.10.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "nlp = spacy.load(\"en_ner_bionlp13cg_md\")"
      ],
      "metadata": {
        "id": "yQOvgEJypmIc"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_excel('merged_sentences_with_meta.xlsx', engine='openpyxl')\n",
        "df = df.drop_duplicates(subset='Sentence', keep='first').reset_index(drop=True)\n"
      ],
      "metadata": {
        "id": "zm2DI_UuxGd8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import hashlib\n",
        "import sqlite3\n",
        "import tempfile\n",
        "from collections import defaultdict\n",
        "from numba import jit\n",
        "from multiprocessing import Pool, cpu_count\n",
        "import time\n",
        "import spacy\n",
        "import networkx as nx\n",
        "\n",
        "nlp = spacy.load(\"en_ner_bionlp13cg_md\", disable=[\"parser\", \"tagger\", \"lemmatizer\"])\n",
        "\n",
        "def process_chunk_parallel(args):\n",
        "    sentences, nlp_model_name = args\n",
        "    import spacy\n",
        "    nlp = spacy.load(nlp_model_name, exclude=[\"parser\", \"tagger\", \"lemmatizer\"])\n",
        "\n",
        "    results = []\n",
        "    docs = list(nlp.pipe(sentences, disable=['parser', 'tagger', 'lemmatizer']))\n",
        "    for sent, doc in zip(sentences, docs):\n",
        "        ents = [ent.text.strip().lower() for ent in doc.ents if len(ent.text.strip()) > 2 and not ent.text.strip().isdigit()]\n",
        "        results.append((sent, ents))\n",
        "    return results\n",
        "\n",
        "def parallel_processing_version(df, nlp_model_name='en_ner_bionlp13cg_md', n_processes=None):\n",
        "    if n_processes is None:\n",
        "        n_processes = min(cpu_count(), 4)\n",
        "\n",
        "    sentences = df['Sentence'].dropna().astype(str).str.strip().str.lower().tolist()\n",
        "    chunked = np.array_split(sentences, n_processes)\n",
        "    chunk_args = [(list(chunk), nlp_model_name) for chunk in chunked]\n",
        "\n",
        "    with Pool(n_processes) as pool:\n",
        "        results = pool.map(process_chunk_parallel, chunk_args)\n",
        "\n",
        "    all_results = []\n",
        "    for r in results:\n",
        "        all_results.extend(r)\n",
        "    return all_results\n",
        "\n",
        "@jit(nopython=True)\n",
        "def fast_combinations(n):\n",
        "    return n * (n - 1) // 2\n",
        "\n",
        "def pandas_optimized_version(df):\n",
        "    df_clean = df.dropna(subset=['Sentence']).copy()\n",
        "    df_clean['sentence_clean'] = df_clean['Sentence'].astype(str).str.strip().str.lower()\n",
        "    df_grouped = df_clean.groupby('sentence_clean').first().reset_index()\n",
        "    print(f\"✅ Reduced from {len(df_clean)} to {len(df_grouped)} unique sentences\")\n",
        "    return df_grouped\n",
        "\n",
        "def sqlite_version(df):\n",
        "    db_fd, db_path = tempfile.mkstemp(suffix='.db')\n",
        "    conn = sqlite3.connect(db_path)\n",
        "    df.to_sql('sentences', conn, if_exists='replace', index=False)\n",
        "    conn.execute('CREATE INDEX IF NOT EXISTS idx_sentence ON sentences(Sentence)')\n",
        "\n",
        "    batch_size = 1000\n",
        "    total_rows = conn.execute('SELECT COUNT(*) FROM sentences WHERE Sentence IS NOT NULL').fetchone()[0]\n",
        "\n",
        "    node_data = defaultdict(set)\n",
        "    edge_data = defaultdict(set)\n",
        "\n",
        "    for offset in range(0, total_rows, batch_size):\n",
        "        cursor = conn.execute(\"\"\"\n",
        "            SELECT Sentence, doi, Year, Title, Authors, keywords_from_filename\n",
        "            FROM sentences\n",
        "            WHERE Sentence IS NOT NULL\n",
        "            LIMIT ? OFFSET ?\n",
        "        \"\"\", (batch_size, offset))\n",
        "        batch = cursor.fetchall()\n",
        "\n",
        "\n",
        "    conn.close()\n",
        "    return node_data, edge_data\n",
        "\n",
        "def cached_nlp_processing(sentences, nlp, cache_file='nlp_cache.pkl', disable=['parser', 'tagger', 'lemmatizer']):\n",
        "    disable_str = '_'.join(disable)\n",
        "    hash_key = hashlib.md5((''.join(sentences) + disable_str).encode()).hexdigest()\n",
        "    cache_path = f\"{cache_file}_{hash_key}\"\n",
        "\n",
        "    try:\n",
        "        with open(cache_path, 'rb') as f:\n",
        "            return pickle.load(f)\n",
        "    except FileNotFoundError:\n",
        "        results = list(nlp.pipe(sentences, disable=disable))\n",
        "        with open(cache_path, 'wb') as f:\n",
        "            pickle.dump(results, f)\n",
        "        return results\n",
        "\n",
        "def stanza_alternative(sentences):\n",
        "    import stanza\n",
        "    nlp = stanza.Pipeline('en', processors='tokenize,ner', use_gpu=True)\n",
        "    results = []\n",
        "    for sent in sentences:\n",
        "        results.append(nlp(sent))\n",
        "    return results\n",
        "\n",
        "def ultimate_optimization(df, nlp, use_cache=True, use_parallel=False):\n",
        "    print(\"🚀 Starting ultimate optimization...\")\n",
        "    df_opt = pandas_optimized_version(df)\n",
        "    sentences = df_opt['sentence_clean'].tolist()\n",
        "\n",
        "    if use_cache:\n",
        "        nlp_results = cached_nlp_processing(sentences, nlp)\n",
        "    elif use_parallel:\n",
        "        nlp_results = parallel_processing_version(df_opt, 'en_ner_bionlp13cg_md')\n",
        "    else:\n",
        "        nlp_results = list(nlp.pipe(sentences, disable=['parser', 'tagger', 'lemmatizer']))\n",
        "\n",
        "    return nlp_results\n",
        "\n",
        "def build_graph_with_metadata(df):\n",
        "    node_meta = defaultdict(lambda: {\n",
        "        'sentences': set(),\n",
        "        'dois': set(),\n",
        "        'years': set(),\n",
        "        'titles': set(),\n",
        "        'authors': set(),\n",
        "        'keywords': set()\n",
        "    })\n",
        "    edge_meta = defaultdict(lambda: {\n",
        "        'sentences': set(),\n",
        "        'dois': set(),\n",
        "        'years': set(),\n",
        "        'titles': set(),\n",
        "        'authors': set(),\n",
        "        'keywords': set()\n",
        "    })\n",
        "\n",
        "    for _, row in df.dropna(subset=['Sentence']).iterrows():\n",
        "        sent = str(row['Sentence']).strip().lower()\n",
        "        doi      = row.get('doi')\n",
        "        year     = row.get('Year')\n",
        "        title    = row.get('Title')\n",
        "        authors  = row.get('Authors')\n",
        "        keywords = row.get('keywords_from_filename')\n",
        "\n",
        "        doc = nlp(sent)\n",
        "        ents = list({ent.text.strip().lower() for ent in doc.ents if len(ent.text.strip()) > 2})\n",
        "        ents = [e for e in ents if e not in nlp.Defaults.stop_words and not e.isdigit()]\n",
        "\n",
        "        for e in ents:\n",
        "            node_meta[e]['sentences'].add(sent)\n",
        "            node_meta[e]['dois'].add(doi)\n",
        "            node_meta[e]['years'].add(year)\n",
        "            node_meta[e]['titles'].add(title)\n",
        "            node_meta[e]['authors'].add(authors)\n",
        "            node_meta[e]['keywords'].add(keywords)\n",
        "\n",
        "        for i in range(len(ents)):\n",
        "            for j in range(i + 1, len(ents)):\n",
        "                u, v = sorted((ents[i], ents[j]))\n",
        "                edge_meta[(u, v)]['sentences'].add(sent)\n",
        "                edge_meta[(u, v)]['dois'].add(doi)\n",
        "                edge_meta[(u, v)]['years'].add(year)\n",
        "                edge_meta[(u, v)]['titles'].add(title)\n",
        "                edge_meta[(u, v)]['authors'].add(authors)\n",
        "                edge_meta[(u, v)]['keywords'].add(keywords)\n",
        "\n",
        "    G = nx.Graph()\n",
        "    for term, meta in node_meta.items():\n",
        "        G.add_node(term, sentences=list(meta['sentences']))\n",
        "\n",
        "    for (u, v), meta in edge_meta.items():\n",
        "        G.add_edge(u, v, weight=len(meta['sentences']), sentences=list(meta['sentences']))\n",
        "\n",
        "    node_full_meta = {\n",
        "        term: {k: list(v) for k, v in meta.items() if k != 'sentences'}\n",
        "        for term, meta in node_meta.items()\n",
        "    }\n",
        "    edge_full_meta = {\n",
        "        (u, v): {k: list(v) for k, v in meta.items() if k != 'sentences'}\n",
        "        for (u, v), meta in edge_meta.items()\n",
        "    }\n",
        "\n",
        "    return G, node_full_meta, edge_full_meta\n",
        "\n",
        "print(\"\"\"\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5tspzSekC5F",
        "outputId": "b67e81e8-368c-4117-d937-78b82a637dee"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "G, node_full_meta, edge_full_meta = build_graph_with_metadata(df)\n"
      ],
      "metadata": {
        "id": "xbs0tnXgpYil"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Теперь:\n",
        "print(\"Узлов в графе:\", G.number_of_nodes())\n",
        "print(\"Рёбер в графе:\", G.number_of_edges())\n",
        "\n",
        "# Пример: получим предложения и метаданные\n",
        "sample_node = list(G.nodes)[0]\n",
        "print(\"Предложения узла\", sample_node, \":\", G.nodes[sample_node]['sentences'])\n",
        "print(\"Метаданные узла\", sample_node, \":\", node_full_meta[sample_node])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQWefi1DpY06",
        "outputId": "9e1a603f-69a9-4436-c861-300089e26f5f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Узлов в графе: 32707\n",
            "Рёбер в графе: 775715\n",
            "Предложения узла muscle biopsies : ['increased dna demethylation was also observed during myogenic differentiation of human myoblast obtained from muscle biopsies, which was linked to increased tet1-2 mrna and 5hmc levels [55].', 'at the same time, we were able to evaluate the transcriptomic changes induced by real µg in differentiating humpcs previously isolated from muscle biopsies of the same crew member and an age- and sex-matched volunteer (fig. 1).', 'in a multi-ethnic study of human sarcopenia versus age-matched controls in muscle biopsies, the transcriptome profiles of genes involved in mitochondria biogenesis, e.g., pgc-1α and errα nuclear receptor, were reduced in sarcopenic individuals[161].', 'to complement our previous analyses of muscle biopsies of human sarcopenia3 and understand if mitochondrial dysfunction and altered nad+ metabolism could be linked to systemic changes, we investigated serum levels of the kynurenine / vitamin b metabolome in individuals with sarcopenia versus healthy controls from the memosa cohort (extended data table 1).']\n",
            "Метаданные узла muscle biopsies : {'dois': ['https://doi.org/10.1186/s13072-025-00601-w', nan], 'years': [2024, 2025, 2022], 'titles': ['Gut microbiota in sarcopenia and heart failure', 'Trigonelline is an NAD+ precursor that improves muscle function during ageing and is reduced in human sarcopenia', 'Cell identity and 5-hydroxymethylcytosine', 'The MyoGravity project to study real microgravity effects on human muscle precursor cells and tissue'], 'authors': ['Mathieu Membrez, Eugenia Migliavacca, Stefan Christen, Keisuke Yaku, Jennifer Trieu, Alaina K. Lee, Francesco Morandini, Maria Pilar Giner, Jade Stiner, Mikhail V. Makarov, Emma S. Garratt, Maria F. Vasiloglou, Lucie Chanvillard, Emilie Dalbram, Amy M. Ehrlich, José Luis Sanchez-Garcia, Carles Canto, Leonidas G. Karagounis, Jonas T. Treebak, Marie E. Migaud, Ramin Heshmat, Farideh Razi, Neerja Karnani, Afshin Ostovar, Farshad Farzadfar, Stacey K. H. Tay, Matthew J. Sanders, Karen A. Lillycrop, Keith M. Godfrey, Takashi Nakagawa, Sofia Moco, René Koopman, Gordon S. Lynch, Vincenzo Sorrentino, Jerome N. Feige, ', 'Ester Sara Di Filippo, Sara Chiappalupi, Stefano Falone, Vincenza Dolo, Fernanda Amicarelli, Silvia Marchianò, Adriana Carino, Gabriele Mascetti, Giovanni Valentini, Sara Piccirillo, Michele Balsamo, Marco Vukich, Stefano Fiorucci, Guglielmo Sorci, Stefania Fulle, ', 'Chia-Feng Liu, W. H. Wilson Tang, ', 'Floris Honig, Adele Murrell, '], 'keywords': ['sentences, with, meta', '5hmC, human']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import time\n",
        "import json\n",
        "import re\n",
        "from collections import defaultdict, Counter\n",
        "import networkx as nx\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "class SpecializedTermSelector:\n",
        "    def __init__(self, G, node_full_meta):\n",
        "        \"\"\"\n",
        "        Специализированный селектор терминов на основе ваших ключевых слов\n",
        "        \"\"\"\n",
        "        self.G = G\n",
        "        self.node_full_meta = node_full_meta\n",
        "        print(f\"🔍 Инициализация для графа: {G.number_of_nodes():,} узлов, {G.number_of_edges():,} рёбер\")\n",
        "\n",
        "        # Ваши ключевые слова, разделенные\n",
        "        self.user_keywords = self._parse_user_keywords()\n",
        "\n",
        "        # Кэш для ускорения работы\n",
        "        self._degree_cache = {}\n",
        "        self._specialized_terms_cache = None\n",
        "        self._doi_cache = {}\n",
        "\n",
        "        # Предварительная индексация\n",
        "        self._build_indices()\n",
        "\n",
        "        # Создаем специализированные категории\n",
        "        self.term_categories = self._build_specialized_categories()\n",
        "\n",
        "    def _parse_user_keywords(self):\n",
        "        \"\"\"Парсинг ваших ключевых слов\"\"\"\n",
        "        raw_keywords = [\n",
        "            '5hmC, human', 'age, related, pathology, human',\n",
        "            'aging, biomarkers, human', 'aging, biomarkers',\n",
        "            'aging, genes, human', 'aging, genes', 'aging, human',\n",
        "            'aging, therapy, human', 'aging, therapy', 'aging, (1)',\n",
        "            'ATAD3A, human', 'Autophagy, activators', 'autophagy, human',\n",
        "            'biological, aging, (1)', 'biomarker, of, senescence, human',\n",
        "            'booster, aging', 'bulk, RNA, human, aging',\n",
        "            'caloric, restriction, human', 'CD38, human, aging',\n",
        "            'clonal, expansion, human, aging', 'clonal, expansion, human',\n",
        "            'COA1', 'COL18A1, human', 'COL18A1',\n",
        "            'correlates, with, lifespan, human', 'cytokines, human, aging',\n",
        "            'decreases, with, age, human', 'destruction, of, senescent, cells',\n",
        "            'DNA, methylation, age', 'DNA, methylation, microbiome',\n",
        "            'drives, inflammation, human', 'EMILIN1', 'EPHAR',\n",
        "            'epigenetic, age, human', 'epigenetic, clock, human',\n",
        "            'epigenetic, clock', 'epigenetic, marks, human', 'frailty, index',\n",
        "            'GATA4, human', 'GDF15, human',\n",
        "            'Genes, associated, with, aging, human',\n",
        "            'Genes, associated, with, aging', 'Gut, aging, human',\n",
        "            'HMGB1, human', 'HOPX', 'HOPX, human', 'immune, profiling, human',\n",
        "            'immunosenescence, human', 'increases, with, age, human',\n",
        "            'inflammaging, human', 'inflammaging, (1)', 'inflammaging, (2)',\n",
        "            'inflammaging', 'is, a, hallmark, of, aging, human',\n",
        "            'is, associated, with, aging, human',\n",
        "            'is, enriched, in, centenarians',\n",
        "            'is, linked, to, longevity, human', 'KAT7, human, (1)',\n",
        "            'KAT7, inhibitors', 'longevity, pathways, human',\n",
        "            'mediates, aging, effects', 'sentences, with, meta',\n",
        "            'methylation, longevity', 'neuropeptide, human, expression',\n",
        "            'potential, modulators, human', 'single, cell, Aging, human',\n",
        "            'T, cell, exhaustion, human', 'TNF, alpha, longevity'\n",
        "        ]\n",
        "\n",
        "        # Разделяем по запятым и убираем лишние символы\n",
        "        all_keywords = set()\n",
        "        for keyword_group in raw_keywords:\n",
        "            # Убираем скобки и лишние символы\n",
        "            clean_group = re.sub(r'[(),]', '', keyword_group)\n",
        "            # Разделяем по запятым и добавляем отдельные слова\n",
        "            words = [word.strip().lower() for word in clean_group.split(',') if word.strip()]\n",
        "            all_keywords.update(words)\n",
        "\n",
        "        print(f\"📋 Извлечено {len(all_keywords)} уникальных ключевых слов\")\n",
        "        return list(all_keywords)\n",
        "\n",
        "    def _build_indices(self):\n",
        "        \"\"\"Предварительная индексация для ускорения\"\"\"\n",
        "        print(\"🚀 Построение индексов...\")\n",
        "\n",
        "        # Кэш степеней узлов\n",
        "        self._degree_cache = dict(self.G.degree())\n",
        "\n",
        "        # Находим специализированные термины\n",
        "        self._specialized_terms_cache = self._find_specialized_terms()\n",
        "\n",
        "        # Индекс DOI\n",
        "        self._build_doi_index()\n",
        "\n",
        "        print(f\"✅ Найдено {len(self._specialized_terms_cache)} специализированных терминов\")\n",
        "\n",
        "    def _find_specialized_terms(self, min_degree=1, max_degree=100):\n",
        "        \"\"\"\n",
        "        Находим термины, соответствующие вашим ключевым словам\n",
        "        \"\"\"\n",
        "        print(\"🔍 Поиск специализированных терминов по вашим ключевым словам...\")\n",
        "\n",
        "        specialized_terms = []\n",
        "\n",
        "        for node in self.G.nodes():\n",
        "            degree = self._degree_cache.get(node, 0)\n",
        "\n",
        "            # Фильтр по степени\n",
        "            if not (min_degree <= degree <= max_degree):\n",
        "                continue\n",
        "\n",
        "            # Проверяем соответствие вашим ключевым словам\n",
        "            if not self._matches_user_keywords(node):\n",
        "                continue\n",
        "\n",
        "            # Базовые фильтры качества\n",
        "            if not self._is_quality_term(node):\n",
        "                continue\n",
        "\n",
        "            # Вычисляем специализированный скор\n",
        "            spec_score = self._calculate_specialized_score(node)\n",
        "\n",
        "            specialized_terms.append((node, degree, spec_score))\n",
        "\n",
        "        # Сортируем по специализированному скору\n",
        "        specialized_terms.sort(key=lambda x: x[2], reverse=True)\n",
        "        return [term for term, _, _ in specialized_terms[:1500]]  # Топ-1500\n",
        "\n",
        "    def _matches_user_keywords(self, term):\n",
        "        \"\"\"Проверяем соответствие термина вашим ключевым словам\"\"\"\n",
        "        term_lower = term.lower()\n",
        "\n",
        "        # Прямое соответствие любому ключевому слову\n",
        "        for keyword in self.user_keywords:\n",
        "            if keyword in term_lower:\n",
        "                return True\n",
        "\n",
        "        # Дополнительные паттерны на основе ваших ключевых слов\n",
        "        aging_patterns = [\n",
        "            r'aging', r'senescence', r'longevity', r'lifespan',\n",
        "            r'biomarker', r'epigenetic', r'inflammaging', r'immunosenescence',\n",
        "            r'methylation', r'autophagy', r'frailty', r'centenarian'\n",
        "        ]\n",
        "\n",
        "        for pattern in aging_patterns:\n",
        "            if re.search(pattern, term_lower):\n",
        "                return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    def _is_quality_term(self, term):\n",
        "        \"\"\"Проверка базового качества термина\"\"\"\n",
        "        if len(term) < 3 or len(term) > 60:\n",
        "            return False\n",
        "\n",
        "        term_lower = term.lower()\n",
        "\n",
        "        # Исключаем технические термины\n",
        "        exclude_patterns = [\n",
        "            r'^\\d+$', r'figure\\s*\\d+', r'table\\s*\\d+',\n",
        "            r'\\.jpg$', r'\\.png$', r'\\.pdf$',\n",
        "            r'^[a-z]\\d+$', r'supplementary', r'additional',\n",
        "            r'method', r'protocol', r'dataset'\n",
        "        ]\n",
        "\n",
        "        for pattern in exclude_patterns:\n",
        "            if re.search(pattern, term_lower):\n",
        "                return False\n",
        "\n",
        "        return True\n",
        "\n",
        "    def _calculate_specialized_score(self, node):\n",
        "        \"\"\"\n",
        "        Специализированный скор на основе ваших критериев\n",
        "        \"\"\"\n",
        "        score = 0\n",
        "        term_lower = node.lower()\n",
        "\n",
        "        # Бонусы за ключевые термины вашего исследования\n",
        "        high_value_keywords = {\n",
        "            'human': 10, 'aging': 8, 'biomarker': 7, 'epigenetic': 6,\n",
        "            'senescence': 6, 'longevity': 8, 'inflammaging': 7,\n",
        "            'immunosenescence': 6, 'methylation': 5, 'autophagy': 5,\n",
        "            'frailty': 5, 'centenarian': 8, 'clock': 6, 'therapy': 5\n",
        "        }\n",
        "\n",
        "        for keyword, bonus in high_value_keywords.items():\n",
        "            if keyword in term_lower:\n",
        "                score += bonus\n",
        "\n",
        "        # Бонус за специфичные гены/белки из ваших ключевых слов\n",
        "        specific_genes = [\n",
        "            'atad3a', 'col18a1', 'coa1', 'emilin1', 'ephar',\n",
        "            'gata4', 'gdf15', 'hmgb1', 'hopx', 'kat7', 'cd38'\n",
        "        ]\n",
        "\n",
        "        for gene in specific_genes:\n",
        "            if gene in term_lower:\n",
        "                score += 15  # Высокий бонус за специфичные гены\n",
        "\n",
        "        # Бонус за редкость (обратная степень)\n",
        "        degree = self._degree_cache.get(node, 0)\n",
        "        rarity_bonus = max(0, 25 - degree)\n",
        "        score += rarity_bonus\n",
        "\n",
        "        # Бонус за наличие DOI\n",
        "        if node in self.node_full_meta:\n",
        "            meta = self.node_full_meta[node]\n",
        "            if isinstance(meta, dict) and meta.get('doi'):\n",
        "                score += 8\n",
        "\n",
        "        # Бонус за разнообразие соседей\n",
        "        neighbors = list(self.G.neighbors(node))\n",
        "        if neighbors:\n",
        "            neighbor_degrees = [self._degree_cache.get(n, 0) for n in neighbors]\n",
        "            diversity = len(set(neighbor_degrees)) / len(neighbor_degrees)\n",
        "            score += diversity * 5\n",
        "\n",
        "        return score\n",
        "\n",
        "    def _build_doi_index(self):\n",
        "        \"\"\"Построение индекса DOI с учетом вашей структуры метаданных\"\"\"\n",
        "        print(\"📚 Построение индекса DOI...\")\n",
        "\n",
        "        # Отладка: смотрим на примеры метаданных\n",
        "        sample_nodes = list(self.node_full_meta.keys())[:3]\n",
        "        print(\"🔍 Примеры метаданных для отладки:\")\n",
        "        for node in sample_nodes:\n",
        "            meta = self.node_full_meta.get(node)\n",
        "            print(f\"   {node}: {type(meta)}\")\n",
        "            if isinstance(meta, dict):\n",
        "                print(f\"      ключи: {list(meta.keys())}\")\n",
        "                if 'dois' in meta:\n",
        "                    print(f\"      dois: {meta['dois'][:3] if len(meta['dois']) > 3 else meta['dois']}\")\n",
        "\n",
        "        # Поля для поиска DOI в вашей структуре\n",
        "        doi_fields_to_check = ['dois', 'doi', 'DOI', 'Doi', 'pmid', 'PMID', 'pubmed_id', 'publication_id']\n",
        "\n",
        "        for node, meta in self.node_full_meta.items():\n",
        "            if isinstance(meta, dict):\n",
        "                # Проверяем разные поля с DOI\n",
        "                for field in doi_fields_to_check:\n",
        "                    if field in meta:\n",
        "                        field_value = meta[field]\n",
        "\n",
        "                        # Если это список (как в вашей структуре)\n",
        "                        if isinstance(field_value, list):\n",
        "                            # Берем первый непустой DOI из списка\n",
        "                            for doi_item in field_value:\n",
        "                                if doi_item and str(doi_item).strip() and str(doi_item).strip().lower() not in ['none', 'nan', 'null']:\n",
        "                                    self._doi_cache[node] = str(doi_item).strip()\n",
        "                                    break\n",
        "                            if node in self._doi_cache:\n",
        "                                break\n",
        "\n",
        "                        # Если это строка\n",
        "                        elif isinstance(field_value, str) and field_value.strip() and field_value.strip().lower() not in ['none', 'nan', 'null']:\n",
        "                            self._doi_cache[node] = field_value.strip()\n",
        "                            break\n",
        "\n",
        "            elif isinstance(meta, str) and ('doi' in meta.lower() or 'pmid' in meta.lower()):\n",
        "                # Если метаданные - строка с DOI\n",
        "                self._doi_cache[node] = meta[:50]  # Берем первые 50 символов\n",
        "\n",
        "        print(f\"✅ Найдено {len(self._doi_cache)} узлов с DOI/PMID\")\n",
        "        if len(self._doi_cache) > 0:\n",
        "            print(\"🔍 Примеры найденных DOI:\")\n",
        "            for i, (node, doi) in enumerate(list(self._doi_cache.items())[:5]):\n",
        "                print(f\"   {node}: {doi}\")\n",
        "        else:\n",
        "            print(\"⚠️ DOI не найдены. Структура метаданных:\")\n",
        "            print(\"   Ожидается: node_full_meta[term]['dois'] как список\")\n",
        "            print(\"   Или: node_full_meta[term]['doi'] как строка\")\n",
        "\n",
        "    def _count_specialized_neighbors(self, term, min_count=3):\n",
        "        \"\"\"\n",
        "        Подсчет специализированных соседей термина\n",
        "        Возвращает количество соседей, соответствующих нашим ключевым словам\n",
        "        \"\"\"\n",
        "        if term not in self.G:\n",
        "            return 0\n",
        "\n",
        "        neighbors = list(self.G.neighbors(term))\n",
        "        specialized_count = 0\n",
        "\n",
        "        for neighbor in neighbors:\n",
        "            if self._matches_user_keywords(neighbor):\n",
        "                specialized_count += 1\n",
        "\n",
        "        return specialized_count\n",
        "\n",
        "    def _build_specialized_categories(self, min_specialized_neighbors=1):\n",
        "        \"\"\"Построение специализированных категорий с расширенными ключевыми словами\"\"\"\n",
        "        print(\"🔧 Построение специализированных категорий с расширенными ключевыми словами...\")\n",
        "        print(f\"🎯 Минимальный порог специализированных соседей: {min_specialized_neighbors}\")\n",
        "\n",
        "        categories = {\n",
        "            '🧬 Биомаркеры старения человека': {\n",
        "                'keywords': [\n",
        "                    'biomarker', 'aging', 'human', 'senescence', 'frailty', 'age',\n",
        "                    'elderly', 'old', 'lifespan', 'longevity', 'mortality', 'centenarian',\n",
        "                    'gerontology', 'geriatric', 'adult', 'cohort'\n",
        "                ],\n",
        "                'terms': []\n",
        "            },\n",
        "            '🔬 Эпигенетические часы': {\n",
        "                'keywords': [\n",
        "                    'epigenetic', 'clock', 'methylation', 'dna', 'histone', 'chromatin',\n",
        "                    'cpg', 'dnmt', 'tet', 'demethylation', 'hypermethylation',\n",
        "                    'h3k4', 'h3k27', 'h3k9', 'acetylation', 'modification'\n",
        "                ],\n",
        "                'terms': []\n",
        "            },\n",
        "            '⚡ Воспаление при старении': {\n",
        "                'keywords': [\n",
        "                    'inflammaging', 'inflammation', 'inflammatory', 'immune', 'cytokine',\n",
        "                    'interleukin', 'tnf', 'il-1', 'il-6', 'il-8', 'nfkb', 'nf-kb',\n",
        "                    'macrophage', 'neutrophil', 'monocyte', 'innate', 'adaptive',\n",
        "                    'interferon', 'chemokine'\n",
        "                ],\n",
        "                'terms': []\n",
        "            },\n",
        "            '🧪 Гены и белки старения': {\n",
        "                'keywords': [\n",
        "                    'gene', 'protein', 'enzyme', 'kinase', 'phosphatase', 'receptor',\n",
        "                    'transcription', 'translation', 'expression', 'regulation',\n",
        "                    'p53', 'p21', 'rb', 'foxo', 'sirt', 'mtor', 'ampk', 'insulin',\n",
        "                    'growth', 'factor', 'hormone', 'pathway', 'signaling'\n",
        "                ],\n",
        "                'terms': []\n",
        "            },\n",
        "            '🏥 Терапевтические мишени': {\n",
        "                'keywords': [\n",
        "                    'therapy', 'treatment', 'drug', 'compound', 'inhibitor', 'activator',\n",
        "                    'modulator', 'intervention', 'pharmaceutical', 'medicine',\n",
        "                    'therapeutic', 'clinical', 'trial', 'dosage', 'administration'\n",
        "                ],\n",
        "                'terms': []\n",
        "            },\n",
        "            '🧠 Иммунное старение': {\n",
        "                'keywords': [\n",
        "                    'immunosenescence', 'immune', 'immunity', 'lymphocyte', 'tcell', 'bcell',\n",
        "                    't cell', 'b cell', 'cd4', 'cd8', 'cd28', 'cd57', 'exhaustion',\n",
        "                    'memory', 'naive', 'effector', 'regulatory', 'treg', 'th1', 'th2'\n",
        "                ],\n",
        "                'terms': []\n",
        "            },\n",
        "            '🔀 Пути долголетия': {\n",
        "                'keywords': [\n",
        "                    'longevity', 'lifespan', 'survival', 'mortality', 'centenarian',\n",
        "                    'pathway', 'signaling', 'cascade', 'network', 'regulation',\n",
        "                    'homeostasis', 'maintenance', 'repair', 'protection'\n",
        "                ],\n",
        "                'terms': []\n",
        "            },\n",
        "            '📊 Клеточные процессы': {\n",
        "                'keywords': [\n",
        "                    'cell', 'cellular', 'mitochondria', 'autophagy', 'apoptosis',\n",
        "                    'proliferation', 'differentiation', 'stem', 'division', 'cycle',\n",
        "                    'metabolism', 'respiration', 'oxidative', 'stress', 'damage',\n",
        "                    'repair', 'maintenance'\n",
        "                ],\n",
        "                'terms': []\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # ДИАГНОСТИКА: анализируем проблему с расширенными ключевыми словами\n",
        "        print(f\"\\n🔍 ДИАГНОСТИКА С РАСШИРЕННЫМИ КЛЮЧЕВЫМИ СЛОВАМИ:\")\n",
        "\n",
        "        # Функция для более гибкого поиска\n",
        "        def flexible_match(term, keywords):\n",
        "            term_lower = term.lower()\n",
        "            # Прямое совпадение\n",
        "            for keyword in keywords:\n",
        "                if keyword in term_lower:\n",
        "                    return True\n",
        "\n",
        "            # Дополнительные паттерны\n",
        "            bio_patterns = [\n",
        "                r'\\baging\\b', r'\\bhuman\\b', r'\\bcell\\b', r'\\bgene\\b', r'\\bprotein\\b',\n",
        "                r'\\binflammation\\b', r'\\bimmune\\b', r'\\bmethylation\\b', r'\\bepigenetic\\b',\n",
        "                r'\\blongevity\\b', r'\\bsenescence\\b', r'\\btherapy\\b', r'\\btreatment\\b'\n",
        "            ]\n",
        "\n",
        "            for pattern in bio_patterns:\n",
        "                if re.search(pattern, term_lower):\n",
        "                    return True\n",
        "\n",
        "            return False\n",
        "\n",
        "        # Анализируем с новыми критериями\n",
        "        neighbor_stats = []\n",
        "        for i, term in enumerate(self._specialized_terms_cache[:20]):\n",
        "            total_neighbors = len(list(self.G.neighbors(term))) if term in self.G else 0\n",
        "            # Используем гибкий поиск\n",
        "            all_keywords = []\n",
        "            for cat_keywords in categories.values():\n",
        "                all_keywords.extend(cat_keywords['keywords'])\n",
        "\n",
        "            matches_flexible = flexible_match(term, all_keywords)\n",
        "            specialized_neighbors = self._count_specialized_neighbors(term) if matches_flexible else 0\n",
        "\n",
        "            neighbor_stats.append((term, total_neighbors, specialized_neighbors, matches_flexible))\n",
        "            status = \"✅\" if matches_flexible else \"❌\"\n",
        "            print(f\"   {i+1:2}. {term:<35} {status} (всего: {total_neighbors}, спец: {specialized_neighbors})\")\n",
        "\n",
        "        # Статистика по всем терминам с гибким поиском\n",
        "        flexible_matching_terms = []\n",
        "        for term in self._specialized_terms_cache:\n",
        "            all_keywords = []\n",
        "            for cat_keywords in categories.values():\n",
        "                all_keywords.extend(cat_keywords['keywords'])\n",
        "\n",
        "            if flexible_match(term, all_keywords):\n",
        "                flexible_matching_terms.append(term)\n",
        "\n",
        "        print(f\"\\n📊 СТАТИСТИКА С РАСШИРЕННЫМИ КРИТЕРИЯМИ:\")\n",
        "        print(f\"   Терминов соответствует расширенным критериям: {len(flexible_matching_terms)}\")\n",
        "\n",
        "        # Если все еще мало, добавляем самые популярные термины\n",
        "        if len(flexible_matching_terms) < 50:\n",
        "            print(f\"⚠️ Все еще мало подходящих терминов. Добавляем популярные биологические термины...\")\n",
        "\n",
        "            # Добавляем термины с высокой степенью связности\n",
        "            high_degree_terms = []\n",
        "            for term in self.G.nodes():\n",
        "                degree = self._degree_cache.get(term, 0)\n",
        "                if degree >= 10:  # Популярные термины\n",
        "                    # Простая проверка на биологичность\n",
        "                    if any(bio_word in term.lower() for bio_word in [\n",
        "                        'cell', 'gene', 'protein', 'human', 'aging', 'age', 'old',\n",
        "                        'inflammation', 'immune', 'therapy', 'treatment', 'disease',\n",
        "                        'cancer', 'diabetes', 'expression', 'regulation', 'pathway',\n",
        "                        'signaling', 'metabolism', 'mitochondria', 'dna', 'rna'\n",
        "                    ]):\n",
        "                        high_degree_terms.append((term, degree))\n",
        "\n",
        "            # Сортируем по популярности\n",
        "            high_degree_terms.sort(key=lambda x: x[1], reverse=True)\n",
        "            popular_terms = [term for term, _ in high_degree_terms[:100]]\n",
        "\n",
        "            print(f\"✅ Добавлено {len(popular_terms)} популярных биологических терминов\")\n",
        "            flexible_matching_terms.extend(popular_terms)\n",
        "\n",
        "        # Счетчики для статистики\n",
        "        filtered_out_count = 0\n",
        "        total_candidates = len(flexible_matching_terms)\n",
        "\n",
        "        # Распределяем термины по категориям с гибким поиском\n",
        "        for term in flexible_matching_terms:\n",
        "            term_lower = term.lower()\n",
        "\n",
        "            # АДАПТИВНЫЙ ФИЛЬТР\n",
        "            if min_specialized_neighbors > 0:\n",
        "                all_keywords = []\n",
        "                for cat_keywords in categories.values():\n",
        "                    all_keywords.extend(cat_keywords['keywords'])\n",
        "\n",
        "                if flexible_match(term, all_keywords):\n",
        "                    specialized_neighbors_count = self._count_specialized_neighbors(term)\n",
        "                else:\n",
        "                    specialized_neighbors_count = 0\n",
        "\n",
        "                if specialized_neighbors_count < min_specialized_neighbors:\n",
        "                    filtered_out_count += 1\n",
        "                    continue\n",
        "            else:\n",
        "                # Если специализированных соседей нет, проверяем общие соседи\n",
        "                total_neighbors = len(list(self.G.neighbors(term))) if term in self.G else 0\n",
        "                if total_neighbors < 5:  # Минимум 5 общих соседей для популярных терминов\n",
        "                    filtered_out_count += 1\n",
        "                    continue\n",
        "\n",
        "            # Распределяем по категориям с гибким поиском\n",
        "            added_to_category = False\n",
        "            for category, data in categories.items():\n",
        "                if not added_to_category and flexible_match(term, data['keywords']):\n",
        "                    data['terms'].append(term)\n",
        "                    added_to_category = True\n",
        "\n",
        "        print(f\"\\n🚫 Отфильтровано {filtered_out_count} из {total_candidates} терминов\")\n",
        "\n",
        "        # Ограничиваем и сортируем по специализированному скору\n",
        "        for category, data in categories.items():\n",
        "            if len(data['terms']) > 15:  # Уменьшаем лимит для лучшего качества\n",
        "                terms_with_score = []\n",
        "                for term in data['terms']:\n",
        "                    score = self._calculate_specialized_score(term)\n",
        "                    degree = self._degree_cache.get(term, 0)\n",
        "                    # Комбинированный скор: специализация + популярность\n",
        "                    combined_score = score + (degree * 0.5)\n",
        "                    terms_with_score.append((term, combined_score))\n",
        "\n",
        "                terms_with_score.sort(key=lambda x: x[1], reverse=True)\n",
        "                data['terms'] = [term for term, _ in terms_with_score[:15]]\n",
        "\n",
        "        # Финальная статистика\n",
        "        total_quality_terms = sum(len(data['terms']) for data in categories.values())\n",
        "        print(f\"✅ В категории добавлено {total_quality_terms} качественных терминов\")\n",
        "\n",
        "        # Показываем статистику по категориям\n",
        "        print(f\"\\n📊 СТАТИСТИКА ПО КАТЕГОРИЯМ (с расширенными ключевыми словами):\")\n",
        "        for category, data in categories.items():\n",
        "            terms_count = len(data['terms'])\n",
        "            if terms_count > 0:\n",
        "                # Показываем средние характеристики\n",
        "                avg_degree = sum(self._degree_cache.get(term, 0) for term in data['terms'][:3]) / min(3, terms_count)\n",
        "                print(f\"   {category}: {terms_count} терминов (средняя степень: {avg_degree:.1f})\")\n",
        "            else:\n",
        "                print(f\"   {category}: 0 терминов ⚠️\")\n",
        "\n",
        "        return categories\n",
        "\n",
        "    def get_term_metadata(self, term):\n",
        "        \"\"\"Получить расширенные метаданные термина с учетом вашей структуры\"\"\"\n",
        "        metadata = {\n",
        "            'term': term,\n",
        "            'degree': self._degree_cache.get(term, 0),\n",
        "            'doi': self._doi_cache.get(term),\n",
        "            'neighbors_count': len(list(self.G.neighbors(term))) if term in self.G else 0,\n",
        "            'specialized_score': self._calculate_specialized_score(term),\n",
        "            'matches_keywords': []\n",
        "        }\n",
        "\n",
        "        # Проверяем какие ключевые слова совпадают\n",
        "        term_lower = term.lower()\n",
        "        for keyword in self.user_keywords:\n",
        "            if keyword in term_lower:\n",
        "                metadata['matches_keywords'].append(keyword)\n",
        "\n",
        "        # Дополнительные метаданные из node_full_meta с учетом вашей структуры\n",
        "        if term in self.node_full_meta:\n",
        "            node_meta = self.node_full_meta[term]\n",
        "            if isinstance(node_meta, dict):\n",
        "                # Извлекаем первые элементы из списков (если они есть)\n",
        "                titles = node_meta.get('titles', [])\n",
        "                authors = node_meta.get('authors', [])\n",
        "                years = node_meta.get('years', [])\n",
        "                keywords = node_meta.get('keywords', [])\n",
        "                dois = node_meta.get('dois', [])\n",
        "\n",
        "                metadata.update({\n",
        "                    'title': titles[0] if titles and titles[0] else None,\n",
        "                    'authors': authors[0] if authors and authors[0] else None,\n",
        "                    'year': years[0] if years and years[0] else None,\n",
        "                    'source_keywords': keywords[0] if keywords and keywords[0] else None,\n",
        "                    'doi_from_meta': dois[0] if dois and dois[0] else None,\n",
        "                    'total_papers': len([d for d in dois if d and str(d).strip()]) if dois else 0,\n",
        "                    'year_range': f\"{min(years) if years else 'N/A'}-{max(years) if years else 'N/A'}\",\n",
        "                    'type': 'biomedical_entity'\n",
        "                })\n",
        "\n",
        "                # Формируем описание\n",
        "                if titles and titles[0]:\n",
        "                    title_text = str(titles[0])[:150]\n",
        "                    metadata['description'] = f\"Biomedical entity from: {title_text}...\"\n",
        "                else:\n",
        "                    metadata['description'] = f\"Biomedical entity with {len(dois) if dois else 0} references\"\n",
        "\n",
        "        return metadata\n",
        "\n",
        "    def show_categories_specialized(self):\n",
        "        \"\"\"Показать специализированные категории\"\"\"\n",
        "        print(\"\\n🎯 СПЕЦИАЛИЗИРОВАННЫЕ КАТЕГОРИИ ПО ВАШИМ КЛЮЧЕВЫМ СЛОВАМ\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        for i, (category, data) in enumerate(self.term_categories.items(), 1):\n",
        "            terms = data['terms']\n",
        "            print(f\"\\n{i}. {category} ({len(terms)} терминов)\")\n",
        "\n",
        "            if terms:\n",
        "                print(\"   Топ-3 по специализированному скору:\")\n",
        "                for j, term in enumerate(terms[:3], 1):\n",
        "                    degree = self._degree_cache.get(term, 0)\n",
        "                    doi_status = \"📚\" if term in self._doi_cache else \"❌\"\n",
        "                    score = self._calculate_specialized_score(term)\n",
        "                    print(f\"   {j}. {term} (связей: {degree}, DOI: {doi_status}, скор: {score:.1f})\")\n",
        "            else:\n",
        "                print(\"   Термины не найдены\")\n",
        "\n",
        "        print(f\"\\n0. ✍️  Ввести термин вручную\")\n",
        "        print(f\"99. 🎲 Случайный специализированный термин\")\n",
        "\n",
        "    def show_term_details(self, term):\n",
        "        \"\"\"Подробная информация о термине с вашими ключевыми словами\"\"\"\n",
        "        print(f\"\\n📋 ДЕТАЛЬНАЯ ИНФОРМАЦИЯ: {term}\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        metadata = self.get_term_metadata(term)\n",
        "\n",
        "        print(f\"🔗 Степень узла: {metadata['degree']}\")\n",
        "        print(f\"🎯 Специализированный скор: {metadata['specialized_score']:.1f}\")\n",
        "        print(f\"👥 Соседей: {metadata['neighbors_count']}\")\n",
        "\n",
        "        if metadata['doi']:\n",
        "            print(f\"📚 DOI: {metadata['doi']}\")\n",
        "        else:\n",
        "            print(f\"📚 DOI: не найден\")\n",
        "\n",
        "        if metadata['matches_keywords']:\n",
        "            print(f\"🔑 Совпадения с вашими ключевыми словами: {', '.join(metadata['matches_keywords'][:5])}\")\n",
        "\n",
        "        # Дополнительная информация из вашей структуры\n",
        "        if metadata.get('title'):\n",
        "            print(f\"📄 Заголовок: {metadata['title'][:100]}...\")\n",
        "\n",
        "        if metadata.get('authors'):\n",
        "            print(f\"👨‍🔬 Авторы: {metadata['authors'][:100]}...\")\n",
        "\n",
        "        if metadata.get('year'):\n",
        "            print(f\"📅 Год: {metadata['year']}\")\n",
        "\n",
        "        if metadata.get('year_range'):\n",
        "            print(f\"📊 Диапазон лет: {metadata['year_range']}\")\n",
        "\n",
        "        if metadata.get('total_papers'):\n",
        "            print(f\"📚 Всего статей: {metadata['total_papers']}\")\n",
        "\n",
        "        if metadata.get('source_keywords'):\n",
        "            print(f\"🏷️  Ключевые слова источника: {metadata['source_keywords'][:100]}...\")\n",
        "\n",
        "        if metadata.get('description'):\n",
        "            print(f\"📝 Описание: {metadata['description']}\")\n",
        "\n",
        "        # Показываем специализированных соседей\n",
        "        if term in self.G:\n",
        "            neighbors = list(self.G.neighbors(term))\n",
        "            if neighbors:\n",
        "                # Сортируем соседей по специализированному скору\n",
        "                neighbor_scores = [(n, self._calculate_specialized_score(n)) for n in neighbors[:15]]\n",
        "                neighbor_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "                print(f\"\\n🔗 Топ-5 специализированных соседей:\")\n",
        "                for i, (neighbor, score) in enumerate(neighbor_scores[:5], 1):\n",
        "                    doi_status = \"📚\" if neighbor in self._doi_cache else \"❌\"\n",
        "                    degree = self._degree_cache.get(neighbor, 0)\n",
        "                    print(f\"   {i}. {neighbor} (скор: {score:.1f}, связей: {degree}, DOI: {doi_status})\")\n",
        "\n",
        "    def get_category_terms(self, category_num):\n",
        "        \"\"\"Получить термины из категории\"\"\"\n",
        "        categories_list = list(self.term_categories.items())\n",
        "\n",
        "        if 1 <= category_num <= len(categories_list):\n",
        "            category_name, data = categories_list[category_num - 1]\n",
        "            return category_name, data['terms']\n",
        "\n",
        "        return None, []\n",
        "\n",
        "    def get_random_term(self):\n",
        "        \"\"\"Получить случайный качественный термин (≥3 специализированных соседей)\"\"\"\n",
        "        quality_terms = []\n",
        "\n",
        "        # Собираем все термины из категорий (они уже прошли фильтрацию)\n",
        "        for data in self.term_categories.values():\n",
        "            quality_terms.extend(data['terms'])\n",
        "\n",
        "        if quality_terms:\n",
        "            return random.choice(quality_terms)\n",
        "        else:\n",
        "            # Если в категориях пусто, ищем среди всех специализированных терминов\n",
        "            print(\"⚠️ Поиск качественных терминов среди всех...\")\n",
        "            for term in self._specialized_terms_cache:\n",
        "                if self._count_specialized_neighbors(term) >= 3:\n",
        "                    quality_terms.append(term)\n",
        "\n",
        "            if quality_terms:\n",
        "                print(f\"✅ Найдено {len(quality_terms)} качественных терминов\")\n",
        "                return random.choice(quality_terms)\n",
        "            else:\n",
        "                print(\"❌ Качественные термины не найдены\")\n",
        "                return None\n",
        "\n",
        "    def validate_manual_term(self, term):\n",
        "        \"\"\"\n",
        "        Валидация ручного ввода термина\n",
        "        Возвращает (is_valid, warning_message, quality_info)\n",
        "        \"\"\"\n",
        "        if term not in self.G:\n",
        "            return False, f\"Термин '{term}' не найден в графе\", {}\n",
        "\n",
        "        # Подсчитываем качественные показатели\n",
        "        degree = self._degree_cache.get(term, 0)\n",
        "        specialized_neighbors = self._count_specialized_neighbors(term)\n",
        "        matches_keywords = self._matches_user_keywords(term)\n",
        "        has_doi = term in self._doi_cache\n",
        "        spec_score = self._calculate_specialized_score(term)\n",
        "\n",
        "        quality_info = {\n",
        "            'degree': degree,\n",
        "            'specialized_neighbors': specialized_neighbors,\n",
        "            'matches_keywords': matches_keywords,\n",
        "            'has_doi': has_doi,\n",
        "            'spec_score': spec_score\n",
        "        }\n",
        "\n",
        "        # Проверяем критерии качества\n",
        "        warnings = []\n",
        "        is_high_quality = True\n",
        "\n",
        "        if specialized_neighbors < 3:\n",
        "            warnings.append(f\"❌ Мало специализированных соседей: {specialized_neighbors} < 3\")\n",
        "            is_high_quality = False\n",
        "\n",
        "        if not matches_keywords:\n",
        "            warnings.append(f\"⚠️ Не соответствует вашим ключевым словам\")\n",
        "\n",
        "        if not has_doi:\n",
        "            warnings.append(f\"⚠️ Отсутствует DOI\")\n",
        "\n",
        "        if degree < 2:\n",
        "            warnings.append(f\"⚠️ Очень мало связей: {degree}\")\n",
        "\n",
        "        if spec_score < 10:\n",
        "            warnings.append(f\"⚠️ Низкий специализированный скор: {spec_score:.1f}\")\n",
        "\n",
        "        # Формируем сообщение\n",
        "        if is_high_quality and not warnings:\n",
        "            message = f\"✅ Качественный термин готов для анализа\"\n",
        "        elif is_high_quality:\n",
        "            message = f\"⚠️ Термин пригоден, но есть замечания:\\n\" + \"\\n\".join([f\"   {w}\" for w in warnings])\n",
        "        else:\n",
        "            message = f\"❌ Термин не рекомендуется для качественного анализа:\\n\" + \"\\n\".join([f\"   {w}\" for w in warnings])\n",
        "\n",
        "        return is_high_quality, message, quality_info\n",
        "\n",
        "class EnhancedHypothesisGenerator:\n",
        "    def __init__(self, G, node_full_meta, edge_full_meta, client):\n",
        "        self.G = G\n",
        "        self.node_full_meta = node_full_meta\n",
        "        self.edge_full_meta = edge_full_meta\n",
        "        self.client = client\n",
        "\n",
        "        # Мягкие критерии качества (не блокирующие)\n",
        "        self.quality_metrics = {\n",
        "            'preferred_neighbors': 3,      # Предпочтительно, но не обязательно\n",
        "            'preferred_doi_count': 1,      # Хорошо иметь, но не критично\n",
        "            'preferred_spec_score': 10     # Рекомендуемый, но гибкий\n",
        "        }\n",
        "\n",
        "    def generate_hypotheses(self, term, count=5, temperature=0.7):\n",
        "        \"\"\"Генерация гипотез БЕЗ строгой фильтрации\"\"\"\n",
        "        print(f\"\\n💡 ГЕНЕРАЦИЯ {count} ГИПОТЕЗ ДЛЯ '{term}'\")\n",
        "        print(f\"🌡️ Температура: {temperature}\")\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "        # 1. Мягкая оценка качества (информативная, не блокирующая)\n",
        "        quality_info = self.assess_term_quality(term)\n",
        "        self._display_quality_info(quality_info)\n",
        "\n",
        "        # 2. Анализ связей (берем что есть)\n",
        "        connections = self.analyze_connections(term)\n",
        "\n",
        "        if not connections['neighbors']:\n",
        "            print(\"⚠️ У термина нет соседей. Генерируем гипотезы на основе метаданных...\")\n",
        "\n",
        "        # 3. Генерация гипотез (всегда пытаемся сгенерировать)\n",
        "        hypotheses = []\n",
        "        attempts = 0\n",
        "        max_attempts = count * 3  # Больше попыток\n",
        "\n",
        "        while len(hypotheses) < count and attempts < max_attempts:\n",
        "            attempts += 1\n",
        "            hypothesis = self._generate_single_hypothesis(term, connections, temperature)\n",
        "\n",
        "            # Мягкая проверка (исправляем, а не отбрасываем)\n",
        "            if hypothesis:\n",
        "                validated_hypothesis = self._enhance_hypothesis(hypothesis, term, connections)\n",
        "                hypotheses.append(validated_hypothesis)\n",
        "\n",
        "        # 4. Ранжирование по качеству\n",
        "        ranked = self._rank_hypotheses(hypotheses)\n",
        "\n",
        "        print(f\"✅ Сгенерировано {len(ranked)} гипотез\")\n",
        "\n",
        "        return {\n",
        "            'term': term,\n",
        "            'quality_info': quality_info,\n",
        "            'connections': connections,\n",
        "            'hypotheses': ranked[:count],\n",
        "            'generation_temperature': temperature,\n",
        "            'total_attempts': attempts,\n",
        "            'success_rate': len(ranked) / attempts if attempts > 0 else 0\n",
        "        }\n",
        "\n",
        "    def assess_term_quality(self, term):\n",
        "        \"\"\"Мягкая оценка качества (информативная, не блокирующая)\"\"\"\n",
        "        if term not in self.G:\n",
        "            return {\n",
        "                'exists': False,\n",
        "                'message': \"Термин не найден в графе\",\n",
        "                'can_generate': False\n",
        "            }\n",
        "\n",
        "        degree = self.G.degree(term)\n",
        "        doi_count = self._count_doi(term)\n",
        "        spec_score = self._calculate_specialized_score(term)\n",
        "\n",
        "        # Мягкие рекомендации (не требования)\n",
        "        quality_level = \"отличное\"\n",
        "        recommendations = []\n",
        "\n",
        "        if degree < self.quality_metrics['preferred_neighbors']:\n",
        "            quality_level = \"хорошее\" if degree >= 2 else \"ограниченное\"\n",
        "            recommendations.append(f\"Больше связей улучшит контекст ({degree} есть)\")\n",
        "\n",
        "        if doi_count < self.quality_metrics['preferred_doi_count']:\n",
        "            recommendations.append(f\"DOI источники укрепят обоснование ({doi_count} найдено)\")\n",
        "\n",
        "        if spec_score < self.quality_metrics['preferred_spec_score']:\n",
        "            recommendations.append(f\"Специализированный скор можно улучшить ({spec_score:.1f})\")\n",
        "\n",
        "        return {\n",
        "            'exists': True,\n",
        "            'can_generate': True,  # Всегда можем попробовать\n",
        "            'quality_level': quality_level,\n",
        "            'message': f\"Качество данных: {quality_level}\",\n",
        "            'recommendations': recommendations,\n",
        "            'metrics': {\n",
        "                'degree': degree,\n",
        "                'doi_count': doi_count,\n",
        "                'spec_score': spec_score\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def _display_quality_info(self, quality_info):\n",
        "        \"\"\"Показ информации о качестве\"\"\"\n",
        "        if not quality_info['exists']:\n",
        "            print(f\"❌ {quality_info['message']}\")\n",
        "            return\n",
        "\n",
        "        print(f\"📊 {quality_info['message']}\")\n",
        "\n",
        "        metrics = quality_info['metrics']\n",
        "        print(f\"   🔗 Связей: {metrics['degree']}\")\n",
        "        print(f\"   📚 DOI источников: {metrics['doi_count']}\")\n",
        "        print(f\"   🎯 Специализированный скор: {metrics['spec_score']:.1f}\")\n",
        "\n",
        "        if quality_info['recommendations']:\n",
        "            print(f\"💡 Рекомендации для улучшения:\")\n",
        "            for rec in quality_info['recommendations']:\n",
        "                print(f\"   • {rec}\")\n",
        "\n",
        "    def analyze_connections(self, term):\n",
        "        \"\"\"Анализ связей без фильтрации\"\"\"\n",
        "        if term not in self.G:\n",
        "            return {'neighbors': [], 'total_neighbors': 0}\n",
        "\n",
        "        all_neighbors = list(self.G.neighbors(term))\n",
        "\n",
        "        # Берем всех соседей, оцениваем их\n",
        "        neighbor_info = []\n",
        "        for neighbor in all_neighbors:\n",
        "            degree = self.G.degree(neighbor)\n",
        "            doi_count = self._count_doi(neighbor)\n",
        "            spec_score = self._calculate_specialized_score(neighbor)\n",
        "\n",
        "            neighbor_info.append({\n",
        "                'term': neighbor,\n",
        "                'degree': degree,\n",
        "                'doi_count': doi_count,\n",
        "                'spec_score': spec_score,\n",
        "                'has_doi': doi_count > 0\n",
        "            })\n",
        "\n",
        "        # Сортируем по комбинированному скору (степень + DOI + специализация)\n",
        "        neighbor_info.sort(key=lambda x: x['degree'] + x['doi_count']*5 + x['spec_score'], reverse=True)\n",
        "\n",
        "        return {\n",
        "            'neighbors': neighbor_info,\n",
        "            'total_neighbors': len(all_neighbors),\n",
        "            'high_quality_neighbors': len([n for n in neighbor_info if n['has_doi']]),\n",
        "            'avg_degree': sum(n['degree'] for n in neighbor_info) / len(neighbor_info) if neighbor_info else 0\n",
        "        }\n",
        "\n",
        "    def _generate_single_hypothesis(self, term, connections, temperature):\n",
        "        \"\"\"Генерация одной гипотезы с гибким подходом\"\"\"\n",
        "        # Выбираем лучших соседей (или всех, если мало)\n",
        "        neighbors = connections['neighbors'][:8]  # Топ-8 или все что есть\n",
        "\n",
        "        # Получаем DOI информацию\n",
        "        doi_sources = []\n",
        "        for neighbor in neighbors:\n",
        "            if neighbor['has_doi']:\n",
        "                neighbor_doi = self._get_doi(neighbor['term'])\n",
        "                if neighbor_doi:\n",
        "                    doi_sources.append({'term': neighbor['term'], 'doi': neighbor_doi})\n",
        "\n",
        "        # Создаем адаптивный промпт\n",
        "        prompt = self._create_adaptive_prompt(term, neighbors, doi_sources, len(connections['neighbors']))\n",
        "\n",
        "        try:\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=\"llama3.1\",\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                temperature=temperature,\n",
        "                max_tokens=4000\n",
        "            )\n",
        "\n",
        "            response_text = response.choices[0].message.content.strip()\n",
        "\n",
        "            # Парсинг JSON с исправлениями\n",
        "            hypothesis = self._parse_hypothesis_response(response_text)\n",
        "\n",
        "            if hypothesis:\n",
        "                hypothesis['source_terms'] = [term] + [n['term'] for n in neighbors[:3]]\n",
        "                hypothesis['doi_sources'] = doi_sources\n",
        "                return hypothesis\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Ошибка при генерации: {e}\")\n",
        "\n",
        "        # Резервная гипотеза если основная не удалась\n",
        "        return self._create_fallback_hypothesis(term, neighbors)\n",
        "\n",
        "    def _create_adaptive_prompt(self, term, neighbors, doi_sources, total_neighbors):\n",
        "        \"\"\"Создание адаптивного промпта в зависимости от доступных данных\"\"\"\n",
        "\n",
        "        neighbor_list = \", \".join([n['term'] for n in neighbors[:5]])\n",
        "\n",
        "        # Адаптируем промпт к качеству данных\n",
        "        if len(neighbors) >= 5 and doi_sources:\n",
        "            # Полный промпт для богатых данных\n",
        "            context_note = f\"У термина {total_neighbors} связей, много контекста\"\n",
        "            doi_note = f\"\\nДОСТУПНЫЕ DOI ИСТОЧНИКИ:\\n\" + \"\\n\".join([f\"- {d['term']}: {d['doi']}\" for d in doi_sources[:3]])\n",
        "        elif len(neighbors) >= 3:\n",
        "            # Средний промпт\n",
        "            context_note = f\"У термина {total_neighbors} связей, умеренный контекст\"\n",
        "            doi_note = f\"\\nDOI источники: {len(doi_sources)} доступно\" if doi_sources else \"\\nDOI ограничены\"\n",
        "        else:\n",
        "            # Минимальный промпт\n",
        "            context_note = f\"У термина {total_neighbors} связей, ограниченный контекст\"\n",
        "            doi_note = \"\\nРаботаем с ограниченными данными\"\n",
        "\n",
        "        return f\"\"\"Ты — эксперт по биологии старения. Создай КОНКРЕТНУЮ гипотезу.\n",
        "\n",
        "ОСНОВНОЙ ТЕРМИН: \"{term}\"\n",
        "СВЯЗАННЫЕ ТЕРМИНЫ: {neighbor_list}\n",
        "КОНТЕКСТ: {context_note}{doi_note}\n",
        "\n",
        "ТРЕБОВАНИЯ - МАКСИМАЛЬНАЯ КОНКРЕТИКА:\n",
        "🧬 МОЛЕКУЛЯРНЫЕ ДЕТАЛИ:\n",
        "- Конкретные белки, гены, пути\n",
        "- Точные концентрации, активности\n",
        "- Специфичные взаимодействия\n",
        "\n",
        "🔬 ИЗМЕРИМЫЕ ПАРАМЕТРЫ:\n",
        "- Конкретные биомаркеры с единицами (нг/мл, мкМ, %)\n",
        "- Временные рамки изменений\n",
        "- Пороговые значения\n",
        "\n",
        "🧪 ЭКСПЕРИМЕНТАЛЬНАЯ ПРОВЕРКА:\n",
        "- Конкретные методы (qPCR, Western blot, ELISA)\n",
        "- Детали протокола\n",
        "- Ожидаемые результаты\n",
        "\n",
        "ФОРМАТ JSON:\n",
        "{{\n",
        "    \"hypothesis\": \"Конкретная гипотеза с точными молекулярными механизмами\",\n",
        "    \"mechanism\": \"Детальный механизм с белками, концентрациями, временными рамками\",\n",
        "    \"measurements\": [\"биомаркер1 (единицы измерения)\", \"биомаркер2 (единицы)\", \"функциональный тест\"],\n",
        "    \"experimental_design\": \"Подробный протокол: модель, методы, временные точки, контроли\",\n",
        "    \"validation\": \"Как именно проверить: какие методы, какие значения ожидать\",\n",
        "    \"terms_used\": [\"{term}\", \"связанный_термин1\", \"связанный_термин2\"],\n",
        "    \"biological_plausibility\": \"Обоснование с известными фактами\",\n",
        "    \"clinical_relevance\": \"Связь с клиническими проявлениями старения\",\n",
        "    \"novelty_score\": 8\n",
        "}}\n",
        "\n",
        "Генерируй ТОЛЬКО JSON без комментариев!\"\"\"\n",
        "\n",
        "    def _parse_hypothesis_response(self, response_text):\n",
        "        \"\"\"Парсинг ответа модели с исправлениями\"\"\"\n",
        "        try:\n",
        "            return json.loads(response_text)\n",
        "        except json.JSONDecodeError:\n",
        "            # Пытаемся извлечь JSON из текста\n",
        "            start_idx = response_text.find('{')\n",
        "            end_idx = response_text.rfind('}') + 1\n",
        "            if start_idx != -1 and end_idx != -1:\n",
        "                json_text = response_text[start_idx:end_idx]\n",
        "                try:\n",
        "                    return json.loads(json_text)\n",
        "                except json.JSONDecodeError:\n",
        "                    pass\n",
        "            return None\n",
        "\n",
        "    def _enhance_hypothesis(self, hypothesis, term, connections):\n",
        "        \"\"\"Улучшение гипотезы (исправление недостатков вместо отбрасывания)\"\"\"\n",
        "        enhanced = dict(hypothesis)\n",
        "\n",
        "        # Исправляем отсутствующие поля\n",
        "        if 'measurements' not in enhanced or not enhanced['measurements']:\n",
        "            enhanced['measurements'] = self._generate_default_measurements(term)\n",
        "\n",
        "        if 'terms_used' not in enhanced or not enhanced['terms_used']:\n",
        "            enhanced['terms_used'] = [term] + [n['term'] for n in connections['neighbors'][:2]]\n",
        "\n",
        "        if 'biological_plausibility' not in enhanced:\n",
        "            enhanced['biological_plausibility'] = f\"Основано на связях {term} в биологической сети\"\n",
        "\n",
        "        # Добавляем метаданные о качестве\n",
        "        enhanced['quality_metrics'] = {\n",
        "            'has_measurements': bool(enhanced.get('measurements')),\n",
        "            'uses_network_terms': any(t in [n['term'] for n in connections['neighbors']] for t in enhanced.get('terms_used', [])),\n",
        "            'has_doi_support': bool(enhanced.get('doi_sources'))\n",
        "        }\n",
        "\n",
        "        return enhanced\n",
        "\n",
        "    def _generate_default_measurements(self, term):\n",
        "        \"\"\"Генерация базовых измерений если они отсутствуют\"\"\"\n",
        "        return [\n",
        "            f\"{term} expression level (fold change)\",\n",
        "            f\"{term} protein concentration (ng/ml)\",\n",
        "            f\"{term} activity assay (AU/min)\"\n",
        "        ]\n",
        "\n",
        "    def _create_fallback_hypothesis(self, term, neighbors):\n",
        "        \"\"\"Создание резервной гипотезы\"\"\"\n",
        "        neighbor_terms = [n['term'] for n in neighbors[:3]] if neighbors else [\"related factors\"]\n",
        "\n",
        "        return {\n",
        "            \"hypothesis\": f\"{term} plays a role in aging through interactions with {', '.join(neighbor_terms)}\",\n",
        "            \"mechanism\": f\"The {term} pathway interacts with cellular aging processes via network connections\",\n",
        "            \"measurements\": self._generate_default_measurements(term),\n",
        "            \"experimental_design\": f\"Study {term} expression and activity in aging models, compare with controls\",\n",
        "            \"validation\": f\"Measure {term} levels in young vs old samples, functional assays\",\n",
        "            \"terms_used\": [term] + neighbor_terms,\n",
        "            \"biological_plausibility\": f\"Network analysis suggests {term} is connected to aging-related processes\",\n",
        "            \"clinical_relevance\": f\"Changes in {term} may serve as biomarkers of aging\",\n",
        "            \"novelty_score\": 6,\n",
        "            \"is_fallback\": True\n",
        "        }\n",
        "\n",
        "    def _rank_hypotheses(self, hypotheses):\n",
        "        \"\"\"Ранжирование гипотез по качеству\"\"\"\n",
        "        def score_hypothesis(h):\n",
        "            score = 0\n",
        "\n",
        "            # Бонус за конкретные измерения\n",
        "            measurements = h.get('measurements', [])\n",
        "            score += sum(2 for m in measurements if '(' in m and ')' in m)  # Есть единицы\n",
        "\n",
        "            # Бонус за использование DOI-терминов\n",
        "            doi_sources = h.get('doi_sources', [])\n",
        "            score += len(doi_sources) * 3\n",
        "\n",
        "            # Бонус за детальность механизма\n",
        "            mechanism = h.get('mechanism', '')\n",
        "            score += min(len(mechanism) // 100, 5)  # До 5 баллов за детальность\n",
        "\n",
        "            # Бонус за экспериментальную проверяемость\n",
        "            validation = h.get('validation', '')\n",
        "            score += min(len(validation) // 50, 3)\n",
        "\n",
        "            # Штраф за резервную гипотезу\n",
        "            if h.get('is_fallback'):\n",
        "                score -= 5\n",
        "\n",
        "            # Бонус за новизну\n",
        "            novelty = h.get('novelty_score', 5)\n",
        "            score += novelty\n",
        "\n",
        "            return score\n",
        "\n",
        "        return sorted(hypotheses, key=score_hypothesis, reverse=True)\n",
        "\n",
        "    def display_hypotheses(self, result):\n",
        "        \"\"\"Показ гипотез с расширенной информацией\"\"\"\n",
        "        if not result or 'hypotheses' not in result:\n",
        "            print(\"❌ Нет гипотез для отображения\")\n",
        "            return\n",
        "\n",
        "        hypotheses = result['hypotheses']\n",
        "        term = result['term']\n",
        "\n",
        "        print(f\"\\n📚 ГИПОТЕЗЫ ДЛЯ '{term.upper()}'\")\n",
        "        print(f\"🌡️ Температура: {result.get('generation_temperature', 'неизвестно')}\")\n",
        "        print(f\"📊 Успешность: {result.get('success_rate', 0):.1%} ({len(hypotheses)} из {result.get('total_attempts', 0)} попыток)\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        for i, hyp in enumerate(hypotheses, 1):\n",
        "            print(f\"\\n{i}. ГИПОТЕЗА:\")\n",
        "            print(f\"   🧬 Термины: {', '.join(hyp.get('terms_used', []))}\")\n",
        "            print(f\"   🔬 Механизм: {hyp.get('hypothesis', '')}\")\n",
        "            print(f\"   📚 Детали: {hyp.get('mechanism', '')}\")\n",
        "\n",
        "            # Измерения\n",
        "            measurements = hyp.get('measurements', [])\n",
        "            if measurements:\n",
        "                print(f\"   📊 Измерения:\")\n",
        "                for j, measurement in enumerate(measurements, 1):\n",
        "                    print(f\"      {j}. {measurement}\")\n",
        "\n",
        "            # Экспериментальная проверка\n",
        "            if 'experimental_design' in hyp:\n",
        "                print(f\"   🧪 Эксперимент: {hyp['experimental_design']}\")\n",
        "\n",
        "            if 'validation' in hyp:\n",
        "                print(f\"   ✅ Валидация: {hyp['validation']}\")\n",
        "\n",
        "            # Клиническая значимость\n",
        "            if 'clinical_relevance' in hyp:\n",
        "                print(f\"   🏥 Клиническая значимость: {hyp['clinical_relevance']}\")\n",
        "\n",
        "            # Новизна\n",
        "            if 'novelty_score' in hyp:\n",
        "                print(f\"   ⭐ Новизна: {hyp['novelty_score']}/10\")\n",
        "\n",
        "            # DOI источники\n",
        "            doi_sources = hyp.get('doi_sources', [])\n",
        "            if doi_sources:\n",
        "                print(f\"   📖 DOI источники:\")\n",
        "                for source in doi_sources:\n",
        "                    print(f\"      • {source['term']}: {source['doi']}\")\n",
        "            else:\n",
        "                print(f\"   📖 DOI источники: отсутствуют ⚠️\")\n",
        "\n",
        "            # Индикатор резервной гипотезы\n",
        "            if hyp.get('is_fallback'):\n",
        "                print(f\"   ⚠️ Резервная гипотеза (ограниченные данные)\")\n",
        "\n",
        "            print(\"-\" * 80)\n",
        "\n",
        "    # Вспомогательные методы адаптированные под вашу структуру данных\n",
        "    def _calculate_specialized_score(self, term):\n",
        "        \"\"\"Расчет специализированного скора\"\"\"\n",
        "        if term not in self.G:\n",
        "            return 0\n",
        "\n",
        "        score = 0\n",
        "        degree = self.G.degree(term)\n",
        "\n",
        "        # Базовый скор от степени\n",
        "        score += min(degree, 20)  # До 20 баллов за связи\n",
        "\n",
        "        # Бонус за DOI\n",
        "        if self._count_doi(term) > 0:\n",
        "            score += 10\n",
        "\n",
        "        # Бонус за биологические ключевые слова\n",
        "        term_lower = term.lower()\n",
        "        bio_keywords = ['human', 'cell', 'gene', 'protein', 'aging', 'dna', 'rna']\n",
        "        for keyword in bio_keywords:\n",
        "            if keyword in term_lower:\n",
        "                score += 3\n",
        "\n",
        "        return score\n",
        "\n",
        "    def _count_doi(self, term):\n",
        "        \"\"\"Подсчет DOI для термина\"\"\"\n",
        "        if term not in self.node_full_meta:\n",
        "            return 0\n",
        "\n",
        "        meta = self.node_full_meta[term]\n",
        "        if not isinstance(meta, dict):\n",
        "            return 0\n",
        "\n",
        "        # Проверяем поле 'dois' (список)\n",
        "        if 'dois' in meta:\n",
        "            dois = meta['dois']\n",
        "            if isinstance(dois, list):\n",
        "                return len([d for d in dois if d and str(d).strip() and str(d).strip().lower() not in ['none', 'nan', 'null']])\n",
        "\n",
        "        # Проверяем другие поля\n",
        "        for field in ['doi', 'DOI', 'Doi']:\n",
        "            if field in meta and meta[field]:\n",
        "                return 1\n",
        "\n",
        "        return 0\n",
        "\n",
        "    def _get_doi(self, term):\n",
        "        \"\"\"Получение первого DOI для термина\"\"\"\n",
        "        if term not in self.node_full_meta:\n",
        "            return None\n",
        "\n",
        "        meta = self.node_full_meta[term]\n",
        "        if not isinstance(meta, dict):\n",
        "            return None\n",
        "\n",
        "        # Проверяем поле 'dois' (список)\n",
        "        if 'dois' in meta:\n",
        "            dois = meta['dois']\n",
        "            if isinstance(dois, list):\n",
        "                for doi in dois:\n",
        "                    if doi and str(doi).strip() and str(doi).strip().lower() not in ['none', 'nan', 'null']:\n",
        "                        return str(doi).strip()\n",
        "\n",
        "        # Проверяем другие поля\n",
        "        for field in ['doi', 'DOI', 'Doi']:\n",
        "            if field in meta and meta[field]:\n",
        "                return str(meta[field]).strip()\n",
        "\n",
        "        return None\n",
        "    def __init__(self, scientific_generator):\n",
        "        \"\"\"Оптимизированный генератор гипотез\"\"\"\n",
        "        self.generator = scientific_generator\n",
        "        self.hypothesis_history = {}\n",
        "        self.doi_metadata = {}\n",
        "\n",
        "    def analyze_specialized_connections(self, search_term, max_neighbors=12):\n",
        "        \"\"\"\n",
        "        Анализ специализированных связей термина с адаптивной логикой\n",
        "        \"\"\"\n",
        "        print(f\"\\n🔍 АНАЛИЗ СПЕЦИАЛИЗИРОВАННЫХ СВЯЗЕЙ: {search_term}\")\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "        if search_term not in self.generator.G:\n",
        "            print(f\"❌ Термин '{search_term}' не найден в графе\")\n",
        "            return None\n",
        "\n",
        "        # Получаем всех соседей\n",
        "        all_neighbors = list(self.generator.G.neighbors(search_term))\n",
        "\n",
        "        if len(all_neighbors) == 0:\n",
        "            print(f\"❌ У термина '{search_term}' нет соседей в графе\")\n",
        "            return None\n",
        "\n",
        "        # Создаем селектор для оценки соседей\n",
        "        selector = SpecializedTermSelector(self.generator.G, self.generator.node_full_meta)\n",
        "\n",
        "        # Оцениваем соседей по специализированному скору\n",
        "        neighbor_scores = []\n",
        "        specialized_count = 0\n",
        "\n",
        "        for neighbor in all_neighbors:\n",
        "            score = selector._calculate_specialized_score(neighbor)\n",
        "            if selector._matches_user_keywords(neighbor):\n",
        "                specialized_count += 1\n",
        "            neighbor_scores.append((neighbor, score))\n",
        "\n",
        "        # Сортируем по скору и берем топ\n",
        "        neighbor_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # АДАПТИВНАЯ ЛОГИКА: если мало специализированных соседей, берем лучших по скору\n",
        "        if specialized_count < 3:\n",
        "            print(f\"⚠️ Найдено только {specialized_count} специализированных соседей\")\n",
        "            print(f\"💡 Используем {max_neighbors} лучших соседей по скору\")\n",
        "            specialized_neighbors = [n for n, _ in neighbor_scores[:max_neighbors]]\n",
        "        else:\n",
        "            # Берем только соседей, соответствующих ключевым словам\n",
        "            specialized_neighbors = []\n",
        "            for neighbor, score in neighbor_scores:\n",
        "                if len(specialized_neighbors) >= max_neighbors:\n",
        "                    break\n",
        "                if selector._matches_user_keywords(neighbor):\n",
        "                    specialized_neighbors.append(neighbor)\n",
        "\n",
        "        print(f\"🎯 Выбрано {len(specialized_neighbors)} соседей для анализа из {len(all_neighbors)}\")\n",
        "        print(f\"   Из них соответствуют ключевым словам: {specialized_count}\")\n",
        "\n",
        "        # Анализируем DOI\n",
        "        doi_info = self._analyze_doi_metadata(specialized_neighbors)\n",
        "\n",
        "        # КРИТИЧЕСКАЯ ПРОВЕРКА: достаточно ли соседей для анализа\n",
        "        min_threshold = 1  # Снижаем порог\n",
        "        sufficient_neighbors = len(specialized_neighbors) >= min_threshold\n",
        "\n",
        "        if not sufficient_neighbors:\n",
        "            print(f\"⚠️ ВНИМАНИЕ: Найдено только {len(specialized_neighbors)} соседей!\")\n",
        "            print(f\"   Это может привести к менее качественным гипотезам.\")\n",
        "\n",
        "        # Показываем информацию о соседях\n",
        "        if len(specialized_neighbors) > 0:\n",
        "            print(f\"\\n📋 Топ-5 выбранных соседей:\")\n",
        "            for i, neighbor in enumerate(specialized_neighbors[:5], 1):\n",
        "                degree = self.generator.G.degree(neighbor)\n",
        "                matches = selector._matches_user_keywords(neighbor)\n",
        "                score = next((s for n, s in neighbor_scores if n == neighbor), 0)\n",
        "                status = \"✅\" if matches else \"⭐\"\n",
        "                print(f\"   {i}. {neighbor} ({status}, связей: {degree}, скор: {score:.1f})\")\n",
        "\n",
        "        print(f\"📚 DOI найдены для {len(doi_info)} из {len(specialized_neighbors)} терминов\")\n",
        "\n",
        "        # КРИТИЧЕСКАЯ ПРОВЕРКА: есть ли DOI\n",
        "        has_doi = len(doi_info) > 0\n",
        "        if not has_doi:\n",
        "            print(f\"⚠️ ВНИМАНИЕ: DOI/PMID не найдены для выбранных терминов!\")\n",
        "            print(f\"   Гипотезы будут генерироваться без прямых научных источников.\")\n",
        "\n",
        "        return {\n",
        "            'specialized_neighbors': specialized_neighbors,\n",
        "            'doi_metadata': doi_info,\n",
        "            'total_neighbors': len(all_neighbors),\n",
        "            'neighbor_scores': neighbor_scores[:max_neighbors],\n",
        "            'quality_check': {\n",
        "                'sufficient_neighbors': sufficient_neighbors,\n",
        "                'has_doi': has_doi,\n",
        "                'neighbor_count': len(specialized_neighbors),\n",
        "                'doi_count': len(doi_info),\n",
        "                'specialized_count': specialized_count\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def _analyze_doi_metadata(self, terms):\n",
        "        \"\"\"Анализ DOI метаданных для терминов с учетом вашей структуры\"\"\"\n",
        "        doi_info = {}\n",
        "\n",
        "        doi_fields_to_check = ['dois', 'doi', 'DOI', 'Doi', 'pmid', 'PMID', 'pubmed_id', 'publication_id']\n",
        "\n",
        "        for term in terms:\n",
        "            if term in self.generator.node_full_meta:\n",
        "                meta = self.generator.node_full_meta[term]\n",
        "                if isinstance(meta, dict):\n",
        "                    # Проверяем разные поля с DOI\n",
        "                    doi = None\n",
        "                    for field in doi_fields_to_check:\n",
        "                        if field in meta:\n",
        "                            field_value = meta[field]\n",
        "\n",
        "                            # Если это список (как в вашей структуре)\n",
        "                            if isinstance(field_value, list):\n",
        "                                # Берем первый непустой DOI из списка\n",
        "                                for doi_item in field_value:\n",
        "                                    if doi_item and str(doi_item).strip() and str(doi_item).strip().lower() not in ['none', 'nan', 'null']:\n",
        "                                        doi = str(doi_item).strip()\n",
        "                                        break\n",
        "                                if doi:\n",
        "                                    break\n",
        "\n",
        "                            # Если это строка\n",
        "                            elif isinstance(field_value, str) and field_value.strip() and field_value.strip().lower() not in ['none', 'nan', 'null']:\n",
        "                                doi = field_value.strip()\n",
        "                                break\n",
        "\n",
        "                    if doi:\n",
        "                        # Извлекаем дополнительные метаданные\n",
        "                        titles = meta.get('titles', [])\n",
        "                        authors = meta.get('authors', [])\n",
        "                        years = meta.get('years', [])\n",
        "                        keywords = meta.get('keywords', [])\n",
        "\n",
        "                        doi_info[term] = {\n",
        "                            'doi': doi,\n",
        "                            'title': titles[0] if titles and titles[0] else None,\n",
        "                            'authors': authors[0] if authors and authors[0] else None,\n",
        "                            'year': years[0] if years and years[0] else None,\n",
        "                            'keywords': keywords[0] if keywords and keywords[0] else None,\n",
        "                            'type': 'scientific_paper',\n",
        "                            'source': 'literature_graph',\n",
        "                            'description': f\"Scientific paper: {titles[0][:100] if titles and titles[0] else 'No title'}\"\n",
        "                        }\n",
        "\n",
        "                elif isinstance(meta, str) and ('doi' in meta.lower() or 'pmid' in meta.lower()):\n",
        "                    # Если метаданные - строка с DOI\n",
        "                    doi_info[term] = {\n",
        "                        'doi': meta[:50],\n",
        "                        'type': 'string_metadata',\n",
        "                        'source': 'embedded',\n",
        "                        'description': meta[:100]\n",
        "                    }\n",
        "\n",
        "        return doi_info\n",
        "\n",
        "    def generate_specialized_hypotheses(self, search_term, specialized_data, count=5, temperature=0.75):\n",
        "        \"\"\"\n",
        "        Генерация гипотез с адаптивной проверкой качества\n",
        "        temperature: 0.1-1.0 для контроля креативности\n",
        "        \"\"\"\n",
        "        print(f\"\\n💡 ГЕНЕРАЦИЯ СПЕЦИАЛИЗИРОВАННЫХ ГИПОТЕЗ\")\n",
        "        print(f\"🌡️ Температура: {temperature} (0.1=консервативно, 1.0=креативно)\")\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "        # АДАПТИВНАЯ ПРОВЕРКА КАЧЕСТВА ДАННЫХ\n",
        "        quality_check = specialized_data.get('quality_check', {})\n",
        "\n",
        "        sufficient_neighbors = quality_check.get('sufficient_neighbors', False)\n",
        "        has_doi = quality_check.get('has_doi', False)\n",
        "        neighbor_count = quality_check.get('neighbor_count', 0)\n",
        "        doi_count = quality_check.get('doi_count', 0)\n",
        "        specialized_count = quality_check.get('specialized_count', 0)\n",
        "\n",
        "        print(f\"🔍 ПРОВЕРКА КАЧЕСТВА ДАННЫХ:\")\n",
        "        print(f\"   • Всего соседей для анализа: {neighbor_count}\")\n",
        "        print(f\"   • Из них специализированных: {specialized_count}\")\n",
        "        print(f\"   • DOI источников: {doi_count}\")\n",
        "        print(f\"   • Достаточно соседей: {'✅' if sufficient_neighbors else '⚠️'}\")\n",
        "        print(f\"   • Есть DOI: {'✅' if has_doi else '⚠️'}\")\n",
        "\n",
        "        # АДАПТИВНАЯ СИСТЕМА ПРЕДУПРЕЖДЕНИЙ\n",
        "        warnings = []\n",
        "        critical_issues = []\n",
        "\n",
        "        if neighbor_count == 0:\n",
        "            critical_issues.append(\"❌ НЕТ соседей для анализа\")\n",
        "        elif neighbor_count < 3:\n",
        "            warnings.append(f\"⚠️ Мало соседей для анализа ({neighbor_count} < 3)\")\n",
        "\n",
        "        if specialized_count == 0:\n",
        "            warnings.append(\"⚠️ НЕТ специализированных соседей (используем лучших по скору)\")\n",
        "        elif specialized_count < 2:\n",
        "            warnings.append(f\"⚠️ Мало специализированных соседей ({specialized_count} < 2)\")\n",
        "\n",
        "        if doi_count == 0:\n",
        "            warnings.append(\"⚠️ НЕТ DOI источников\")\n",
        "\n",
        "        # Если есть критические проблемы, блокируем\n",
        "        if critical_issues:\n",
        "            print(f\"\\n🚨 КРИТИЧЕСКИЕ ПРОБЛЕМЫ:\")\n",
        "            for issue in critical_issues:\n",
        "                print(f\"   {issue}\")\n",
        "            print(f\"❌ Генерация невозможна\")\n",
        "            return None\n",
        "\n",
        "        # Если есть предупреждения, показываем их\n",
        "        if warnings:\n",
        "            print(f\"\\n⚠️ ПРЕДУПРЕЖДЕНИЯ О КАЧЕСТВЕ:\")\n",
        "            for warning in warnings:\n",
        "                print(f\"   {warning}\")\n",
        "\n",
        "            print(f\"\\n💡 АДАПТИВНЫЕ МЕРЫ:\")\n",
        "            if specialized_count == 0:\n",
        "                print(f\"   • Используем соседей с высоким скором релевантности\")\n",
        "            if doi_count == 0:\n",
        "                print(f\"   • Гипотезы будут основаны на структурных связях графа\")\n",
        "            if neighbor_count < 3:\n",
        "                print(f\"   • Генерируем {count} гипотез с ограниченным контекстом\")\n",
        "\n",
        "            # Менее строгий запрос подтверждения\n",
        "            if len(warnings) >= 2:  # Только если много предупреждений\n",
        "                confirm = input(f\"\\n❓ Продолжить генерацию с указанными ограничениями? (Y/n): \").strip().lower()\n",
        "\n",
        "                if confirm in ['n', 'no', 'нет']:\n",
        "                    print(f\"❌ Генерация отменена пользователем\")\n",
        "                    return None\n",
        "                else:\n",
        "                    print(f\"✅ Продолжаем с адаптивными мерами...\")\n",
        "\n",
        "        specialized_neighbors = specialized_data['specialized_neighbors']\n",
        "        doi_metadata = specialized_data['doi_metadata']\n",
        "        neighbor_scores = specialized_data['neighbor_scores']\n",
        "\n",
        "        # Создаем специализированный промпт\n",
        "        prompt = self._create_specialized_prompt(\n",
        "            search_term, specialized_neighbors, doi_metadata, neighbor_scores, count\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            response = self.generator.client.chat.completions.create(\n",
        "                model=\"llama3.1\",\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                temperature=temperature,\n",
        "                max_tokens=4000\n",
        "            )\n",
        "\n",
        "            response_text = response.choices[0].message.content.strip()\n",
        "\n",
        "            # Парсинг JSON\n",
        "            try:\n",
        "                hypotheses_data = json.loads(response_text)\n",
        "            except json.JSONDecodeError:\n",
        "                start_idx = response_text.find('{')\n",
        "                end_idx = response_text.rfind('}') + 1\n",
        "                if start_idx != -1 and end_idx != -1:\n",
        "                    json_text = response_text[start_idx:end_idx]\n",
        "                    hypotheses_data = json.loads(json_text)\n",
        "                else:\n",
        "                    raise ValueError(\"Не удалось извлечь JSON\")\n",
        "\n",
        "            # Добавляем метаданные о качестве и температуре\n",
        "            hypotheses_data['generation_temperature'] = temperature\n",
        "            hypotheses_data['data_quality'] = quality_check\n",
        "            hypotheses_data['generation_warnings'] = warnings\n",
        "            hypotheses_data['adaptive_measures'] = []\n",
        "\n",
        "            if specialized_count == 0:\n",
        "                hypotheses_data['adaptive_measures'].append(\"Используются соседи с высоким скором\")\n",
        "            if doi_count == 0:\n",
        "                hypotheses_data['adaptive_measures'].append(\"Основано на структурных связях графа\")\n",
        "\n",
        "            # Добавляем DOI метаданные и скоры к гипотезам\n",
        "            if 'hypotheses' in hypotheses_data:\n",
        "                for hyp in hypotheses_data['hypotheses']:\n",
        "                    hyp['doi_sources'] = []\n",
        "                    hyp['term_scores'] = []\n",
        "\n",
        "                    for term in hyp.get('terms', []):\n",
        "                        if term in doi_metadata:\n",
        "                            hyp['doi_sources'].append({\n",
        "                                'term': term,\n",
        "                                'doi': doi_metadata[term]['doi']\n",
        "                            })\n",
        "\n",
        "                        # Добавляем скоры терминов\n",
        "                        for scored_term, score in neighbor_scores:\n",
        "                            if scored_term == term:\n",
        "                                hyp['term_scores'].append({\n",
        "                                    'term': term,\n",
        "                                    'score': score\n",
        "                                })\n",
        "                                break\n",
        "\n",
        "            quality_level = \"высокое\" if not warnings else \"среднее\" if len(warnings) < 2 else \"ограниченное\"\n",
        "            print(f\"✅ Гипотезы сгенерированы (качество данных: {quality_level})\")\n",
        "            return hypotheses_data\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Ошибка генерации гипотез: {e}\")\n",
        "            return None\n",
        "\n",
        "    def _create_specialized_prompt(self, search_term, specialized_neighbors, doi_metadata, neighbor_scores, count):\n",
        "        \"\"\"Создание специализированного промпта с высокой конкретикой\"\"\"\n",
        "\n",
        "        # Формируем информацию о соседях с скорами\n",
        "        neighbors_info = \"СПЕЦИАЛИЗИРОВАННЫЕ СВЯЗАННЫЕ ТЕРМИНЫ (с приоритетом):\\n\"\n",
        "        for i, (term, score) in enumerate(neighbor_scores[:15], 1):\n",
        "            neighbors_info += f\"{i}. {term} (приоритет: {score:.1f})\\n\"\n",
        "\n",
        "        # Формируем информацию о DOI\n",
        "        doi_info = \"\"\n",
        "        if doi_metadata:\n",
        "            doi_info = \"\\nДОСТУПНЫЕ DOI ИСТОЧНИКИ:\\n\"\n",
        "            for term, meta in doi_metadata.items():\n",
        "                doi_info += f\"- {term}: DOI {meta['doi']}\\n\"\n",
        "\n",
        "        return f\"\"\"Ты — эксперт по молекулярной биологии старения человека. Создавай МАКСИМАЛЬНО КОНКРЕТНЫЕ и ИЗМЕРИМЫЕ гипотезы.\n",
        "\n",
        "ОСНОВНОЙ ТЕРМИН: \"{search_term}\"\n",
        "\n",
        "{neighbors_info}\n",
        "\n",
        "{doi_info}\n",
        "\n",
        "КРИТИЧЕСКИ ВАЖНО - КАЖДАЯ ГИПОТЕЗА ДОЛЖНА ВКЛЮЧАТЬ:\n",
        "\n",
        "🧬 ЭПИГЕНЕТИЧЕСКАЯ КОНКРЕТИКА:\n",
        "- Конкретные CpG-сайты (например, cg05575921, cg09809672)\n",
        "- Гистоновые метки (H3K4me3, H3K27me3, H3K9me3, H4K16ac)\n",
        "- Конкретные DNMT (DNMT1, DNMT3A, DNMT3B)\n",
        "- TET-ферменты (TET1, TET2, TET3)\n",
        "\n",
        "🔥 ВОСПАЛИТЕЛЬНАЯ КОНКРЕТИКА:\n",
        "- Конкретные цитокины (IL-1β, IL-6, TNF-α, IL-8, MCP-1)\n",
        "- Рецепторы (TLR4, TLR2, IL-1R, TNFR1/2)\n",
        "- Сигнальные пути (NF-κB p65, IRF3, STAT3)\n",
        "- Inflammasome компоненты (NLRP3, ASC, caspase-1)\n",
        "\n",
        "🧪 ИММУННАЯ КОНКРЕТИКА:\n",
        "- Субпопуляции клеток (CD4+CD28-, CD8+CD57+, Tregs CD4+CD25+FOXP3+)\n",
        "- Поверхностные маркеры (CD38, CD57, KLRG1, PD-1, LAG-3)\n",
        "- Секретируемые факторы (IFN-γ, perforin, granzyme B)\n",
        "\n",
        "📊 ЭКСПЕРИМЕНТАЛЬНАЯ КОНКРЕТИКА:\n",
        "- In vitro модели: первичные фибробласты человека, HUVEC, PBMC\n",
        "- In vivo модели: мыши C57BL/6, крысы Wistar, приматы\n",
        "- Клинические когорты: возраст участников, размер выборки\n",
        "- Конкретные методы: qPCR праймеры, антитела для Western blot, ELISA киты\n",
        "\n",
        "💉 БИОМАРКЕРЫ - УКАЖИ ТОЧНО:\n",
        "- Какие белки измерять в плазме/сыворотке (нг/мл, пг/мл)\n",
        "- Какие мРНК в каких тканях (fold change, ΔCt)\n",
        "- Какие метилированные сайты (% methylation)\n",
        "- Функциональные тесты (время, концентрации)\n",
        "\n",
        "ЗАДАЧА: Сгенерируй {count} МАКСИМАЛЬНО КОНКРЕТНЫХ гипотез о \"{search_term}\" в старении человека.\n",
        "\n",
        "ФОРМАТ JSON:\n",
        "{{\n",
        "  \"focus\": \"конкретные молекулярные механизмы с измеримыми параметрами\",\n",
        "  \"search_term\": \"{search_term}\",\n",
        "  \"hypotheses\": [\n",
        "    {{\n",
        "      \"terms\": [\"{search_term}\", \"конкретный_термин1\", \"конкретный_термин2\"],\n",
        "      \"hypothesis\": \"Конкретная гипотеза с точными молекулярными мишенями\",\n",
        "      \"rationale\": \"Механистическое обоснование с конкретными белками/генами\",\n",
        "      \"epigenetic_specifics\": \"Конкретные CpG-сайты, гистоновые метки, ферменты\",\n",
        "      \"inflammatory_specifics\": \"Точные цитокины, рецепторы, концентрации\",\n",
        "      \"immune_specifics\": \"Конкретные клеточные популяции и маркеры\",\n",
        "      \"experimental_design\": \"Детальный протокол: модель, методы, временные точки\",\n",
        "      \"measurable_biomarkers\": \"Конкретные измеримые параметры с единицами\",\n",
        "      \"clinical_validation\": \"Как проверить у человека: когорта, методы, биомаркеры\",\n",
        "      \"therapeutic_target\": \"Конкретная мишень для вмешательства\"\n",
        "    }}\n",
        "  ]\n",
        "}}\n",
        "\n",
        "ПРИМЕРЫ КОНКРЕТИКИ:\n",
        "❌ ПЛОХО: \"изменения метилирования ДНК\"\n",
        "✅ ХОРОШО: \"гиперметилирование CpG-сайта cg05575921 в промотере CDKN2A\"\n",
        "\n",
        "❌ ПЛОХО: \"провоспалительные цитокины\"\n",
        "✅ ХОРОШО: \"повышение IL-6 до 15.2±3.1 пг/мл и TNF-α до 8.7±2.3 пг/мл в плазме\"\n",
        "\n",
        "❌ ПЛОХО: \"иммунные клетки\"\n",
        "✅ ХОРОШО: \"увеличение CD8+CD57+KLRG1+ сенесцентных T-клеток до 23±5% от CD8+\"\n",
        "\n",
        "Генерируй ТОЛЬКО JSON с максимальной конкретикой!\"\"\"\n",
        "\n",
        "    def display_specialized_hypotheses(self, hypotheses_data):\n",
        "        \"\"\"Показ специализированных гипотез с конкретными метаданными\"\"\"\n",
        "        if not hypotheses_data or 'hypotheses' not in hypotheses_data:\n",
        "            print(\"❌ Нет данных для отображения\")\n",
        "            return\n",
        "\n",
        "        hypotheses = hypotheses_data['hypotheses']\n",
        "        temperature = hypotheses_data.get('generation_temperature', 'неизвестно')\n",
        "        data_quality = hypotheses_data.get('data_quality', {})\n",
        "        warnings = hypotheses_data.get('generation_warnings', [])\n",
        "\n",
        "        print(f\"\\n📚 КОНКРЕТНЫЕ ГИПОТЕЗЫ СТАРЕНИЯ ЧЕЛОВЕКА\")\n",
        "        print(f\"🌡️ Температура генерации: {temperature}\")\n",
        "\n",
        "        # Показываем предупреждения о качестве\n",
        "        if warnings:\n",
        "            print(f\"⚠️ ПРЕДУПРЕЖДЕНИЯ О КАЧЕСТВЕ:\")\n",
        "            for warning in warnings:\n",
        "                print(f\"   {warning}\")\n",
        "            print(f\"🔍 Качество данных: специализированных соседей={data_quality.get('neighbor_count', 0)}, DOI={data_quality.get('doi_count', 0)}\")\n",
        "        else:\n",
        "            print(f\"✅ Качество данных: специализированных соседей={data_quality.get('neighbor_count', 0)}, DOI={data_quality.get('doi_count', 0)}\")\n",
        "\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        for i, hyp in enumerate(hypotheses, 1):\n",
        "            print(f\"\\n{i}. ГИПОТЕЗА:\")\n",
        "            print(f\"   🧬 Термины: {', '.join(hyp.get('terms', []))}\")\n",
        "            print(f\"   🔬 Механизм: {hyp.get('hypothesis', '')}\")\n",
        "            print(f\"   📚 Обоснование: {hyp.get('rationale', '')}\")\n",
        "\n",
        "            # Новые конкретные поля\n",
        "            if 'epigenetic_specifics' in hyp:\n",
        "                print(f\"   🧬 Эпигенетика: {hyp['epigenetic_specifics']}\")\n",
        "\n",
        "            if 'inflammatory_specifics' in hyp:\n",
        "                print(f\"   🔥 Воспаление: {hyp['inflammatory_specifics']}\")\n",
        "\n",
        "            if 'immune_specifics' in hyp:\n",
        "                print(f\"   🛡️  Иммунитет: {hyp['immune_specifics']}\")\n",
        "\n",
        "            if 'experimental_design' in hyp:\n",
        "                print(f\"   🧪 Эксперимент: {hyp['experimental_design']}\")\n",
        "\n",
        "            if 'measurable_biomarkers' in hyp:\n",
        "                print(f\"   📊 Биомаркеры: {hyp['measurable_biomarkers']}\")\n",
        "\n",
        "            if 'clinical_validation' in hyp:\n",
        "                print(f\"   🏥 Клиническая валидация: {hyp['clinical_validation']}\")\n",
        "\n",
        "            if 'therapeutic_target' in hyp:\n",
        "                print(f\"   💊 Терапевтическая мишень: {hyp['therapeutic_target']}\")\n",
        "\n",
        "            # Старые поля для совместимости\n",
        "            if 'clinical_relevance' in hyp:\n",
        "                print(f\"   🎯 Клиническая значимость: {hyp['clinical_relevance']}\")\n",
        "\n",
        "            # Скоры терминов\n",
        "            if hyp.get('term_scores'):\n",
        "                print(f\"   📈 Приоритет терминов:\")\n",
        "                for term_score in hyp['term_scores']:\n",
        "                    print(f\"      • {term_score['term']}: {term_score['score']:.1f}\")\n",
        "\n",
        "            # DOI источники\n",
        "            if hyp.get('doi_sources'):\n",
        "                print(f\"   📖 DOI источники:\")\n",
        "                for source in hyp['doi_sources']:\n",
        "                    print(f\"      • {source['term']}: {source['doi']}\")\n",
        "            else:\n",
        "                print(f\"   📖 DOI источники: отсутствуют ⚠️\")\n",
        "\n",
        "            print(\"-\" * 80)\n",
        "\n",
        "        # Итоговая оценка качества результата\n",
        "        if warnings:\n",
        "            print(f\"\\n⚠️ ИТОГОВАЯ ОЦЕНКА: Гипотезы сгенерированы с ограничениями\")\n",
        "            print(f\"💡 Рекомендуется выбрать термин с большим количеством релевантных связей и DOI\")\n",
        "        else:\n",
        "            print(f\"\\n✅ ИТОГОВАЯ ОЦЕНКА: Гипотезы высокого качества с научным обоснованием\")\n",
        "\n",
        "def specialized_interactive_mode(graph=None, node_meta=None, edge_meta=None):\n",
        "    \"\"\"\n",
        "    Специализированный интерактивный режим на основе ваших ключевых слов\n",
        "    \"\"\"\n",
        "    print(\"🚀 СПЕЦИАЛИЗИРОВАННЫЙ РЕЖИМ НА ОСНОВЕ ВАШИХ КЛЮЧЕВЫХ СЛОВ\")\n",
        "    print(\"=\" * 80)\n",
        "    print(\"Специализация:\")\n",
        "    print(\"  🧬 Биомаркеры старения человека\")\n",
        "    print(\"  🔬 Эпигенетические часы\")\n",
        "    print(\"  ⚡ Воспаление при старении\")\n",
        "    print(\"  🧪 Специфичные гены (ATAD3A, COL18A1, GATA4, etc.)\")\n",
        "    print(\"  🏥 Терапевтические мишени\")\n",
        "    print(\"  🧠 Иммунное старение\")\n",
        "    print(\"  🔀 Пути долголетия\")\n",
        "    print(\"  📊 Одноклеточный анализ\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Пытаемся найти граф в разных местах\n",
        "    G_to_use = None\n",
        "    node_meta_to_use = None\n",
        "    edge_meta_to_use = None\n",
        "\n",
        "    if graph is not None and node_meta is not None:\n",
        "        G_to_use = graph\n",
        "        node_meta_to_use = node_meta\n",
        "        edge_meta_to_use = edge_meta\n",
        "    else:\n",
        "        # Пытаемся найти в глобальных переменных\n",
        "        try:\n",
        "            import inspect\n",
        "            frame = inspect.currentframe()\n",
        "            while frame:\n",
        "                if 'G' in frame.f_globals and 'node_full_meta' in frame.f_globals:\n",
        "                    G_to_use = frame.f_globals['G']\n",
        "                    node_meta_to_use = frame.f_globals['node_full_meta']\n",
        "                    edge_meta_to_use = frame.f_globals.get('edge_full_meta')\n",
        "                    break\n",
        "                frame = frame.f_back\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    if G_to_use is None:\n",
        "        print(\"❌ Граф не найден!\")\n",
        "        print(\"💡 Используйте: specialized_interactive_mode(G, node_full_meta, edge_full_meta)\")\n",
        "        print(\"   где G, node_full_meta, edge_full_meta - ваши переменные\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        # Создаем специализированные компоненты\n",
        "        print(f\"📊 Используем граф: {G_to_use.number_of_nodes():,} узлов, {G_to_use.number_of_edges():,} рёбер\")\n",
        "\n",
        "        specialized_selector = SpecializedTermSelector(G_to_use, node_meta_to_use)\n",
        "\n",
        "        from openai import OpenAI\n",
        "        client = OpenAI(base_url=\"http://80.209.242.40:8000/v1\", api_key=\"dummy-key\")\n",
        "\n",
        "        # Создаем временный scientific_generator для совместимости\n",
        "        class TempScientificGenerator:\n",
        "            def __init__(self, G, node_meta, edge_meta, client):\n",
        "                self.G = G\n",
        "                self.node_full_meta = node_meta\n",
        "                self.edge_full_meta = edge_meta\n",
        "                self.client = client\n",
        "\n",
        "        scientific_generator = TempScientificGenerator(G_to_use, node_meta_to_use, edge_meta_to_use, client)\n",
        "        specialized_generator = OptimizedHypothesisGenerator(scientific_generator)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Ошибка создания компонентов: {e}\")\n",
        "        return\n",
        "\n",
        "    current_term = None\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            if current_term is None:\n",
        "                print(f\"\\n{'🎯' * 20}\")\n",
        "                print(\"ВЫБОР СПЕЦИАЛИЗИРОВАННОГО ТЕРМИНА\")\n",
        "                print(f\"{'🎯' * 20}\")\n",
        "\n",
        "                specialized_selector.show_categories_specialized()\n",
        "\n",
        "                choice = input(\"\\n👆 Выберите категорию (1-8), 0 для ручного ввода, 99 для случайного: \").strip()\n",
        "\n",
        "                if choice == '0':\n",
        "                    # Ручной ввод с валидацией\n",
        "                    manual_term = input(\"✍️ Введите термин: \").strip()\n",
        "                    if manual_term:\n",
        "                        is_valid, message, quality_info = specialized_selector.validate_manual_term(manual_term)\n",
        "\n",
        "                        print(f\"\\n🔍 ВАЛИДАЦИЯ ТЕРМИНА '{manual_term}':\")\n",
        "                        print(message)\n",
        "\n",
        "                        if quality_info:\n",
        "                            print(f\"\\n📊 ХАРАКТЕРИСТИКИ:\")\n",
        "                            print(f\"   🔗 Всего связей: {quality_info['degree']}\")\n",
        "                            print(f\"   🎯 Специализированных соседей: {quality_info['specialized_neighbors']}\")\n",
        "                            print(f\"   🔑 Соответствует ключевым словам: {'✅' if quality_info['matches_keywords'] else '❌'}\")\n",
        "                            print(f\"   📚 DOI: {'✅' if quality_info['has_doi'] else '❌'}\")\n",
        "                            print(f\"   📈 Специализированный скор: {quality_info['spec_score']:.1f}\")\n",
        "\n",
        "                        if not is_valid:\n",
        "                            confirm = input(f\"\\n❓ Продолжить с этим термином несмотря на низкое качество? (y/N): \").strip().lower()\n",
        "                            if confirm not in ['y', 'yes', 'да', 'д']:\n",
        "                                print(\"❌ Выбор термина отменен\")\n",
        "                                continue\n",
        "\n",
        "                        current_term = manual_term\n",
        "                        if is_valid:\n",
        "                            print(f\"✅ Термин принят для анализа\")\n",
        "                        else:\n",
        "                            print(f\"⚠️ Термин принят с предупреждениями\")\n",
        "\n",
        "                        specialized_selector.show_term_details(current_term)\n",
        "                    continue\n",
        "\n",
        "                elif choice == '99':\n",
        "                    current_term = specialized_selector.get_random_term()\n",
        "                    if current_term:\n",
        "                        print(f\"🎲 Выбран случайный качественный термин: {current_term}\")\n",
        "\n",
        "                        # Показываем почему этот термин качественный\n",
        "                        spec_neighbors = specialized_selector._count_specialized_neighbors(current_term)\n",
        "                        doi_status = \"📚\" if current_term in specialized_selector._doi_cache else \"❌\"\n",
        "                        score = specialized_selector._calculate_specialized_score(current_term)\n",
        "\n",
        "                        print(f\"✅ КАЧЕСТВЕННЫЕ ПОКАЗАТЕЛИ:\")\n",
        "                        print(f\"   🎯 Специализированных соседей: {spec_neighbors} (≥3 ✅)\")\n",
        "                        print(f\"   📚 DOI: {doi_status}\")\n",
        "                        print(f\"   📈 Специализированный скор: {score:.1f}\")\n",
        "\n",
        "                        specialized_selector.show_term_details(current_term)\n",
        "                    else:\n",
        "                        print(\"❌ Не удалось найти качественный термин\")\n",
        "                        print(\"💡 Попробуйте расширить ключевые слова или проверить данные\")\n",
        "                    continue\n",
        "\n",
        "                elif choice.lower() in ['quit', 'exit', 'q']:\n",
        "                    break\n",
        "\n",
        "                else:\n",
        "                    try:\n",
        "                        category_num = int(choice)\n",
        "                        category_name, terms = specialized_selector.get_category_terms(category_num)\n",
        "\n",
        "                        if terms:\n",
        "                            print(f\"\\n📋 {category_name}\")\n",
        "                            print(\"=\" * 60)\n",
        "\n",
        "                            for i, term in enumerate(terms, 1):\n",
        "                                metadata = specialized_selector.get_term_metadata(term)\n",
        "                                doi_status = \"📚\" if metadata['doi'] else \"❌\"\n",
        "                                keywords_count = len(metadata['matches_keywords'])\n",
        "                                print(f\"{i:2}. {term:<35} (связей: {metadata['degree']}, DOI: {doi_status}, КС: {keywords_count}, скор: {metadata['specialized_score']:.1f})\")\n",
        "\n",
        "                            term_choice = input(f\"\\n👆 Выберите термин (1-{len(terms)}): \").strip()\n",
        "                            term_idx = int(term_choice) - 1\n",
        "\n",
        "                            if 0 <= term_idx < len(terms):\n",
        "                                current_term = terms[term_idx]\n",
        "                                specialized_selector.show_term_details(current_term)\n",
        "\n",
        "                    except (ValueError, IndexError):\n",
        "                        print(\"⚠️ Неверный выбор\")\n",
        "                        continue\n",
        "\n",
        "            else:\n",
        "                # Работа с выбранным термином\n",
        "                print(f\"\\n{'🔬' * 30}\")\n",
        "                print(f\"СПЕЦИАЛИЗИРОВАННЫЙ АНАЛИЗ: {current_term.upper()}\")\n",
        "                print(f\"{'🔬' * 30}\")\n",
        "\n",
        "                print(\"Выберите действие:\")\n",
        "                print(\"1. 🔍 Анализ специализированных связей\")\n",
        "                print(\"2. 💡 Генерация клинических гипотез\")\n",
        "                print(\"3. 📚 Показать метаданные и DOI\")\n",
        "                print(\"4. 🔄 Выбрать новый термин\")\n",
        "                print(\"5. ❌ Выход\")\n",
        "\n",
        "                action = input(\"\\n👆 Выберите действие (1-5): \").strip()\n",
        "\n",
        "                if action == '1':\n",
        "                    print(\"⏳ Анализ специализированных связей...\")\n",
        "                    spec_data = specialized_generator.analyze_specialized_connections(current_term)\n",
        "\n",
        "                    if spec_data:\n",
        "                        quality_check = spec_data.get('quality_check', {})\n",
        "\n",
        "                        print(f\"\\n📊 РЕЗУЛЬТАТЫ АНАЛИЗА:\")\n",
        "                        print(f\"✅ Найдено {len(spec_data['specialized_neighbors'])} специализированных связей\")\n",
        "                        print(f\"📚 DOI доступны для {len(spec_data['doi_metadata'])} терминов\")\n",
        "\n",
        "                        # Оценка качества\n",
        "                        if quality_check.get('sufficient_neighbors') and quality_check.get('has_doi'):\n",
        "                            print(f\"🌟 КАЧЕСТВО: Отличное - достаточно связей и есть DOI\")\n",
        "                        elif quality_check.get('sufficient_neighbors'):\n",
        "                            print(f\"⚠️ КАЧЕСТВО: Хорошее - достаточно связей, но мало DOI\")\n",
        "                        elif quality_check.get('has_doi'):\n",
        "                            print(f\"⚠️ КАЧЕСТВО: Среднее - есть DOI, но мало связей\")\n",
        "                        else:\n",
        "                            print(f\"❌ КАЧЕСТВО: Низкое - мало связей и нет DOI\")\n",
        "\n",
        "                        # Показываем топ связи\n",
        "                        if len(spec_data['neighbor_scores']) > 0:\n",
        "                            print(f\"\\n🔗 Топ-5 специализированных связей:\")\n",
        "                            for i, (neighbor, score) in enumerate(spec_data['neighbor_scores'][:5], 1):\n",
        "                                doi_status = \"📚\" if neighbor in spec_data['doi_metadata'] else \"❌\"\n",
        "                                print(f\"   {i}. {neighbor} (скор: {score:.1f}, DOI: {doi_status})\")\n",
        "\n",
        "                        # Рекомендации\n",
        "                        if not quality_check.get('sufficient_neighbors'):\n",
        "                            print(f\"\\n💡 РЕКОМЕНДАЦИЯ: Выберите термин с большим количеством релевантных связей\")\n",
        "                        if not quality_check.get('has_doi'):\n",
        "                            print(f\"💡 РЕКОМЕНДАЦИЯ: Проверьте наличие DOI/PMID в метаданных\")\n",
        "                    else:\n",
        "                        print(f\"❌ Анализ не удался\")\n",
        "\n",
        "                elif action == '2':\n",
        "                    print(\"⏳ Генерация конкретных клинических гипотез...\")\n",
        "\n",
        "                    # Запрашиваем температуру у пользователя\n",
        "                    temp_input = input(\"🌡️ Выберите температуру (0.1-1.0) [по умолчанию 0.75]: \").strip()\n",
        "                    try:\n",
        "                        if temp_input:\n",
        "                            temperature = float(temp_input)\n",
        "                            if not (0.1 <= temperature <= 1.0):\n",
        "                                print(\"⚠️ Температура должна быть от 0.1 до 1.0, используем 0.75\")\n",
        "                                temperature = 0.75\n",
        "                        else:\n",
        "                            temperature = 0.75\n",
        "                    except ValueError:\n",
        "                        print(\"⚠️ Неверный формат, используем температуру 0.75\")\n",
        "                        temperature = 0.75\n",
        "\n",
        "                    print(f\"🌡️ Используем температуру: {temperature}\")\n",
        "                    print(\"💡 Совет: 0.1-0.3=консервативно, 0.4-0.7=сбалансированно, 0.8-1.0=креативно\")\n",
        "\n",
        "                    spec_data = specialized_generator.analyze_specialized_connections(current_term)\n",
        "\n",
        "                    if spec_data:\n",
        "                        hypotheses = specialized_generator.generate_specialized_hypotheses(\n",
        "                            current_term, spec_data, 5, temperature\n",
        "                        )\n",
        "\n",
        "                        if hypotheses:\n",
        "                            specialized_generator.display_specialized_hypotheses(hypotheses)\n",
        "\n",
        "                elif action == '3':\n",
        "                    metadata = specialized_selector.get_term_metadata(current_term)\n",
        "                    print(f\"\\n📚 МЕТАДАННЫЕ ДЛЯ '{current_term}':\")\n",
        "                    print(f\"DOI: {metadata['doi'] or 'не найден'}\")\n",
        "                    print(f\"Ключевые слова: {', '.join(metadata['matches_keywords'][:10])}\")\n",
        "                    print(f\"Специализированный скор: {metadata['specialized_score']:.1f}\")\n",
        "\n",
        "                elif action == '4':\n",
        "                    current_term = None\n",
        "                    continue\n",
        "\n",
        "                elif action == '5':\n",
        "                    break\n",
        "\n",
        "                else:\n",
        "                    print(\"⚠️ Неверный выбор\")\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\n⏹️ Прерывание (Ctrl+C)\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Ошибка: {e}\")\n",
        "            continue\n",
        "\n",
        "# === ДИАГНОСТИЧЕСКАЯ ФУНКЦИЯ ===\n",
        "# === ДИАГНОСТИЧЕСКИЕ ФУНКЦИИ ===\n",
        "def explore_graph_terms(G, node_full_meta, top_n=50):\n",
        "    \"\"\"\n",
        "    Исследование реальных терминов в графе для настройки системы\n",
        "    \"\"\"\n",
        "    print(\"🔍 ИССЛЕДОВАНИЕ ТЕРМИНОВ В ВАШЕМ ГРАФЕ\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    if not G or G.number_of_nodes() == 0:\n",
        "        print(\"❌ Граф пуст\")\n",
        "        return\n",
        "\n",
        "    print(f\"📊 Всего узлов в графе: {G.number_of_nodes():,}\")\n",
        "    print(f\"📊 Всего рёбер в графе: {G.number_of_edges():,}\")\n",
        "\n",
        "    # Получаем все узлы с их степенями\n",
        "    degree_cache = dict(G.degree())\n",
        "    terms_by_degree = [(term, degree) for term, degree in degree_cache.items()]\n",
        "    terms_by_degree.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    print(f\"\\n🔝 ТОП-{top_n} ТЕРМИНОВ ПО КОЛИЧЕСТВУ СВЯЗЕЙ:\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    for i, (term, degree) in enumerate(terms_by_degree[:top_n], 1):\n",
        "        # Проверяем наличие DOI\n",
        "        doi_status = \"📚\" if term in node_full_meta and _has_doi(node_full_meta[term]) else \"❌\"\n",
        "        print(f\"{i:2}. {term:<45} (связей: {degree:3}, DOI: {doi_status})\")\n",
        "\n",
        "    # Анализируем по словам\n",
        "    print(f\"\\n🔍 АНАЛИЗ ПО БИОЛОГИЧЕСКИМ СЛОВАМ:\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    bio_words = {\n",
        "        'human': [], 'cell': [], 'gene': [], 'protein': [], 'dna': [], 'aging': [],\n",
        "        'inflammation': [], 'immune': [], 'therapy': [], 'methylation': [],\n",
        "        'expression': [], 'signaling': [], 'pathway': [], 'cancer': [], 'diabetes': []\n",
        "    }\n",
        "\n",
        "    # Ищем термины, содержащие биологические слова\n",
        "    for term, degree in terms_by_degree[:200]:  # Проверяем топ-200\n",
        "        term_lower = term.lower()\n",
        "        for bio_word in bio_words:\n",
        "            if bio_word in term_lower and degree >= 3:  # Минимум 3 связи\n",
        "                bio_words[bio_word].append((term, degree))\n",
        "\n",
        "    # Показываем результаты\n",
        "    for bio_word, terms in bio_words.items():\n",
        "        if terms:\n",
        "            terms.sort(key=lambda x: x[1], reverse=True)  # Сортируем по степени\n",
        "            top_terms = terms[:5]  # Топ-5\n",
        "            print(f\"\\n🔬 '{bio_word}' ({len(terms)} терминов):\")\n",
        "            for i, (term, degree) in enumerate(top_terms, 1):\n",
        "                doi_status = \"📚\" if term in node_full_meta and _has_doi(node_full_meta[term]) else \"❌\"\n",
        "                print(f\"   {i}. {term:<40} (связей: {degree}, DOI: {doi_status})\")\n",
        "\n",
        "    print(f\"\\n💡 РЕКОМЕНДАЦИИ ДЛЯ НАСТРОЙКИ:\")\n",
        "    print(\"1. Используйте термины из списка выше для анализа\")\n",
        "    print(\"2. Выберите термин с высокой степенью связности (>10)\")\n",
        "    print(\"3. Предпочтительно с DOI для научного обоснования\")\n",
        "    print(\"4. Или запустите режим 'без фильтрации' для работы с любыми терминами\")\n",
        "\n",
        "def _has_doi(meta):\n",
        "    \"\"\"Проверка наличия DOI в метаданных\"\"\"\n",
        "    if isinstance(meta, dict):\n",
        "        doi_fields = ['dois', 'doi', 'DOI', 'Doi']\n",
        "        for field in doi_fields:\n",
        "            if field in meta:\n",
        "                value = meta[field]\n",
        "                if isinstance(value, list):\n",
        "                    return any(d and str(d).strip() and str(d).strip().lower() not in ['none', 'nan', 'null'] for d in value)\n",
        "                elif value and str(value).strip() and str(value).strip().lower() not in ['none', 'nan', 'null']:\n",
        "                    return True\n",
        "    return False\n",
        "\n",
        "def create_custom_categories(G, node_full_meta, min_degree=5):\n",
        "    \"\"\"\n",
        "    Создание категорий на основе реальных данных в графе\n",
        "    \"\"\"\n",
        "    print(\"🏗️ СОЗДАНИЕ КАТЕГОРИЙ НА ОСНОВЕ РЕАЛЬНЫХ ДАННЫХ\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Получаем популярные термины\n",
        "    degree_cache = dict(G.degree())\n",
        "    popular_terms = [(term, degree) for term, degree in degree_cache.items() if degree >= min_degree]\n",
        "    popular_terms.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    print(f\"📊 Найдено {len(popular_terms)} терминов с ≥{min_degree} связями\")\n",
        "\n",
        "    # Автоматически группируем по общим словам\n",
        "    auto_categories = {\n",
        "        '🧬 Человек и возраст': {\n",
        "            'patterns': [r'human', r'age', r'aging', r'old', r'adult', r'elderly'],\n",
        "            'terms': []\n",
        "        },\n",
        "        '🔬 Клетки и биология': {\n",
        "            'patterns': [r'cell', r'cellular', r'tissue', r'organ', r'biological'],\n",
        "            'terms': []\n",
        "        },\n",
        "        '🧪 Гены и белки': {\n",
        "            'patterns': [r'gene', r'protein', r'enzyme', r'receptor', r'expression'],\n",
        "            'terms': []\n",
        "        },\n",
        "        '⚡ Болезни и патология': {\n",
        "            'patterns': [r'cancer', r'diabetes', r'disease', r'disorder', r'pathology'],\n",
        "            'terms': []\n",
        "        },\n",
        "        '🔥 Иммунитет и воспаление': {\n",
        "            'patterns': [r'immune', r'inflammation', r'inflammatory', r'cytokine'],\n",
        "            'terms': []\n",
        "        },\n",
        "        '💊 Лечение и терапия': {\n",
        "            'patterns': [r'therapy', r'treatment', r'drug', r'therapeutic', r'clinical'],\n",
        "            'terms': []\n",
        "        },\n",
        "        '📊 Молекулярные процессы': {\n",
        "            'patterns': [r'dna', r'rna', r'methylation', r'signaling', r'pathway'],\n",
        "            'terms': []\n",
        "        },\n",
        "        '🎯 Высокосвязанные термины': {\n",
        "            'patterns': [],  # Для самых популярных терминов\n",
        "            'terms': []\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Распределяем термины\n",
        "    unmatched_terms = []\n",
        "\n",
        "    for term, degree in popular_terms[:200]:  # Анализируем топ-200\n",
        "        term_lower = term.lower()\n",
        "        matched = False\n",
        "\n",
        "        # Проверяем паттерны для каждой категории\n",
        "        for category, data in list(auto_categories.items())[:-1]:  # Исключаем последнюю категорию\n",
        "            for pattern in data['patterns']:\n",
        "                if re.search(pattern, term_lower):\n",
        "                    data['terms'].append(term)\n",
        "                    matched = True\n",
        "                    break\n",
        "            if matched:\n",
        "                break\n",
        "\n",
        "        if not matched:\n",
        "            unmatched_terms.append((term, degree))\n",
        "\n",
        "    # Добавляем самые популярные неопознанные термины в последнюю категорию\n",
        "    auto_categories['🎯 Высокосвязанные термины']['terms'] = [\n",
        "        term for term, _ in unmatched_terms[:15]\n",
        "    ]\n",
        "\n",
        "    # Ограничиваем количество терминов в каждой категории\n",
        "    for category, data in auto_categories.items():\n",
        "        if len(data['terms']) > 15:\n",
        "            # Сортируем по степени и берем топ-15\n",
        "            terms_with_degree = [(term, degree_cache.get(term, 0)) for term in data['terms']]\n",
        "            terms_with_degree.sort(key=lambda x: x[1], reverse=True)\n",
        "            data['terms'] = [term for term, _ in terms_with_degree[:15]]\n",
        "\n",
        "    # Показываем результаты\n",
        "    print(f\"\\n📋 АВТОМАТИЧЕСКИ СОЗДАННЫЕ КАТЕГОРИИ:\")\n",
        "    total_terms = 0\n",
        "    for category, data in auto_categories.items():\n",
        "        terms_count = len(data['terms'])\n",
        "        total_terms += terms_count\n",
        "        print(f\"\\n{category} ({terms_count} терминов):\")\n",
        "\n",
        "        if terms_count > 0:\n",
        "            for i, term in enumerate(data['terms'][:5], 1):  # Показываем топ-5\n",
        "                degree = degree_cache.get(term, 0)\n",
        "                doi_status = \"📚\" if _has_doi(node_full_meta.get(term, {})) else \"❌\"\n",
        "                print(f\"   {i}. {term:<35} (связей: {degree}, DOI: {doi_status})\")\n",
        "            if terms_count > 5:\n",
        "                print(f\"   ... и еще {terms_count - 5} терминов\")\n",
        "        else:\n",
        "            print(\"   Термины не найдены\")\n",
        "\n",
        "    print(f\"\\n✅ Всего подготовлено {total_terms} терминов для анализа\")\n",
        "    return auto_categories\n",
        "\n",
        "def enhanced_no_filter_mode(G, node_full_meta, edge_full_meta):\n",
        "    \"\"\"\n",
        "    Расширенный режим без фильтрации с улучшенным генератором гипотез\n",
        "    \"\"\"\n",
        "    print(\"🚀 РАСШИРЕННЫЙ РЕЖИМ БЕЗ ФИЛЬТРАЦИИ\")\n",
        "    print(\"=\" * 70)\n",
        "    print(\"Возможности:\")\n",
        "    print(\"• 💡 Умная генерация гипотез с контролем качества\")\n",
        "    print(\"• 🔧 Адаптивные промпты под ваши данные\")\n",
        "    print(\"• 📊 Детальная оценка качества терминов\")\n",
        "    print(\"• 🧪 Конкретные измеримые параметры\")\n",
        "    print(\"• 📚 Интеграция DOI источников\")\n",
        "    print(\"• ✅ Резервные механизмы для любых данных\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    try:\n",
        "        from openai import OpenAI\n",
        "        client = OpenAI(base_url=\"http://80.209.242.40:8000/v1\", api_key=\"dummy-key\")\n",
        "\n",
        "        # Создаем улучшенный генератор\n",
        "        enhanced_generator = EnhancedHypothesisGenerator(G, node_full_meta, edge_full_meta, client)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Ошибка создания компонентов: {e}\")\n",
        "        return\n",
        "\n",
        "    current_term = None\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            if current_term is None:\n",
        "                print(f\"\\n{'🎯' * 20}\")\n",
        "                print(\"ВЫБОР ТЕРМИНА ДЛЯ УЛУЧШЕННОГО АНАЛИЗА\")\n",
        "                print(f\"{'🎯' * 20}\")\n",
        "\n",
        "                print(\"Варианты:\")\n",
        "                print(\"1. 🔍 Исследовать граф (популярные термины)\")\n",
        "                print(\"2. ✍️ Ввести термин вручную\")\n",
        "                print(\"3. 🎲 Случайный качественный термин\")\n",
        "                print(\"4. 🏆 Рекомендуемые термины (на основе ваших данных)\")\n",
        "                print(\"5. ❌ Выход\")\n",
        "\n",
        "                choice = input(\"\\n👆 Выберите действие (1-5): \").strip()\n",
        "\n",
        "                if choice == '1':\n",
        "                    explore_graph_terms(G, node_full_meta, 30)\n",
        "                    continue\n",
        "\n",
        "                elif choice == '2':\n",
        "                    manual_term = input(\"✍️ Введите термин: \").strip()\n",
        "                    if manual_term and manual_term in G:\n",
        "                        current_term = manual_term\n",
        "\n",
        "                        # Показываем мгновенную оценку\n",
        "                        quality_info = enhanced_generator.assess_term_quality(manual_term)\n",
        "                        enhanced_generator._display_quality_info(quality_info)\n",
        "                    else:\n",
        "                        print(f\"❌ Термин '{manual_term}' не найден в графе\")\n",
        "                    continue\n",
        "\n",
        "                elif choice == '3':\n",
        "                    degree_cache = dict(G.degree())\n",
        "                    # Ищем термины с DOI и хорошими связями\n",
        "                    good_terms = []\n",
        "                    for term, degree in degree_cache.items():\n",
        "                        if degree >= 3 and enhanced_generator._count_doi(term) > 0:\n",
        "                            good_terms.append((term, degree, enhanced_generator._count_doi(term)))\n",
        "\n",
        "                    if good_terms:\n",
        "                        # Сортируем по комбинированному скору\n",
        "                        good_terms.sort(key=lambda x: x[1] + x[2]*5, reverse=True)\n",
        "                        current_term = random.choice(good_terms[:20])[0]  # Из топ-20\n",
        "\n",
        "                        print(f\"🎲 Выбран качественный термин: {current_term}\")\n",
        "                        quality_info = enhanced_generator.assess_term_quality(current_term)\n",
        "                        enhanced_generator._display_quality_info(quality_info)\n",
        "                    else:\n",
        "                        # Резервный выбор\n",
        "                        popular_terms = [(term, degree) for term, degree in degree_cache.items() if degree >= 5]\n",
        "                        if popular_terms:\n",
        "                            current_term = random.choice(popular_terms)[0]\n",
        "                            print(f\"🎲 Выбран популярный термин: {current_term}\")\n",
        "                        else:\n",
        "                            print(\"❌ Подходящие термины не найдены\")\n",
        "                    continue\n",
        "\n",
        "                elif choice == '4':\n",
        "                    # Показываем рекомендуемые термины на основе анализа их данных\n",
        "                    print(\"\\n🏆 РЕКОМЕНДУЕМЫЕ ТЕРМИНЫ НА ОСНОВЕ ВАШИХ ДАННЫХ:\")\n",
        "                    print(\"(высокое качество: много связей + DOI)\")\n",
        "\n",
        "                    degree_cache = dict(G.degree())\n",
        "                    recommendations = []\n",
        "\n",
        "                    for term, degree in degree_cache.items():\n",
        "                        doi_count = enhanced_generator._count_doi(term)\n",
        "                        if degree >= 5 and doi_count > 0:\n",
        "                            spec_score = enhanced_generator._calculate_specialized_score(term)\n",
        "                            recommendations.append((term, degree, doi_count, spec_score))\n",
        "\n",
        "                    # Сортируем по качеству\n",
        "                    recommendations.sort(key=lambda x: x[1] + x[2]*5 + x[3], reverse=True)\n",
        "\n",
        "                    print(f\"\\nТоп-10 рекомендованных терминов:\")\n",
        "                    for i, (term, degree, doi_count, spec_score) in enumerate(recommendations[:10], 1):\n",
        "                        print(f\"{i:2}. {term:<35} (связей: {degree}, DOI: {doi_count}, скор: {spec_score:.1f})\")\n",
        "\n",
        "                    if recommendations:\n",
        "                        term_choice = input(f\"\\n👆 Выберите термин (1-{min(10, len(recommendations))}) или 0 для возврата: \").strip()\n",
        "                        try:\n",
        "                            term_idx = int(term_choice) - 1\n",
        "                            if 0 <= term_idx < min(10, len(recommendations)):\n",
        "                                current_term = recommendations[term_idx][0]\n",
        "                                print(f\"✅ Выбран: {current_term}\")\n",
        "                        except (ValueError, IndexError):\n",
        "                            if term_choice != '0':\n",
        "                                print(\"⚠️ Неверный выбор\")\n",
        "                    continue\n",
        "\n",
        "                elif choice == '5':\n",
        "                    break\n",
        "\n",
        "                else:\n",
        "                    print(\"⚠️ Неверный выбор\")\n",
        "                    continue\n",
        "\n",
        "            else:\n",
        "                # Работа с выбранным термином\n",
        "                print(f\"\\n{'🔬' * 40}\")\n",
        "                print(f\"УЛУЧШЕННЫЙ АНАЛИЗ: {current_term.upper()}\")\n",
        "                print(f\"{'🔬' * 40}\")\n",
        "\n",
        "                print(\"Выберите действие:\")\n",
        "                print(\"1. 📊 Оценка качества термина\")\n",
        "                print(\"2. 🔍 Анализ связей и контекста\")\n",
        "                print(\"3. 💡 Генерация гипотез (стандарт)\")\n",
        "                print(\"4. 🔥 Генерация гипотез (высокая креативность)\")\n",
        "                print(\"5. ❄️ Генерация гипотез (консервативно)\")\n",
        "                print(\"6. 🎛️ Кастомная генерация (выбор параметров)\")\n",
        "                print(\"7. 🔄 Выбрать новый термин\")\n",
        "                print(\"8. ❌ Выход\")\n",
        "\n",
        "                action = input(\"\\n👆 Выберите действие (1-8): \").strip()\n",
        "\n",
        "                if action == '1':\n",
        "                    print(\"⏳ Оценка качества термина...\")\n",
        "                    quality_info = enhanced_generator.assess_term_quality(current_term)\n",
        "                    enhanced_generator._display_quality_info(quality_info)\n",
        "\n",
        "                    # Дополнительная информация\n",
        "                    connections = enhanced_generator.analyze_connections(current_term)\n",
        "                    print(f\"\\n🔗 АНАЛИЗ СВЯЗЕЙ:\")\n",
        "                    print(f\"   Всего соседей: {connections['total_neighbors']}\")\n",
        "                    print(f\"   С DOI: {connections['high_quality_neighbors']}\")\n",
        "                    print(f\"   Средняя степень соседей: {connections['avg_degree']:.1f}\")\n",
        "\n",
        "                elif action == '2':\n",
        "                    print(\"⏳ Детальный анализ связей...\")\n",
        "                    connections = enhanced_generator.analyze_connections(current_term)\n",
        "\n",
        "                    print(f\"\\n🔗 ДЕТАЛЬНЫЙ АНАЛИЗ СВЯЗЕЙ ДЛЯ '{current_term}':\")\n",
        "                    print(f\"   Всего соседей: {connections['total_neighbors']}\")\n",
        "                    print(f\"   С DOI источниками: {connections['high_quality_neighbors']}\")\n",
        "\n",
        "                    if connections['neighbors']:\n",
        "                        print(f\"\\n🏆 Топ-10 связанных терминов:\")\n",
        "                        for i, neighbor in enumerate(connections['neighbors'][:10], 1):\n",
        "                            doi_status = \"📚\" if neighbor['has_doi'] else \"❌\"\n",
        "                            print(f\"   {i:2}. {neighbor['term']:<30} (связей: {neighbor['degree']}, DOI: {doi_status}, скор: {neighbor['spec_score']:.1f})\")\n",
        "\n",
        "                elif action == '3':\n",
        "                    print(\"⏳ Стандартная генерация гипотез...\")\n",
        "                    result = enhanced_generator.generate_hypotheses(current_term, count=5, temperature=0.7)\n",
        "                    if result:\n",
        "                        enhanced_generator.display_hypotheses(result)\n",
        "\n",
        "                elif action == '4':\n",
        "                    print(\"⏳ Креативная генерация гипотез...\")\n",
        "                    result = enhanced_generator.generate_hypotheses(current_term, count=5, temperature=0.9)\n",
        "                    if result:\n",
        "                        enhanced_generator.display_hypotheses(result)\n",
        "\n",
        "                elif action == '5':\n",
        "                    print(\"⏳ Консервативная генерация гипотез...\")\n",
        "                    result = enhanced_generator.generate_hypotheses(current_term, count=5, temperature=0.3)\n",
        "                    if result:\n",
        "                        enhanced_generator.display_hypotheses(result)\n",
        "\n",
        "                elif action == '6':\n",
        "                    print(\"🎛️ Кастомная настройка генерации:\")\n",
        "\n",
        "                    # Количество гипотез\n",
        "                    count_input = input(\"   📊 Количество гипотез (1-10) [5]: \").strip()\n",
        "                    try:\n",
        "                        count = int(count_input) if count_input else 5\n",
        "                        count = max(1, min(10, count))\n",
        "                    except ValueError:\n",
        "                        count = 5\n",
        "\n",
        "                    # Температура\n",
        "                    temp_input = input(\"   🌡️ Температура (0.1-1.0) [0.7]: \").strip()\n",
        "                    try:\n",
        "                        temperature = float(temp_input) if temp_input else 0.7\n",
        "                        temperature = max(0.1, min(1.0, temperature))\n",
        "                    except ValueError:\n",
        "                        temperature = 0.7\n",
        "\n",
        "                    print(f\"⏳ Генерация {count} гипотез с температурой {temperature}...\")\n",
        "                    result = enhanced_generator.generate_hypotheses(current_term, count=count, temperature=temperature)\n",
        "                    if result:\n",
        "                        enhanced_generator.display_hypotheses(result)\n",
        "\n",
        "                elif action == '7':\n",
        "                    current_term = None\n",
        "                    continue\n",
        "\n",
        "                elif action == '8':\n",
        "                    break\n",
        "\n",
        "                else:\n",
        "                    print(\"⚠️ Неверный выбор\")\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\n⏹️ Прерывание (Ctrl+C)\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Ошибка: {e}\")\n",
        "            continue\n",
        "    \"\"\"\n",
        "    Режим без фильтрации - работа с любыми терминами\n",
        "    \"\"\"\n",
        "    print(\"🚀 РЕЖИМ БЕЗ ФИЛЬТРАЦИИ - РАБОТА С ЛЮБЫМИ ТЕРМИНАМИ\")\n",
        "    print(\"=\" * 70)\n",
        "    print(\"В этом режиме вы можете:\")\n",
        "    print(\"• Ввести любой термин из графа\")\n",
        "    print(\"• Получить анализ его связей\")\n",
        "    print(\"• Сгенерировать гипотезы на основе его соседей\")\n",
        "    print(\"• Не ограничиваться ключевыми словами\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Создаем простые компоненты без фильтрации\n",
        "    try:\n",
        "        from openai import OpenAI\n",
        "        client = OpenAI(base_url=\"http://80.209.242.40:8000/v1\", api_key=\"dummy-key\")\n",
        "\n",
        "        class TempScientificGenerator:\n",
        "            def __init__(self, G, node_meta, edge_meta, client):\n",
        "                self.G = G\n",
        "                self.node_full_meta = node_meta\n",
        "                self.edge_full_meta = edge_meta\n",
        "                self.client = client\n",
        "\n",
        "        scientific_generator = TempScientificGenerator(G, node_full_meta, edge_full_meta, client)\n",
        "        optimized_generator = OptimizedHypothesisGenerator(scientific_generator)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Ошибка создания компонентов: {e}\")\n",
        "        return\n",
        "\n",
        "    current_term = None\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            if current_term is None:\n",
        "                print(f\"\\n{'🎯' * 20}\")\n",
        "                print(\"ВЫБОР ЛЮБОГО ТЕРМИНА ИЗ ГРАФА\")\n",
        "                print(f\"{'🎯' * 20}\")\n",
        "\n",
        "                print(\"Варианты:\")\n",
        "                print(\"1. 🔍 Исследовать граф (показать популярные термины)\")\n",
        "                print(\"2. ✍️ Ввести термин вручную\")\n",
        "                print(\"3. 🎲 Случайный популярный термин\")\n",
        "                print(\"4. ❌ Выход\")\n",
        "\n",
        "                choice = input(\"\\n👆 Выберите действие (1-4): \").strip()\n",
        "\n",
        "                if choice == '1':\n",
        "                    explore_graph_terms(G, node_full_meta, 30)\n",
        "                    continue\n",
        "\n",
        "                elif choice == '2':\n",
        "                    manual_term = input(\"✍️ Введите термин: \").strip()\n",
        "                    if manual_term and manual_term in G:\n",
        "                        current_term = manual_term\n",
        "                        degree = G.degree(manual_term)\n",
        "                        neighbors_count = len(list(G.neighbors(manual_term)))\n",
        "                        doi_status = \"📚\" if _has_doi(node_full_meta.get(manual_term, {})) else \"❌\"\n",
        "\n",
        "                        print(f\"\\n✅ ТЕРМИН НАЙДЕН: {current_term}\")\n",
        "                        print(f\"   🔗 Связей: {degree}\")\n",
        "                        print(f\"   👥 Соседей: {neighbors_count}\")\n",
        "                        print(f\"   📚 DOI: {doi_status}\")\n",
        "                    else:\n",
        "                        print(f\"❌ Термин '{manual_term}' не найден в графе\")\n",
        "                    continue\n",
        "\n",
        "                elif choice == '3':\n",
        "                    degree_cache = dict(G.degree())\n",
        "                    popular_terms = [(term, degree) for term, degree in degree_cache.items() if degree >= 10]\n",
        "                    if popular_terms:\n",
        "                        current_term = random.choice(popular_terms)[0]\n",
        "                        degree = degree_cache[current_term]\n",
        "                        doi_status = \"📚\" if _has_doi(node_full_meta.get(current_term, {})) else \"❌\"\n",
        "\n",
        "                        print(f\"🎲 Выбран случайный популярный термин: {current_term}\")\n",
        "                        print(f\"   🔗 Связей: {degree}\")\n",
        "                        print(f\"   📚 DOI: {doi_status}\")\n",
        "                    else:\n",
        "                        print(\"❌ Популярные термины не найдены\")\n",
        "                    continue\n",
        "\n",
        "                elif choice == '4':\n",
        "                    break\n",
        "\n",
        "                else:\n",
        "                    print(\"⚠️ Неверный выбор\")\n",
        "                    continue\n",
        "\n",
        "            else:\n",
        "                # Работа с выбранным термином\n",
        "                print(f\"\\n{'🔬' * 30}\")\n",
        "                print(f\"АНАЛИЗ ТЕРМИНА: {current_term.upper()}\")\n",
        "                print(f\"{'🔬' * 30}\")\n",
        "\n",
        "                print(\"Выберите действие:\")\n",
        "                print(\"1. 🔍 Анализ всех связей\")\n",
        "                print(\"2. 💡 Генерация гипотез\")\n",
        "                print(\"3. 🔄 Выбрать новый термин\")\n",
        "                print(\"4. ❌ Выход\")\n",
        "\n",
        "                action = input(\"\\n👆 Выберите действие (1-4): \").strip()\n",
        "\n",
        "                if action == '1':\n",
        "                    print(\"⏳ Анализ связей...\")\n",
        "                    spec_data = optimized_generator.analyze_specialized_connections(current_term)\n",
        "\n",
        "                    if spec_data:\n",
        "                        print(f\"✅ Найдено {len(spec_data['specialized_neighbors'])} соседей для анализа\")\n",
        "                        print(f\"📚 DOI доступны для {len(spec_data['doi_metadata'])} терминов\")\n",
        "\n",
        "                elif action == '2':\n",
        "                    print(\"⏳ Генерация гипотез...\")\n",
        "\n",
        "                    # Запрашиваем температуру\n",
        "                    temp_input = input(\"🌡️ Выберите температуру (0.1-1.0) [0.75]: \").strip()\n",
        "                    try:\n",
        "                        temperature = float(temp_input) if temp_input else 0.75\n",
        "                        temperature = max(0.1, min(1.0, temperature))\n",
        "                    except ValueError:\n",
        "                        temperature = 0.75\n",
        "\n",
        "                    spec_data = optimized_generator.analyze_specialized_connections(current_term)\n",
        "\n",
        "                    if spec_data:\n",
        "                        hypotheses = optimized_generator.generate_specialized_hypotheses(\n",
        "                            current_term, spec_data, 5, temperature\n",
        "                        )\n",
        "\n",
        "                        if hypotheses:\n",
        "                            optimized_generator.display_specialized_hypotheses(hypotheses)\n",
        "\n",
        "                elif action == '3':\n",
        "                    current_term = None\n",
        "                    continue\n",
        "\n",
        "                elif action == '4':\n",
        "                    break\n",
        "\n",
        "                else:\n",
        "                    print(\"⚠️ Неверный выбор\")\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\n⏹️ Прерывание (Ctrl+C)\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Ошибка: {e}\")\n",
        "            continue\n",
        "    \"\"\"\n",
        "    Диагностика структуры метаданных для отладки DOI\n",
        "    \"\"\"\n",
        "    print(\"🔍 ДИАГНОСТИКА СТРУКТУРЫ МЕТАДАННЫХ\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    if not node_full_meta:\n",
        "        print(\"❌ node_full_meta пуст\")\n",
        "        return\n",
        "\n",
        "    sample_keys = list(node_full_meta.keys())[:max_examples]\n",
        "\n",
        "    print(f\"📊 Всего терминов в метаданных: {len(node_full_meta)}\")\n",
        "    print(f\"🔍 Анализ первых {len(sample_keys)} примеров:\")\n",
        "\n",
        "    doi_counts = {'dois': 0, 'doi': 0, 'DOI': 0, 'other': 0}\n",
        "\n",
        "    for i, key in enumerate(sample_keys, 1):\n",
        "        meta = node_full_meta[key]\n",
        "        print(f\"\\n{i}. Термин: '{key}'\")\n",
        "        print(f\"   Тип метаданных: {type(meta)}\")\n",
        "\n",
        "        if isinstance(meta, dict):\n",
        "            print(f\"   Ключи: {list(meta.keys())}\")\n",
        "\n",
        "            # Проверяем DOI поля\n",
        "            for field in ['dois', 'doi', 'DOI']:\n",
        "                if field in meta:\n",
        "                    value = meta[field]\n",
        "                    print(f\"   {field}: {type(value)} = {value}\")\n",
        "                    if field == 'dois' and isinstance(value, list):\n",
        "                        valid_dois = [d for d in value if d and str(d).strip() and str(d).strip().lower() not in ['none', 'nan', 'null']]\n",
        "                        print(f\"   Valid DOIs in list: {len(valid_dois)}\")\n",
        "                        if valid_dois:\n",
        "                            doi_counts['dois'] += 1\n",
        "                    elif field in ['doi', 'DOI'] and value:\n",
        "                        doi_counts[field] += 1\n",
        "        else:\n",
        "            print(f\"   Содержимое: {str(meta)[:100]}...\")\n",
        "\n",
        "    print(f\"\\n📊 СТАТИСТИКА DOI:\")\n",
        "    print(f\"   Термины с 'dois' (список): {doi_counts['dois']}\")\n",
        "    print(f\"   Термины с 'doi': {doi_counts['doi']}\")\n",
        "    print(f\"   Термины с 'DOI': {doi_counts['DOI']}\")\n",
        "\n",
        "    # Общая статистика\n",
        "    total_with_dois = 0\n",
        "    all_doi_fields = ['dois', 'doi', 'DOI', 'Doi', 'pmid', 'PMID']\n",
        "\n",
        "    for term, meta in node_full_meta.items():\n",
        "        if isinstance(meta, dict):\n",
        "            has_doi = False\n",
        "            for field in all_doi_fields:\n",
        "                if field in meta:\n",
        "                    value = meta[field]\n",
        "                    if isinstance(value, list):\n",
        "                        if any(d and str(d).strip() and str(d).strip().lower() not in ['none', 'nan', 'null'] for d in value):\n",
        "                            has_doi = True\n",
        "                            break\n",
        "                    elif value and str(value).strip() and str(value).strip().lower() not in ['none', 'nan', 'null']:\n",
        "                        has_doi = True\n",
        "                        break\n",
        "            if has_doi:\n",
        "                total_with_dois += 1\n",
        "\n",
        "    print(f\"\\n🎯 ИТОГО: {total_with_dois} из {len(node_full_meta)} терминов имеют DOI\")\n",
        "    percentage = (total_with_dois / len(node_full_meta)) * 100 if node_full_meta else 0\n",
        "    print(f\"   Процент покрытия DOI: {percentage:.1f}%\")\n",
        "\n",
        "    if total_with_dois == 0:\n",
        "        print(f\"\\n⚠️ РЕКОМЕНДАЦИИ:\")\n",
        "        print(f\"   1. Проверьте, что DataFrame содержит колонку 'doi'\")\n",
        "        print(f\"   2. Убедитесь, что build_graph_with_metadata() корректно сохраняет DOI\")\n",
        "        print(f\"   3. Проверьте, что DOI не равны None/NaN/пустой строке\")\n",
        "\n",
        "# === ГОТОВЫЕ КОМАНДЫ ===\n",
        "print(\"\"\"\n",
        "🎯 ЗАЩИЩЕННАЯ СПЕЦИАЛИЗИРОВАННАЯ СИСТЕМА ГОТОВА!\n",
        "\n",
        "Основные команды:\n",
        "specialized_interactive_mode(G, node_full_meta, edge_full_meta)  # С вашими переменными\n",
        "diagnose_metadata_structure(node_full_meta)  # Диагностика DOI\n",
        "\n",
        "🔧 ИСПРАВЛЕНА СТРУКТУРА DOI:\n",
        "✅ Поддержка поля 'dois' как список (ваша структура)\n",
        "✅ Поддержка поля 'doi' как строка\n",
        "✅ Извлечение первого валидного DOI из списка\n",
        "✅ Обработка titles, authors, years как списков\n",
        "✅ Расширенная диагностика метаданных\n",
        "\n",
        "🛡️ ЗАЩИТА ОТ СЛУЧАЙНЫХ ГИПОТЕЗ:\n",
        "✅ Проверка качества данных перед генерацией\n",
        "✅ Блокировка при отсутствии специализированных соседей\n",
        "✅ Предупреждения при отсутствии DOI\n",
        "✅ Запрос подтверждения при низком качестве\n",
        "✅ Рекомендации по улучшению\n",
        "\n",
        "🔍 ДИАГНОСТИКА:\n",
        "Если DOI по-прежнему = 0, запустите:\n",
        "diagnose_metadata_structure(node_full_meta)\n",
        "\n",
        "ЗАПУСК:\n",
        "specialized_interactive_mode(G, node_full_meta, edge_full_meta)\n",
        "\"\"\")\n",
        "\n",
        "# Функция для быстрого запуска\n",
        "def quick_start():\n",
        "    \"\"\"Быстрый запуск с автопоиском графа\"\"\"\n",
        "    print(\"🚀 Попытка автоматического запуска...\")\n",
        "    specialized_interactive_mode()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xazX-S3xLDJi",
        "outputId": "007114be-88ae-4f05-ead7-b02b407fe0f1"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🎯 ЗАЩИЩЕННАЯ СПЕЦИАЛИЗИРОВАННАЯ СИСТЕМА ГОТОВА!\n",
            "\n",
            "Основные команды:\n",
            "specialized_interactive_mode(G, node_full_meta, edge_full_meta)  # С вашими переменными\n",
            "diagnose_metadata_structure(node_full_meta)  # Диагностика DOI\n",
            "\n",
            "🔧 ИСПРАВЛЕНА СТРУКТУРА DOI:\n",
            "✅ Поддержка поля 'dois' как список (ваша структура)\n",
            "✅ Поддержка поля 'doi' как строка  \n",
            "✅ Извлечение первого валидного DOI из списка\n",
            "✅ Обработка titles, authors, years как списков\n",
            "✅ Расширенная диагностика метаданных\n",
            "\n",
            "🛡️ ЗАЩИТА ОТ СЛУЧАЙНЫХ ГИПОТЕЗ:\n",
            "✅ Проверка качества данных перед генерацией\n",
            "✅ Блокировка при отсутствии специализированных соседей\n",
            "✅ Предупреждения при отсутствии DOI\n",
            "✅ Запрос подтверждения при низком качестве\n",
            "✅ Рекомендации по улучшению\n",
            "\n",
            "🔍 ДИАГНОСТИКА:\n",
            "Если DOI по-прежнему = 0, запустите:\n",
            "diagnose_metadata_structure(node_full_meta)\n",
            "\n",
            "ЗАПУСК:\n",
            "specialized_interactive_mode(G, node_full_meta, edge_full_meta)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "specialized_interactive_mode(G, node_full_meta, edge_full_meta)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "is6NRPaBLDSN",
        "outputId": "222d474e-4f3f-425a-fbed-b26c12e63fed"
      },
      "execution_count": 69,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 СПЕЦИАЛИЗИРОВАННЫЙ РЕЖИМ НА ОСНОВЕ ВАШИХ КЛЮЧЕВЫХ СЛОВ\n",
            "================================================================================\n",
            "Специализация:\n",
            "  🧬 Биомаркеры старения человека\n",
            "  🔬 Эпигенетические часы\n",
            "  ⚡ Воспаление при старении\n",
            "  🧪 Специфичные гены (ATAD3A, COL18A1, GATA4, etc.)\n",
            "  🏥 Терапевтические мишени\n",
            "  🧠 Иммунное старение\n",
            "  🔀 Пути долголетия\n",
            "  📊 Одноклеточный анализ\n",
            "================================================================================\n",
            "📊 Используем граф: 32,707 узлов, 775,715 рёбер\n",
            "🔍 Инициализация для графа: 32,707 узлов, 775,715 рёбер\n",
            "📋 Извлечено 68 уникальных ключевых слов\n",
            "🚀 Построение индексов...\n",
            "🔍 Поиск специализированных терминов по вашим ключевым словам...\n",
            "📚 Построение индекса DOI...\n",
            "🔍 Примеры метаданных для отладки:\n",
            "   muscle biopsies: <class 'dict'>\n",
            "      ключи: ['dois', 'years', 'titles', 'authors', 'keywords']\n",
            "      dois: ['https://doi.org/10.1186/s13072-025-00601-w', nan]\n",
            "   human myoblast: <class 'dict'>\n",
            "      ключи: ['dois', 'years', 'titles', 'authors', 'keywords']\n",
            "      dois: ['https://doi.org/10.1186/s13072-025-00601-w', 'https://doi.org/10.3390/ijms241713181']\n",
            "   tet1-2: <class 'dict'>\n",
            "      ключи: ['dois', 'years', 'titles', 'authors', 'keywords']\n",
            "      dois: ['https://doi.org/10.1186/s13072-025-00601-w']\n",
            "✅ Найдено 23286 узлов с DOI/PMID\n",
            "🔍 Примеры найденных DOI:\n",
            "   muscle biopsies: https://doi.org/10.1186/s13072-025-00601-w\n",
            "   human myoblast: https://doi.org/10.1186/s13072-025-00601-w\n",
            "   tet1-2: https://doi.org/10.1186/s13072-025-00601-w\n",
            "   human cancers [: https://doi.org/10.1186/s13100-024-00339-4\n",
            "   tet2: https://doi.org/10.1038/s42255-023-00855-2\n",
            "✅ Найдено 276 специализированных терминов\n",
            "🔧 Построение специализированных категорий с расширенными ключевыми словами...\n",
            "🎯 Минимальный порог специализированных соседей: 1\n",
            "\n",
            "🔍 ДИАГНОСТИКА С РАСШИРЕННЫМИ КЛЮЧЕВЫМИ СЛОВАМИ:\n",
            "    1. human hopx genome.blue arrowhead    ✅ (всего: 1, спец: 0)\n",
            "    2. human coa1                          ✅ (всего: 3, спец: 0)\n",
            "    3. human col18a1 promoter 2            ✅ (всего: 3, спец: 0)\n",
            "    4. human hopx proteins                 ✅ (всего: 3, спец: 0)\n",
            "    5. anti-human emilin1                  ✅ (всего: 6, спец: 2)\n",
            "    6. high hopx methylation               ✅ (всего: 1, спец: 0)\n",
            "    7. gdf15 human                         ✅ (всего: 6, спец: 0)\n",
            "    8. anti‐human hmgb1 human              ✅ (всего: 6, спец: 0)\n",
            "    9. human hopx protein                  ✅ (всего: 5, спец: 0)\n",
            "   10. human emilin1                       ✅ (всего: 7, спец: 2)\n",
            "   11. human hopx                          ✅ (всего: 7, спец: 0)\n",
            "   12. human pro-longevity                 ✅ (всего: 1, спец: 0)\n",
            "   13. human aging.skin                    ✅ (всего: 1, спец: 0)\n",
            "   14. human col18a1                       ✅ (всего: 8, спец: 0)\n",
            "   15. human longevity.23                  ✅ (всего: 2, спец: 0)\n",
            "   16. human longevity-specific            ✅ (всего: 2, спец: 0)\n",
            "   17. human senescence-associated         ✅ (всего: 1, спец: 0)\n",
            "   18. human senescence-associated β-galactosidase ✅ (всего: 1, спец: 0)\n",
            "   19. human cellular senescence-associated ✅ (всего: 1, спец: 0)\n",
            "   20. coa1/c7orf44/mitrac15               ❌ (всего: 1, спец: 0)\n",
            "\n",
            "📊 СТАТИСТИКА С РАСШИРЕННЫМИ КРИТЕРИЯМИ:\n",
            "   Терминов соответствует расширенным критериям: 178\n",
            "\n",
            "🚫 Отфильтровано 151 из 178 терминов\n",
            "✅ В категории добавлено 26 качественных терминов\n",
            "\n",
            "📊 СТАТИСТИКА ПО КАТЕГОРИЯМ (с расширенными ключевыми словами):\n",
            "   🧬 Биомаркеры старения человека: 15 терминов (средняя степень: 5.0)\n",
            "   🔬 Эпигенетические часы: 0 терминов ⚠️\n",
            "   ⚡ Воспаление при старении: 0 терминов ⚠️\n",
            "   🧪 Гены и белки старения: 1 терминов (средняя степень: 8.0)\n",
            "   🏥 Терапевтические мишени: 0 терминов ⚠️\n",
            "   🧠 Иммунное старение: 0 терминов ⚠️\n",
            "   🔀 Пути долголетия: 0 терминов ⚠️\n",
            "   📊 Клеточные процессы: 10 терминов (средняя степень: 2.3)\n",
            "\n",
            "🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯\n",
            "ВЫБОР СПЕЦИАЛИЗИРОВАННОГО ТЕРМИНА\n",
            "🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯\n",
            "\n",
            "🎯 СПЕЦИАЛИЗИРОВАННЫЕ КАТЕГОРИИ ПО ВАШИМ КЛЮЧЕВЫМ СЛОВАМ\n",
            "================================================================================\n",
            "\n",
            "1. 🧬 Биомаркеры старения человека (15 терминов)\n",
            "   Топ-3 по специализированному скору:\n",
            "   1. anti-human emilin1 (связей: 6, DOI: 📚, скор: 49.0)\n",
            "   2. human emilin1 (связей: 7, DOI: 📚, скор: 47.3)\n",
            "   3. extracellular protein emilin1 (связей: 2, DOI: 📚, скор: 43.0)\n",
            "\n",
            "2. 🔬 Эпигенетические часы (0 терминов)\n",
            "   Термины не найдены\n",
            "\n",
            "3. ⚡ Воспаление при старении (0 терминов)\n",
            "   Термины не найдены\n",
            "\n",
            "4. 🧪 Гены и белки старения (1 терминов)\n",
            "   Топ-3 по специализированному скору:\n",
            "   1. ecm glycoprotein emilin1 (связей: 8, DOI: 📚, скор: 37.0)\n",
            "\n",
            "5. 🏥 Терапевтические мишени (0 терминов)\n",
            "   Термины не найдены\n",
            "\n",
            "6. 🧠 Иммунное старение (0 терминов)\n",
            "   Термины не найдены\n",
            "\n",
            "7. 🔀 Пути долголетия (0 терминов)\n",
            "   Термины не найдены\n",
            "\n",
            "8. 📊 Клеточные процессы (10 терминов)\n",
            "   Топ-3 по специализированному скору:\n",
            "   1. phospho-vimentin+/hopx+ cells (связей: 1, DOI: 📚, скор: 44.0)\n",
            "   2. coa1δ cells (связей: 3, DOI: 📚, скор: 42.0)\n",
            "   3. hopx-derived alveolar cells (связей: 3, DOI: 📚, скор: 42.0)\n",
            "\n",
            "0. ✍️  Ввести термин вручную\n",
            "99. 🎲 Случайный специализированный термин\n",
            "\n",
            "👆 Выберите категорию (1-8), 0 для ручного ввода, 99 для случайного: 99\n",
            "🎲 Выбран случайный качественный термин: anti-human emilin1\n",
            "✅ КАЧЕСТВЕННЫЕ ПОКАЗАТЕЛИ:\n",
            "   🎯 Специализированных соседей: 2 (≥3 ✅)\n",
            "   📚 DOI: 📚\n",
            "   📈 Специализированный скор: 49.0\n",
            "\n",
            "📋 ДЕТАЛЬНАЯ ИНФОРМАЦИЯ: anti-human emilin1\n",
            "============================================================\n",
            "🔗 Степень узла: 6\n",
            "🎯 Специализированный скор: 49.0\n",
            "👥 Соседей: 6\n",
            "📚 DOI: https://doi.org/10.1042/CS20160064\n",
            "🔑 Совпадения с вашими ключевыми словами: emilin1\n",
            "📄 Заголовок: Local inhibition of elastase reduces EMILIN1 cleavage reactivating lymphatic vessel function in a mo...\n",
            "👨‍🔬 Авторы: Eliana Pivetta, Bruna Wassermann, Lisa Del Bel Belluz, Carla Danussi, Teresa Maria Elisa Modica, Orl...\n",
            "📅 Год: 2016\n",
            "📊 Диапазон лет: 2016-2016\n",
            "📚 Всего статей: 1\n",
            "🏷️  Ключевые слова источника: EMILIN1...\n",
            "📝 Описание: Biomedical entity from: Local inhibition of elastase reduces EMILIN1 cleavage reactivating lymphatic vessel function in a mouse lymphoedema model...\n",
            "\n",
            "🔗 Топ-5 специализированных соседей:\n",
            "   1. human emilin1 (скор: 47.3, связей: 7, DOI: 📚)\n",
            "   2. mouse tissue lysates (скор: 24.0, связей: 6, DOI: 📚)\n",
            "   3. emilin1 (скор: 17.1, связей: 649, DOI: 📚)\n",
            "   4. rabbit anti-mouse (скор: 9.9, связей: 19, DOI: 📚)\n",
            "   5. membranes (скор: 3.1, связей: 143, DOI: 📚)\n",
            "\n",
            "🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬\n",
            "СПЕЦИАЛИЗИРОВАННЫЙ АНАЛИЗ: ANTI-HUMAN EMILIN1\n",
            "🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬\n",
            "Выберите действие:\n",
            "1. 🔍 Анализ специализированных связей\n",
            "2. 💡 Генерация клинических гипотез\n",
            "3. 📚 Показать метаданные и DOI\n",
            "4. 🔄 Выбрать новый термин\n",
            "5. ❌ Выход\n",
            "\n",
            "👆 Выберите действие (1-5): 2\n",
            "⏳ Генерация конкретных клинических гипотез...\n",
            "🌡️ Выберите температуру (0.1-1.0) [по умолчанию 0.75]: 0.5\n",
            "🌡️ Используем температуру: 0.5\n",
            "💡 Совет: 0.1-0.3=консервативно, 0.4-0.7=сбалансированно, 0.8-1.0=креативно\n",
            "\n",
            "🔍 АНАЛИЗ СПЕЦИАЛИЗИРОВАННЫХ СВЯЗЕЙ: anti-human emilin1\n",
            "------------------------------------------------------------\n",
            "🔍 Инициализация для графа: 32,707 узлов, 775,715 рёбер\n",
            "📋 Извлечено 68 уникальных ключевых слов\n",
            "🚀 Построение индексов...\n",
            "🔍 Поиск специализированных терминов по вашим ключевым словам...\n",
            "📚 Построение индекса DOI...\n",
            "🔍 Примеры метаданных для отладки:\n",
            "   muscle biopsies: <class 'dict'>\n",
            "      ключи: ['dois', 'years', 'titles', 'authors', 'keywords']\n",
            "      dois: ['https://doi.org/10.1186/s13072-025-00601-w', nan]\n",
            "   human myoblast: <class 'dict'>\n",
            "      ключи: ['dois', 'years', 'titles', 'authors', 'keywords']\n",
            "      dois: ['https://doi.org/10.1186/s13072-025-00601-w', 'https://doi.org/10.3390/ijms241713181']\n",
            "   tet1-2: <class 'dict'>\n",
            "      ключи: ['dois', 'years', 'titles', 'authors', 'keywords']\n",
            "      dois: ['https://doi.org/10.1186/s13072-025-00601-w']\n",
            "✅ Найдено 23286 узлов с DOI/PMID\n",
            "🔍 Примеры найденных DOI:\n",
            "   muscle biopsies: https://doi.org/10.1186/s13072-025-00601-w\n",
            "   human myoblast: https://doi.org/10.1186/s13072-025-00601-w\n",
            "   tet1-2: https://doi.org/10.1186/s13072-025-00601-w\n",
            "   human cancers [: https://doi.org/10.1186/s13100-024-00339-4\n",
            "   tet2: https://doi.org/10.1038/s42255-023-00855-2\n",
            "✅ Найдено 276 специализированных терминов\n",
            "🔧 Построение специализированных категорий с расширенными ключевыми словами...\n",
            "🎯 Минимальный порог специализированных соседей: 1\n",
            "\n",
            "🔍 ДИАГНОСТИКА С РАСШИРЕННЫМИ КЛЮЧЕВЫМИ СЛОВАМИ:\n",
            "    1. human hopx genome.blue arrowhead    ✅ (всего: 1, спец: 0)\n",
            "    2. human coa1                          ✅ (всего: 3, спец: 0)\n",
            "    3. human col18a1 promoter 2            ✅ (всего: 3, спец: 0)\n",
            "    4. human hopx proteins                 ✅ (всего: 3, спец: 0)\n",
            "    5. anti-human emilin1                  ✅ (всего: 6, спец: 2)\n",
            "    6. high hopx methylation               ✅ (всего: 1, спец: 0)\n",
            "    7. gdf15 human                         ✅ (всего: 6, спец: 0)\n",
            "    8. anti‐human hmgb1 human              ✅ (всего: 6, спец: 0)\n",
            "    9. human hopx protein                  ✅ (всего: 5, спец: 0)\n",
            "   10. human emilin1                       ✅ (всего: 7, спец: 2)\n",
            "   11. human hopx                          ✅ (всего: 7, спец: 0)\n",
            "   12. human pro-longevity                 ✅ (всего: 1, спец: 0)\n",
            "   13. human aging.skin                    ✅ (всего: 1, спец: 0)\n",
            "   14. human col18a1                       ✅ (всего: 8, спец: 0)\n",
            "   15. human longevity.23                  ✅ (всего: 2, спец: 0)\n",
            "   16. human longevity-specific            ✅ (всего: 2, спец: 0)\n",
            "   17. human senescence-associated         ✅ (всего: 1, спец: 0)\n",
            "   18. human senescence-associated β-galactosidase ✅ (всего: 1, спец: 0)\n",
            "   19. human cellular senescence-associated ✅ (всего: 1, спец: 0)\n",
            "   20. coa1/c7orf44/mitrac15               ❌ (всего: 1, спец: 0)\n",
            "\n",
            "📊 СТАТИСТИКА С РАСШИРЕННЫМИ КРИТЕРИЯМИ:\n",
            "   Терминов соответствует расширенным критериям: 178\n",
            "\n",
            "🚫 Отфильтровано 151 из 178 терминов\n",
            "✅ В категории добавлено 26 качественных терминов\n",
            "\n",
            "📊 СТАТИСТИКА ПО КАТЕГОРИЯМ (с расширенными ключевыми словами):\n",
            "   🧬 Биомаркеры старения человека: 15 терминов (средняя степень: 5.0)\n",
            "   🔬 Эпигенетические часы: 0 терминов ⚠️\n",
            "   ⚡ Воспаление при старении: 0 терминов ⚠️\n",
            "   🧪 Гены и белки старения: 1 терминов (средняя степень: 8.0)\n",
            "   🏥 Терапевтические мишени: 0 терминов ⚠️\n",
            "   🧠 Иммунное старение: 0 терминов ⚠️\n",
            "   🔀 Пути долголетия: 0 терминов ⚠️\n",
            "   📊 Клеточные процессы: 10 терминов (средняя степень: 2.3)\n",
            "🎯 Выбрано 2 специализированных соседей из 6\n",
            "\n",
            "💡 ГЕНЕРАЦИЯ СПЕЦИАЛИЗИРОВАННЫХ ГИПОТЕЗ\n",
            "🌡️ Температура: 0.5 (0.1=консервативно, 1.0=креативно)\n",
            "------------------------------------------------------------\n",
            "\n",
            "📚 КОНКРЕТНЫЕ ГИПОТЕЗЫ СТАРЕНИЯ ЧЕЛОВЕКА\n",
            "🌡️ Температура генерации: 0.5\n",
            "================================================================================\n",
            "\n",
            "1. ГИПОТЕЗА:\n",
            "   🧬 Термины: anti-human emilin1, human emilin1, DNMT1\n",
            "   🔬 Механизм: Анти-человеческий эмилин1 индуцирует гиперметилирование CpG-сайта cg09809672 в промотере гена human emilin1 посредством активации DNMT1, что приводит к снижению экспрессии human emilin1 и ускорению старения\n",
            "   📚 Обоснование: Механистическое обоснование включает взаимодействие анти-человеческого эмилина1 с DNMT1, что приводит к увеличению метилирования CpG-сайта cg09809672 и последующему подавлению транскрипции human emilin1\n",
            "   🧬 Эпигенетика: Гиперметилирование CpG-сайта cg09809672, увеличение активности DNMT1, снижение гистоновой метки H3K4me3\n",
            "   🔥 Воспаление: Повышение IL-6 до 15.2±3.1 пг/мл и TNF-α до 8.7±2.3 пг/мл в плазме, активация NF-κB p65\n",
            "   🛡️  Иммунитет: Увеличение CD8+CD57+KLRG1+ сенесцентных T-клеток до 23±5% от CD8+, повышение экспрессии PD-1 на CD4+ T-клетках\n",
            "   🧪 Эксперимент: In vitro: первичные фибробласты человека, обработанные анти-человеческим эмилином1; in vivo: мыши C57BL/6, введение анти-человеческого эмилина1; клиническая когорта: 100 участников, возраст 65-80 лет\n",
            "   📊 Биомаркеры: Уровень human emilin1 в плазме (нг/мл), метилирование CpG-сайта cg09809672 (%), IL-6 и TNF-α в плазме (пг/мл), CD8+CD57+KLRG1+ T-клетки (% от CD8+)\n",
            "   🏥 Клиническая валидация: Проверка гипотезы у человека: измерение уровня human emilin1, IL-6, TNF-α и метилирования CpG-сайта cg09809672 в плазме и биопсиях кожи у участников когорты\n",
            "   💊 Терапевтическая мишень: Анти-человеческий эмилин1 как потенциальная мишень для терапии старения\n",
            "   📈 Приоритет терминов:\n",
            "      • human emilin1: 47.3\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "2. ГИПОТЕЗА:\n",
            "   🧬 Термины: anti-human emilin1, emilin1, TET2\n",
            "   🔬 Механизм: Анти-человеческий эмилин1 индуцирует деметилирование CpG-сайта cg05575921 в промотере гена emilin1 посредством активации TET2, что приводит к увеличению экспрессии emilin1 и замедлению старения\n",
            "   📚 Обоснование: Механистическое обоснование включает взаимодействие анти-человеческого эмилина1 с TET2, что приводит к увеличению деметилирования CpG-сайта cg05575921 и последующему повышению транскрипции emilin1\n",
            "   🧬 Эпигенетика: Деметилирование CpG-сайта cg05575921, увеличение активности TET2, повышение гистоновой метки H3K4me3\n",
            "   🔥 Воспаление: Снижение IL-1β до 2.1±0.5 пг/мл и MCP-1 до 1.3±0.2 пг/мл в плазме, ингибирование NF-κB p65\n",
            "   🛡️  Иммунитет: Снижение CD4+CD28- сенесцентных T-клеток до 10±2% от CD4+, снижение экспрессии LAG-3 на CD8+ T-клетках\n",
            "   🧪 Эксперимент: In vitro: HUVEC, обработанные анти-человеческим эмилином1; in vivo: крысы Wistar, введение анти-человеческого эмилина1; клиническая когорта: 50 участников, возраст 60-75 лет\n",
            "   📊 Биомаркеры: Уровень emilin1 в плазме (нг/мл), деметилирование CpG-сайта cg05575921 (%), IL-1β и MCP-1 в плазме (пг/мл), CD4+CD28- T-клетки (% от CD4+)\n",
            "   🏥 Клиническая валидация: Проверка гипотезы у человека: измерение уровня emilin1, IL-1β, MCP-1 и деметилирования CpG-сайта cg05575921 в плазме и биопсиях кожи у участников когорты\n",
            "   💊 Терапевтическая мишень: Анти-человеческий эмилин1 как потенциальная мишень для терапии старения\n",
            "   📈 Приоритет терминов:\n",
            "      • emilin1: 17.1\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "3. ГИПОТЕЗА:\n",
            "   🧬 Термины: anti-human emilin1, human emilin1, H3K27me3\n",
            "   🔬 Механизм: Анти-человеческий эмилин1 индуцирует триметилирование гистона H3K27 в промотере гена human emilin1, что приводит к снижению экспрессии human emilin1 и ускорению старения\n",
            "   📚 Обоснование: Механистическое обоснование включает взаимодействие анти-человеческого эмилина1 с гистоном H3K27, что приводит к увеличению триметилирования и последующему подавлению транскрипции human emilin1\n",
            "   🧬 Эпигенетика: Триметилирование гистона H3K27, снижение гистоновой метки H3K4me3\n",
            "   🔥 Воспаление: Повышение IL-8 до 12.5±2.8 пг/мл и TNF-α до 10.2±2.5 пг/мл в плазме, активация IRF3\n",
            "   🛡️  Иммунитет: Увеличение CD8+CD57+KLRG1+ сенесцентных T-клеток до 25±5% от CD8+, повышение экспрессии PD-1 на CD4+ T-клетках\n",
            "   🧪 Эксперимент: In vitro: первичные фибробласты человека, обработанные анти-человеческим эмилином1; in vivo: мыши C57BL/6, введение анти-человеческого эмилина1; клиническая когорта: 100 участников, возраст 65-80 лет\n",
            "   📊 Биомаркеры: Уровень human emilin1 в плазме (нг/мл), триметилирование гистона H3K27, IL-8 и TNF-α в плазме (пг/мл), CD8+CD57+KLRG1+ T-клетки (% от CD8+)\n",
            "   🏥 Клиническая валидация: Проверка гипотезы у человека: измерение уровня human emilin1, IL-8, TNF-α и триметилирования гистона H3K27 в плазме и биопсиях кожи у участников когорты\n",
            "   💊 Терапевтическая мишень: Анти-человеческий эмилин1 как потенциальная мишень для терапии старения\n",
            "   📈 Приоритет терминов:\n",
            "      • human emilin1: 47.3\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "4. ГИПОТЕЗА:\n",
            "   🧬 Термины: anti-human emilin1, emilin1, STAT3\n",
            "   🔬 Механизм: Анти-человеческий эмилин1 индуцирует фосфорилирование STAT3, что приводит к увеличению экспрессии emilin1 и замедлению старения\n",
            "   📚 Обоснование: Механистическое обоснование включает взаимодействие анти-человеческого эмилина1 с STAT3, что приводит к увеличению фосфорилирования и последующему повышению транскрипции emilin1\n",
            "   🧬 Эпигенетика: Деметилирование CpG-сайта cg05575921, увеличение активности TET2, повышение гистоновой метки H3K4me3\n",
            "   🔥 Воспаление: Снижение IL-1β до 2.1±0.5 пг/мл и MCP-1 до 1.3±0.2 пг/мл в плазме, ингибирование NF-κB p65\n",
            "   🛡️  Иммунитет: Снижение CD4+CD28- сенесцентных T-клеток до 10±2% от CD4+, снижение экспрессии LAG-3 на CD8+ T-клетках\n",
            "   🧪 Эксперимент: In vitro: HUVEC, обработанные анти-человеческим эмилином1; in vivo: крысы Wistar, введение анти-человеческого эмилина1; клиническая когорта: 50 участников, возраст 60-75 лет\n",
            "   📊 Биомаркеры: Уровень emilin1 в плазме (нг/мл), фосфорилирование STAT3, IL-1β и MCP-1 в плазме (пг/мл), CD4+CD28- T-клетки (% от CD4+)\n",
            "   🏥 Клиническая валидация: Проверка гипотезы у человека: измерение уровня emilin1, IL-1β, MCP-1 и фосфорилирования STAT3 в плазме и биопсиях кожи у участников когорты\n",
            "   💊 Терапевтическая мишень: Анти-человеческий эмилин1 как потенциальная мишень для терапии старения\n",
            "   📈 Приоритет терминов:\n",
            "      • emilin1: 17.1\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "5. ГИПОТЕЗА:\n",
            "   🧬 Термины: anti-human emilin1, human emilin1, NLRP3\n",
            "   🔬 Механизм: Анти-человеческий эмилин1 индуцирует активацию NLRP3, что приводит к увеличению производства IL-1β и ускорению старения\n",
            "   📚 Обоснование: Механистическое обоснование включает взаимодействие анти-человеческого эмилина1 с NLRP3, что приводит к увеличению активации и последующему повышению производства IL-1β\n",
            "   🧬 Эпигенетика: Гиперметилирование CpG-сайта cg09809672, увеличение активности DNMT1, снижение гистоновой метки H3K4me3\n",
            "   🔥 Воспаление: Повышение IL-1β до 15.2±3.1 пг/мл и TNF-α до 8.7±2.3 пг/мл в плазме, активация NF-κB p65\n",
            "   🛡️  Иммунитет: Увеличение CD8+CD57+KLRG1+ сенесцентных T-клеток до 23±5% от CD8+, повышение экспрессии PD-1 на CD4+ T-клетках\n",
            "   🧪 Эксперимент: In vitro: первичные фибробласты человека, обработанные анти-человеческим эмилином1; in vivo: мыши C57BL/6, введение анти-человеческого эмилина1; клиническая когорта: 100 участников, возраст 65-80 лет\n",
            "   📊 Биомаркеры: Уровень human emilin1 в плазме (нг/мл), активация NLRP3, IL-1β и TNF-α в плазме (пг/мл), CD8+CD57+KLRG1+ T-клетки (% от CD8+)\n",
            "   🏥 Клиническая валидация: Проверка гипотезы у человека: измерение уровня human emilin1, IL-1β, TNF-α и активации NLRP3 в плазме и биопсиях кожи у участников когорты\n",
            "   💊 Терапевтическая мишень: Анти-человеческий эмилин1 как потенциальная мишень для терапии старения\n",
            "   📈 Приоритет терминов:\n",
            "      • human emilin1: 47.3\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬\n",
            "СПЕЦИАЛИЗИРОВАННЫЙ АНАЛИЗ: ANTI-HUMAN EMILIN1\n",
            "🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬\n",
            "Выберите действие:\n",
            "1. 🔍 Анализ специализированных связей\n",
            "2. 💡 Генерация клинических гипотез\n",
            "3. 📚 Показать метаданные и DOI\n",
            "4. 🔄 Выбрать новый термин\n",
            "5. ❌ Выход\n",
            "\n",
            "👆 Выберите действие (1-5): 4\n",
            "\n",
            "🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯\n",
            "ВЫБОР СПЕЦИАЛИЗИРОВАННОГО ТЕРМИНА\n",
            "🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯\n",
            "\n",
            "🎯 СПЕЦИАЛИЗИРОВАННЫЕ КАТЕГОРИИ ПО ВАШИМ КЛЮЧЕВЫМ СЛОВАМ\n",
            "================================================================================\n",
            "\n",
            "1. 🧬 Биомаркеры старения человека (15 терминов)\n",
            "   Топ-3 по специализированному скору:\n",
            "   1. anti-human emilin1 (связей: 6, DOI: 📚, скор: 49.0)\n",
            "   2. human emilin1 (связей: 7, DOI: 📚, скор: 47.3)\n",
            "   3. extracellular protein emilin1 (связей: 2, DOI: 📚, скор: 43.0)\n",
            "\n",
            "2. 🔬 Эпигенетические часы (0 терминов)\n",
            "   Термины не найдены\n",
            "\n",
            "3. ⚡ Воспаление при старении (0 терминов)\n",
            "   Термины не найдены\n",
            "\n",
            "4. 🧪 Гены и белки старения (1 терминов)\n",
            "   Топ-3 по специализированному скору:\n",
            "   1. ecm glycoprotein emilin1 (связей: 8, DOI: 📚, скор: 37.0)\n",
            "\n",
            "5. 🏥 Терапевтические мишени (0 терминов)\n",
            "   Термины не найдены\n",
            "\n",
            "6. 🧠 Иммунное старение (0 терминов)\n",
            "   Термины не найдены\n",
            "\n",
            "7. 🔀 Пути долголетия (0 терминов)\n",
            "   Термины не найдены\n",
            "\n",
            "8. 📊 Клеточные процессы (10 терминов)\n",
            "   Топ-3 по специализированному скору:\n",
            "   1. phospho-vimentin+/hopx+ cells (связей: 1, DOI: 📚, скор: 44.0)\n",
            "   2. coa1δ cells (связей: 3, DOI: 📚, скор: 42.0)\n",
            "   3. hopx-derived alveolar cells (связей: 3, DOI: 📚, скор: 42.0)\n",
            "\n",
            "0. ✍️  Ввести термин вручную\n",
            "99. 🎲 Случайный специализированный термин\n",
            "\n",
            "👆 Выберите категорию (1-8), 0 для ручного ввода, 99 для случайного: 99\n",
            "🎲 Выбран случайный качественный термин: hopx−/prosp-c+ cells\n",
            "✅ КАЧЕСТВЕННЫЕ ПОКАЗАТЕЛИ:\n",
            "   🎯 Специализированных соседей: 2 (≥3 ✅)\n",
            "   📚 DOI: 📚\n",
            "   📈 Специализированный скор: 37.3\n",
            "\n",
            "📋 ДЕТАЛЬНАЯ ИНФОРМАЦИЯ: hopx−/prosp-c+ cells\n",
            "============================================================\n",
            "🔗 Степень узла: 7\n",
            "🎯 Специализированный скор: 37.3\n",
            "👥 Соседей: 7\n",
            "📚 DOI: https://doi.org/10.1038/s41598-018-31214-x\n",
            "🔑 Совпадения с вашими ключевыми словами: hopx\n",
            "📄 Заголовок: Dynamic expression of HOPX in alveolar epithelial cells reflects injury and repair during the progre...\n",
            "👨‍🔬 Авторы: Chiharu Ota, John-Poul Ng-Blichfeldt, Martina Korfei, Hani N. Alsafadi, Mareike Lehmann, Wioletta Sk...\n",
            "📅 Год: 2018\n",
            "📊 Диапазон лет: 2018-2018\n",
            "📚 Всего статей: 1\n",
            "🏷️  Ключевые слова источника: HOPX...\n",
            "📝 Описание: Biomedical entity from: Dynamic expression of HOPX in alveolar epithelial cells reflects injury and repair during the progression of pulmonary fibrosis...\n",
            "\n",
            "🔗 Топ-5 специализированных соседей:\n",
            "   1. hopx+/prosp-c− cells (скор: 39.8, связей: 4, DOI: 📚)\n",
            "   2. hopx+/prosp-c+ cells (скор: 39.0, связей: 6, DOI: 📚)\n",
            "   3. blm-pmatii cells (скор: 26.0, связей: 4, DOI: 📚)\n",
            "   4. prosp-c (скор: 22.3, связей: 7, DOI: 📚)\n",
            "   5. fcm (скор: 16.8, связей: 12, DOI: 📚)\n",
            "\n",
            "🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬\n",
            "СПЕЦИАЛИЗИРОВАННЫЙ АНАЛИЗ: HOPX−/PROSP-C+ CELLS\n",
            "🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬\n",
            "Выберите действие:\n",
            "1. 🔍 Анализ специализированных связей\n",
            "2. 💡 Генерация клинических гипотез\n",
            "3. 📚 Показать метаданные и DOI\n",
            "4. 🔄 Выбрать новый термин\n",
            "5. ❌ Выход\n",
            "\n",
            "👆 Выберите действие (1-5): 2\n",
            "⏳ Генерация конкретных клинических гипотез...\n",
            "🌡️ Выберите температуру (0.1-1.0) [по умолчанию 0.75]: 0.3\n",
            "🌡️ Используем температуру: 0.3\n",
            "💡 Совет: 0.1-0.3=консервативно, 0.4-0.7=сбалансированно, 0.8-1.0=креативно\n",
            "\n",
            "🔍 АНАЛИЗ СПЕЦИАЛИЗИРОВАННЫХ СВЯЗЕЙ: hopx−/prosp-c+ cells\n",
            "------------------------------------------------------------\n",
            "🔍 Инициализация для графа: 32,707 узлов, 775,715 рёбер\n",
            "📋 Извлечено 68 уникальных ключевых слов\n",
            "🚀 Построение индексов...\n",
            "🔍 Поиск специализированных терминов по вашим ключевым словам...\n",
            "📚 Построение индекса DOI...\n",
            "🔍 Примеры метаданных для отладки:\n",
            "   muscle biopsies: <class 'dict'>\n",
            "      ключи: ['dois', 'years', 'titles', 'authors', 'keywords']\n",
            "      dois: ['https://doi.org/10.1186/s13072-025-00601-w', nan]\n",
            "   human myoblast: <class 'dict'>\n",
            "      ключи: ['dois', 'years', 'titles', 'authors', 'keywords']\n",
            "      dois: ['https://doi.org/10.1186/s13072-025-00601-w', 'https://doi.org/10.3390/ijms241713181']\n",
            "   tet1-2: <class 'dict'>\n",
            "      ключи: ['dois', 'years', 'titles', 'authors', 'keywords']\n",
            "      dois: ['https://doi.org/10.1186/s13072-025-00601-w']\n",
            "✅ Найдено 23286 узлов с DOI/PMID\n",
            "🔍 Примеры найденных DOI:\n",
            "   muscle biopsies: https://doi.org/10.1186/s13072-025-00601-w\n",
            "   human myoblast: https://doi.org/10.1186/s13072-025-00601-w\n",
            "   tet1-2: https://doi.org/10.1186/s13072-025-00601-w\n",
            "   human cancers [: https://doi.org/10.1186/s13100-024-00339-4\n",
            "   tet2: https://doi.org/10.1038/s42255-023-00855-2\n",
            "✅ Найдено 276 специализированных терминов\n",
            "🔧 Построение специализированных категорий с расширенными ключевыми словами...\n",
            "🎯 Минимальный порог специализированных соседей: 1\n",
            "\n",
            "🔍 ДИАГНОСТИКА С РАСШИРЕННЫМИ КЛЮЧЕВЫМИ СЛОВАМИ:\n",
            "    1. human hopx genome.blue arrowhead    ✅ (всего: 1, спец: 0)\n",
            "    2. human coa1                          ✅ (всего: 3, спец: 0)\n",
            "    3. human col18a1 promoter 2            ✅ (всего: 3, спец: 0)\n",
            "    4. human hopx proteins                 ✅ (всего: 3, спец: 0)\n",
            "    5. anti-human emilin1                  ✅ (всего: 6, спец: 2)\n",
            "    6. high hopx methylation               ✅ (всего: 1, спец: 0)\n",
            "    7. gdf15 human                         ✅ (всего: 6, спец: 0)\n",
            "    8. anti‐human hmgb1 human              ✅ (всего: 6, спец: 0)\n",
            "    9. human hopx protein                  ✅ (всего: 5, спец: 0)\n",
            "   10. human emilin1                       ✅ (всего: 7, спец: 2)\n",
            "   11. human hopx                          ✅ (всего: 7, спец: 0)\n",
            "   12. human pro-longevity                 ✅ (всего: 1, спец: 0)\n",
            "   13. human aging.skin                    ✅ (всего: 1, спец: 0)\n",
            "   14. human col18a1                       ✅ (всего: 8, спец: 0)\n",
            "   15. human longevity.23                  ✅ (всего: 2, спец: 0)\n",
            "   16. human longevity-specific            ✅ (всего: 2, спец: 0)\n",
            "   17. human senescence-associated         ✅ (всего: 1, спец: 0)\n",
            "   18. human senescence-associated β-galactosidase ✅ (всего: 1, спец: 0)\n",
            "   19. human cellular senescence-associated ✅ (всего: 1, спец: 0)\n",
            "   20. coa1/c7orf44/mitrac15               ❌ (всего: 1, спец: 0)\n",
            "\n",
            "📊 СТАТИСТИКА С РАСШИРЕННЫМИ КРИТЕРИЯМИ:\n",
            "   Терминов соответствует расширенным критериям: 178\n",
            "\n",
            "🚫 Отфильтровано 151 из 178 терминов\n",
            "✅ В категории добавлено 26 качественных терминов\n",
            "\n",
            "📊 СТАТИСТИКА ПО КАТЕГОРИЯМ (с расширенными ключевыми словами):\n",
            "   🧬 Биомаркеры старения человека: 15 терминов (средняя степень: 5.0)\n",
            "   🔬 Эпигенетические часы: 0 терминов ⚠️\n",
            "   ⚡ Воспаление при старении: 0 терминов ⚠️\n",
            "   🧪 Гены и белки старения: 1 терминов (средняя степень: 8.0)\n",
            "   🏥 Терапевтические мишени: 0 терминов ⚠️\n",
            "   🧠 Иммунное старение: 0 терминов ⚠️\n",
            "   🔀 Пути долголетия: 0 терминов ⚠️\n",
            "   📊 Клеточные процессы: 10 терминов (средняя степень: 2.3)\n",
            "🎯 Выбрано 2 специализированных соседей из 7\n",
            "\n",
            "💡 ГЕНЕРАЦИЯ СПЕЦИАЛИЗИРОВАННЫХ ГИПОТЕЗ\n",
            "🌡️ Температура: 0.3 (0.1=консервативно, 1.0=креативно)\n",
            "------------------------------------------------------------\n",
            "\n",
            "📚 КОНКРЕТНЫЕ ГИПОТЕЗЫ СТАРЕНИЯ ЧЕЛОВЕКА\n",
            "🌡️ Температура генерации: 0.3\n",
            "================================================================================\n",
            "\n",
            "1. ГИПОТЕЗА:\n",
            "   🧬 Термины: hopx−/prosp-c+ cells, hopx+/prosp-c− cells, DNMT1\n",
            "   🔬 Механизм: Гиперметилирование CpG-сайта cg09809672 в промотере гена CDKN2A в hopx−/prosp-c+ клетках приводит к увеличению экспрессии DNMT1 и снижению экспрессии TET2, что способствует старению\n",
            "   📚 Обоснование: Механистическое обоснование с конкретными белками/генами: DNMT1 и TET2 играют ключевую роль в регуляции метилирования ДНК и деметилирования, что влияет на экспрессию генов, участвующих в старении\n",
            "   🧬 Эпигенетика: Гиперметилирование CpG-сайта cg09809672, увеличение гистоновой метки H3K27me3, снижение гистоновой метки H3K4me3, повышение активности DNMT1 и снижение активности TET2\n",
            "   🔥 Воспаление: Повышение IL-6 до 15.2±3.1 пг/мл и TNF-α до 8.7±2.3 пг/мл в плазме, активация сигнального пути NF-κB p65 и IRF3\n",
            "   🛡️  Иммунитет: Увеличение CD8+CD57+KLRG1+ сенесцентных T-клеток до 23±5% от CD8+, снижение экспрессии CD28 на CD4+ T-клетках\n",
            "   🧪 Эксперимент: In vitro модель: первичные фибробласты человека, методы: qPCR, Western blot, ELISA, временные точки: 24, 48, 72 часа\n",
            "   📊 Биомаркеры: Концентрация IL-6 и TNF-α в плазме (пг/мл), метилирование CpG-сайта cg09809672 (%), экспрессия CDKN2A (fold change)\n",
            "   🏥 Клиническая валидация: Когорта: 100 человек в возрасте 60-80 лет, методы: анализ плазмы, биопсия кожи, биомаркеры: IL-6, TNF-α, метилирование CpG-сайта cg09809672\n",
            "   💊 Терапевтическая мишень: Ингибитор DNMT1 или активатор TET2\n",
            "   📈 Приоритет терминов:\n",
            "      • hopx+/prosp-c− cells: 39.8\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "2. ГИПОТЕЗА:\n",
            "   🧬 Термины: hopx−/prosp-c+ cells, hopx+/prosp-c+ cells, H3K4me3\n",
            "   🔬 Механизм: Снижение гистоновой метки H3K4me3 в промотере гена SIRT1 в hopx−/prosp-c+ клетках приводит к снижению экспрессии SIRT1 и увеличению экспрессии п53, что способствует старению\n",
            "   📚 Обоснование: Механистическое обоснование с конкретными белками/генами: SIRT1 и п53 играют ключевую роль в регуляции клеточного старения и выживания\n",
            "   🧬 Эпигенетика: Снижение гистоновой метки H3K4me3, увеличение гистоновой метки H3K27me3, повышение активности HDAC1 и снижение активности HAT1\n",
            "   🔥 Воспаление: Повышение IL-1β до 10.5±2.5 пг/мл и MCP-1 до 12.1±3.2 пг/мл в плазме, активация сигнального пути NF-κB p65 и IRF3\n",
            "   🛡️  Иммунитет: Увеличение CD4+CD28- сенесцентных T-клеток до 18±4% от CD4+, снижение экспрессии CD25 на Tregs\n",
            "   🧪 Эксперимент: In vitro модель: HUVEC, методы: qPCR, Western blot, ELISA, временные точки: 24, 48, 72 часа\n",
            "   📊 Биомаркеры: Концентрация IL-1β и MCP-1 в плазме (пг/мл), метилирование CpG-сайта cg05575921 (%), экспрессия SIRT1 (fold change)\n",
            "   🏥 Клиническая валидация: Когорта: 150 человек в возрасте 50-70 лет, методы: анализ плазмы, биопсия кожи, биомаркеры: IL-1β, MCP-1, метилирование CpG-сайта cg05575921\n",
            "   💊 Терапевтическая мишень: Активатор SIRT1 или ингибитор п53\n",
            "   📈 Приоритет терминов:\n",
            "      • hopx+/prosp-c+ cells: 39.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "3. ГИПОТЕЗА:\n",
            "   🧬 Термины: hopx−/prosp-c+ cells, hopx+/prosp-c− cells, TET1\n",
            "   🔬 Механизм: Гиперметилирование CpG-сайта cg09809672 в промотере гена CDKN2A в hopx−/prosp-c+ клетках приводит к увеличению экспрессии DNMT1 и снижению экспрессии TET1, что способствует старению\n",
            "   📚 Обоснование: Механистическое обоснование с конкретными белками/генами: DNMT1 и TET1 играют ключевую роль в регуляции метилирования ДНК и деметилирования, что влияет на экспрессию генов, участвующих в старении\n",
            "   🧬 Эпигенетика: Гиперметилирование CpG-сайта cg09809672, увеличение гистоновой метки H3K27me3, снижение гистоновой метки H3K4me3, повышение активности DNMT1 и снижение активности TET1\n",
            "   🔥 Воспаление: Повышение IL-6 до 15.2±3.1 пг/мл и TNF-α до 8.7±2.3 пг/мл в плазме, активация сигнального пути NF-κB p65 и IRF3\n",
            "   🛡️  Иммунитет: Увеличение CD8+CD57+KLRG1+ сенесцентных T-клеток до 23±5% от CD8+, снижение экспрессии CD28 на CD4+ T-клетках\n",
            "   🧪 Эксперимент: In vitro модель: первичные фибробласты человека, методы: qPCR, Western blot, ELISA, временные точки: 24, 48, 72 часа\n",
            "   📊 Биомаркеры: Концентрация IL-6 и TNF-α в плазме (пг/мл), метилирование CpG-сайта cg09809672 (%), экспрессия CDKN2A (fold change)\n",
            "   🏥 Клиническая валидация: Когорта: 100 человек в возрасте 60-80 лет, методы: анализ плазмы, биопсия кожи, биомаркеры: IL-6, TNF-α, метилирование CpG-сайта cg09809672\n",
            "   💊 Терапевтическая мишень: Ингибитор DNMT1 или активатор TET1\n",
            "   📈 Приоритет терминов:\n",
            "      • hopx+/prosp-c− cells: 39.8\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "4. ГИПОТЕЗА:\n",
            "   🧬 Термины: hopx−/prosp-c+ cells, hopx+/prosp-c+ cells, H3K9me3\n",
            "   🔬 Механизм: Снижение гистоновой метки H3K9me3 в промотере гена SIRT1 в hopx−/prosp-c+ клетках приводит к снижению экспрессии SIRT1 и увеличению экспрессии п53, что способствует старению\n",
            "   📚 Обоснование: Механистическое обоснование с конкретными белками/генами: SIRT1 и п53 играют ключевую роль в регуляции клеточного старения и выживания\n",
            "   🧬 Эпигенетика: Снижение гистоновой метки H3K9me3, увеличение гистоновой метки H3K27me3, повышение активности HDAC1 и снижение активности HAT1\n",
            "   🔥 Воспаление: Повышение IL-1β до 10.5±2.5 пг/мл и MCP-1 до 12.1±3.2 пг/мл в плазме, активация сигнального пути NF-κB p65 и IRF3\n",
            "   🛡️  Иммунитет: Увеличение CD4+CD28- сенесцентных T-клеток до 18±4% от CD4+, снижение экспрессии CD25 на Tregs\n",
            "   🧪 Эксперимент: In vitro модель: HUVEC, методы: qPCR, Western blot, ELISA, временные точки: 24, 48, 72 часа\n",
            "   📊 Биомаркеры: Концентрация IL-1β и MCP-1 в плазме (пг/мл), метилирование CpG-сайта cg05575921 (%), экспрессия SIRT1 (fold change)\n",
            "   🏥 Клиническая валидация: Когорта: 150 человек в возрасте 50-70 лет, методы: анализ плазмы, биопсия кожи, биомаркеры: IL-1β, MCP-1, метилирование CpG-сайта cg05575921\n",
            "   💊 Терапевтическая мишень: Активатор SIRT1 или ингибитор п53\n",
            "   📈 Приоритет терминов:\n",
            "      • hopx+/prosp-c+ cells: 39.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "5. ГИПОТЕЗА:\n",
            "   🧬 Термины: hopx−/prosp-c+ cells, hopx+/prosp-c− cells, DNMT3A\n",
            "   🔬 Механизм: Гиперметилирование CpG-сайта cg09809672 в промотере гена CDKN2A в hopx−/prosp-c+ клетках приводит к увеличению экспрессии DNMT3A и снижению экспрессии TET2, что способствует старению\n",
            "   📚 Обоснование: Механистическое обоснование с конкретными белками/генами: DNMT3A и TET2 играют ключевую роль в регуляции метилирования ДНК и деметилирования, что влияет на экспрессию генов, участвующих в старении\n",
            "   🧬 Эпигенетика: Гиперметилирование CpG-сайта cg09809672, увеличение гистоновой метки H3K27me3, снижение гистоновой метки H3K4me3, повышение активности DNMT3A и снижение активности TET2\n",
            "   🔥 Воспаление: Повышение IL-6 до 15.2±3.1 пг/мл и TNF-α до 8.7±2.3 пг/мл в плазме, активация сигнального пути NF-κB p65 и IRF3\n",
            "   🛡️  Иммунитет: Увеличение CD8+CD57+KLRG1+ сенесцентных T-клеток до 23±5% от CD8+, снижение экспрессии CD28 на CD4+ T-клетках\n",
            "   🧪 Эксперимент: In vitro модель: первичные фибробласты человека, методы: qPCR, Western blot, ELISA, временные точки: 24, 48, 72 часа\n",
            "   📊 Биомаркеры: Концентрация IL-6 и TNF-α в плазме (пг/мл), метилирование CpG-сайта cg09809672 (%), экспрессия CDKN2A (fold change)\n",
            "   🏥 Клиническая валидация: Когорта: 100 человек в возрасте 60-80 лет, методы: анализ плазмы, биопсия кожи, биомаркеры: IL-6, TNF-α, метилирование CpG-сайта cg09809672\n",
            "   💊 Терапевтическая мишень: Ингибитор DNMT3A или активатор TET2\n",
            "   📈 Приоритет терминов:\n",
            "      • hopx+/prosp-c− cells: 39.8\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬\n",
            "СПЕЦИАЛИЗИРОВАННЫЙ АНАЛИЗ: HOPX−/PROSP-C+ CELLS\n",
            "🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬\n",
            "Выберите действие:\n",
            "1. 🔍 Анализ специализированных связей\n",
            "2. 💡 Генерация клинических гипотез\n",
            "3. 📚 Показать метаданные и DOI\n",
            "4. 🔄 Выбрать новый термин\n",
            "5. ❌ Выход\n",
            "\n",
            "👆 Выберите действие (1-5): 2\n",
            "⏳ Генерация конкретных клинических гипотез...\n",
            "🌡️ Выберите температуру (0.1-1.0) [по умолчанию 0.75]: \n",
            "🌡️ Используем температуру: 0.75\n",
            "💡 Совет: 0.1-0.3=консервативно, 0.4-0.7=сбалансированно, 0.8-1.0=креативно\n",
            "\n",
            "🔍 АНАЛИЗ СПЕЦИАЛИЗИРОВАННЫХ СВЯЗЕЙ: hopx−/prosp-c+ cells\n",
            "------------------------------------------------------------\n",
            "🔍 Инициализация для графа: 32,707 узлов, 775,715 рёбер\n",
            "📋 Извлечено 68 уникальных ключевых слов\n",
            "🚀 Построение индексов...\n",
            "🔍 Поиск специализированных терминов по вашим ключевым словам...\n",
            "📚 Построение индекса DOI...\n",
            "🔍 Примеры метаданных для отладки:\n",
            "   muscle biopsies: <class 'dict'>\n",
            "      ключи: ['dois', 'years', 'titles', 'authors', 'keywords']\n",
            "      dois: ['https://doi.org/10.1186/s13072-025-00601-w', nan]\n",
            "   human myoblast: <class 'dict'>\n",
            "      ключи: ['dois', 'years', 'titles', 'authors', 'keywords']\n",
            "      dois: ['https://doi.org/10.1186/s13072-025-00601-w', 'https://doi.org/10.3390/ijms241713181']\n",
            "   tet1-2: <class 'dict'>\n",
            "      ключи: ['dois', 'years', 'titles', 'authors', 'keywords']\n",
            "      dois: ['https://doi.org/10.1186/s13072-025-00601-w']\n",
            "✅ Найдено 23286 узлов с DOI/PMID\n",
            "🔍 Примеры найденных DOI:\n",
            "   muscle biopsies: https://doi.org/10.1186/s13072-025-00601-w\n",
            "   human myoblast: https://doi.org/10.1186/s13072-025-00601-w\n",
            "   tet1-2: https://doi.org/10.1186/s13072-025-00601-w\n",
            "   human cancers [: https://doi.org/10.1186/s13100-024-00339-4\n",
            "   tet2: https://doi.org/10.1038/s42255-023-00855-2\n",
            "✅ Найдено 276 специализированных терминов\n",
            "🔧 Построение специализированных категорий с расширенными ключевыми словами...\n",
            "🎯 Минимальный порог специализированных соседей: 1\n",
            "\n",
            "🔍 ДИАГНОСТИКА С РАСШИРЕННЫМИ КЛЮЧЕВЫМИ СЛОВАМИ:\n",
            "    1. human hopx genome.blue arrowhead    ✅ (всего: 1, спец: 0)\n",
            "    2. human coa1                          ✅ (всего: 3, спец: 0)\n",
            "    3. human col18a1 promoter 2            ✅ (всего: 3, спец: 0)\n",
            "    4. human hopx proteins                 ✅ (всего: 3, спец: 0)\n",
            "    5. anti-human emilin1                  ✅ (всего: 6, спец: 2)\n",
            "    6. high hopx methylation               ✅ (всего: 1, спец: 0)\n",
            "    7. gdf15 human                         ✅ (всего: 6, спец: 0)\n",
            "    8. anti‐human hmgb1 human              ✅ (всего: 6, спец: 0)\n",
            "    9. human hopx protein                  ✅ (всего: 5, спец: 0)\n",
            "   10. human emilin1                       ✅ (всего: 7, спец: 2)\n",
            "   11. human hopx                          ✅ (всего: 7, спец: 0)\n",
            "   12. human pro-longevity                 ✅ (всего: 1, спец: 0)\n",
            "   13. human aging.skin                    ✅ (всего: 1, спец: 0)\n",
            "   14. human col18a1                       ✅ (всего: 8, спец: 0)\n",
            "   15. human longevity.23                  ✅ (всего: 2, спец: 0)\n",
            "   16. human longevity-specific            ✅ (всего: 2, спец: 0)\n",
            "   17. human senescence-associated         ✅ (всего: 1, спец: 0)\n",
            "   18. human senescence-associated β-galactosidase ✅ (всего: 1, спец: 0)\n",
            "   19. human cellular senescence-associated ✅ (всего: 1, спец: 0)\n",
            "   20. coa1/c7orf44/mitrac15               ❌ (всего: 1, спец: 0)\n",
            "\n",
            "📊 СТАТИСТИКА С РАСШИРЕННЫМИ КРИТЕРИЯМИ:\n",
            "   Терминов соответствует расширенным критериям: 178\n",
            "\n",
            "🚫 Отфильтровано 151 из 178 терминов\n",
            "✅ В категории добавлено 26 качественных терминов\n",
            "\n",
            "📊 СТАТИСТИКА ПО КАТЕГОРИЯМ (с расширенными ключевыми словами):\n",
            "   🧬 Биомаркеры старения человека: 15 терминов (средняя степень: 5.0)\n",
            "   🔬 Эпигенетические часы: 0 терминов ⚠️\n",
            "   ⚡ Воспаление при старении: 0 терминов ⚠️\n",
            "   🧪 Гены и белки старения: 1 терминов (средняя степень: 8.0)\n",
            "   🏥 Терапевтические мишени: 0 терминов ⚠️\n",
            "   🧠 Иммунное старение: 0 терминов ⚠️\n",
            "   🔀 Пути долголетия: 0 терминов ⚠️\n",
            "   📊 Клеточные процессы: 10 терминов (средняя степень: 2.3)\n",
            "🎯 Выбрано 2 специализированных соседей из 7\n",
            "\n",
            "💡 ГЕНЕРАЦИЯ СПЕЦИАЛИЗИРОВАННЫХ ГИПОТЕЗ\n",
            "🌡️ Температура: 0.75 (0.1=консервативно, 1.0=креативно)\n",
            "------------------------------------------------------------\n",
            "\n",
            "📚 КОНКРЕТНЫЕ ГИПОТЕЗЫ СТАРЕНИЯ ЧЕЛОВЕКА\n",
            "🌡️ Температура генерации: 0.75\n",
            "================================================================================\n",
            "\n",
            "1. ГИПОТЕЗА:\n",
            "   🧬 Термины: hopx−/prosp-c+ cells, hopx+/prosp-c− cells, DNMT1\n",
            "   🔬 Механизм: Гипометилирование CpG-сайта cg09809672 в промотере генов, связанных с старением, в результате снижения активности DNMT1 в hopx−/prosp-c+ клетках приводит к увеличению экспрессии провоспалительных цитокинов, таких как IL-1β и TNF-α, и активации NF-κB p65 сигнального пути\n",
            "   📚 Обоснование: Снижение активности DNMT1 в hopx−/prosp-c+ клетках может привести к гипометилированию CpG-сайта cg09809672, что, в свою очередь, увеличивает экспрессию генов, связанных с воспалением и старением\n",
            "   🧬 Эпигенетика: Гипометилирование CpG-сайта cg09809672, снижение активности DNMT1, увеличение гистоновой метки H3K4me3\n",
            "   🔥 Воспаление: Повышение IL-1β до 12.5±2.8 пг/мл и TNF-α до 6.3±1.9 пг/мл в плазме, активация NF-κB p65 сигнального пути\n",
            "   🛡️  Иммунитет: Увеличение CD4+CD28- сенесцентных T-клеток до 18±4% от CD4+, повышение экспрессии поверхностного маркера CD57\n",
            "   🧪 Эксперимент: In vitro модель: первичные фибробласты человека, методы: qPCR, Western blot, ELISA, временные точки: 24, 48, 72 часа\n",
            "   📊 Биомаркеры: Концентрация IL-1β и TNF-α в плазме (пг/мл), процент метилирования CpG-сайта cg09809672 (%), экспрессия мРНК генов, связанных с воспалением и старением (fold change, ΔCt)\n",
            "   🏥 Клиническая валидация: Клиническая когорта: 100 участников в возрасте 65-85 лет, методы: анализ плазмы, биопсия кожи, биомаркеры: концентрация IL-1β и TNF-α, процент метилирования CpG-сайта cg09809672\n",
            "   💊 Терапевтическая мишень: DNMT1, NF-κB p65 сигнальный путь\n",
            "   📈 Приоритет терминов:\n",
            "      • hopx+/prosp-c− cells: 39.8\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "2. ГИПОТЕЗА:\n",
            "   🧬 Термины: hopx−/prosp-c+ cells, hopx+/prosp-c+ cells, TET2\n",
            "   🔬 Механизм: Гиперметилирование CpG-сайта cg05575921 в промотере генов, связанных с дифференцировкой, в результате повышения активности TET2 в hopx−/prosp-c+ клетках приводит к снижению экспрессии генов, связанных с дифференцировкой, и увеличению экспрессии сенесцентных маркеров, таких как p16INK4a и p21CIP1\n",
            "   📚 Обоснование: Повышение активности TET2 в hopx−/prosp-c+ клетках может привести к гиперметилированию CpG-сайта cg05575921, что, в свою очередь, снижает экспрессию генов, связанных с дифференцировкой, и увеличивает экспрессию сенесцентных маркеров\n",
            "   🧬 Эпигенетика: Гиперметилирование CpG-сайта cg05575921, повышение активности TET2, снижение гистоновой метки H3K27me3\n",
            "   🔥 Воспаление: Повышение IL-8 до 10.2±2.5 пг/мл и MCP-1 до 5.6±1.7 пг/мл в плазме, активация STAT3 сигнального пути\n",
            "   🛡️  Иммунитет: Увеличение CD8+CD57+ сенесцентных T-клеток до 25±5% от CD8+, повышение экспрессии поверхностного маркера KLRG1\n",
            "   🧪 Эксперимент: In vitro модель: HUVEC, методы: qPCR, Western blot, ELISA, временные точки: 24, 48, 72 часа\n",
            "   📊 Биомаркеры: Концентрация IL-8 и MCP-1 в плазме (пг/мл), процент метилирования CpG-сайта cg05575921 (%), экспрессия мРНК генов, связанных с дифференцировкой и сенесценцией (fold change, ΔCt)\n",
            "   🏥 Клиническая валидация: Клиническая когорта: 100 участников в возрасте 65-85 лет, методы: анализ плазмы, биопсия кожи, биомаркеры: концентрация IL-8 и MCP-1, процент метилирования CpG-сайта cg05575921\n",
            "   💊 Терапевтическая мишень: TET2, STAT3 сигнальный путь\n",
            "   📈 Приоритет терминов:\n",
            "      • hopx+/prosp-c+ cells: 39.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "3. ГИПОТЕЗА:\n",
            "   🧬 Термины: hopx−/prosp-c+ cells, hopx+/prosp-c− cells, DNMT3A\n",
            "   🔬 Механизм: Гипометилирование CpG-сайта cg09809672 в промотере генов, связанных с апоптозом, в результате снижения активности DNMT3A в hopx−/prosp-c+ клетках приводит к увеличению экспрессии провоспалительных цитокинов, таких как IL-1β и TNF-α, и активации NF-κB p65 сигнального пути\n",
            "   📚 Обоснование: Снижение активности DNMT3A в hopx−/prosp-c+ клетках может привести к гипометилированию CpG-сайта cg09809672, что, в свою очередь, увеличивает экспрессию генов, связанных с воспалением и апоптозом\n",
            "   🧬 Эпигенетика: Гипометилирование CpG-сайта cg09809672, снижение активности DNMT3A, увеличение гистоновой метки H3K4me3\n",
            "   🔥 Воспаление: Повышение IL-1β до 12.5±2.8 пг/мл и TNF-α до 6.3±1.9 пг/мл в плазме, активация NF-κB p65 сигнального пути\n",
            "   🛡️  Иммунитет: Увеличение CD4+CD28- сенесцентных T-клеток до 18±4% от CD4+, повышение экспрессии поверхностного маркера CD57\n",
            "   🧪 Эксперимент: In vitro модель: первичные фибробласты человека, методы: qPCR, Western blot, ELISA, временные точки: 24, 48, 72 часа\n",
            "   📊 Биомаркеры: Концентрация IL-1β и TNF-α в плазме (пг/мл), процент метилирования CpG-сайта cg09809672 (%), экспрессия мРНК генов, связанных с воспалением и апоптозом (fold change, ΔCt)\n",
            "   🏥 Клиническая валидация: Клиническая когорта: 100 участников в возрасте 65-85 лет, методы: анализ плазмы, биопсия кожи, биомаркеры: концентрация IL-1β и TNF-α, процент метилирования CpG-сайта cg09809672\n",
            "   💊 Терапевтическая мишень: DNMT3A, NF-κB p65 сигнальный путь\n",
            "   📈 Приоритет терминов:\n",
            "      • hopx+/prosp-c− cells: 39.8\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "4. ГИПОТЕЗА:\n",
            "   🧬 Термины: hopx−/prosp-c+ cells, hopx+/prosp-c+ cells, TET1\n",
            "   🔬 Механизм: Гиперметилирование CpG-сайта cg05575921 в промотере генов, связанных с пролиферацией, в результате повышения активности TET1 в hopx−/prosp-c+ клетках приводит к снижению экспрессии генов, связанных с пролиферацией, и увеличению экспрессии сенесцентных маркеров, таких как p16INK4a и p21CIP1\n",
            "   📚 Обоснование: Повышение активности TET1 в hopx−/prosp-c+ клетках может привести к гиперметилированию CpG-сайта cg05575921, что, в свою очередь, снижает экспрессию генов, связанных с пролиферацией, и увеличивает экспрессию сенесцентных маркеров\n",
            "   🧬 Эпигенетика: Гиперметилирование CpG-сайта cg05575921, повышение активности TET1, снижение гистоновой метки H3K27me3\n",
            "   🔥 Воспаление: Повышение IL-8 до 10.2±2.5 пг/мл и MCP-1 до 5.6±1.7 пг/мл в плазме, активация STAT3 сигнального пути\n",
            "   🛡️  Иммунитет: Увеличение CD8+CD57+ сенесцентных T-клеток до 25±5% от CD8+, повышение экспрессии поверхностного маркера KLRG1\n",
            "   🧪 Эксперимент: In vitro модель: HUVEC, методы: qPCR, Western blot, ELISA, временные точки: 24, 48, 72 часа\n",
            "   📊 Биомаркеры: Концентрация IL-8 и MCP-1 в плазме (пг/мл), процент метилирования CpG-сайта cg05575921 (%), экспрессия мРНК генов, связанных с пролиферацией и сенесценцией (fold change, ΔCt)\n",
            "   🏥 Клиническая валидация: Клиническая когорта: 100 участников в возрасте 65-85 лет, методы: анализ плазмы, биопсия кожи, биомаркеры: концентрация IL-8 и MCP-1, процент метилирования CpG-сайта cg05575921\n",
            "   💊 Терапевтическая мишень: TET1, STAT3 сигнальный путь\n",
            "   📈 Приоритет терминов:\n",
            "      • hopx+/prosp-c+ cells: 39.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "5. ГИПОТЕЗА:\n",
            "   🧬 Термины: hopx−/prosp-c+ cells, hopx+/prosp-c− cells, DNMT3B\n",
            "   🔬 Механизм: Гипометилирование CpG-сайта cg09809672 в промотере генов, связанных с дифференцировкой, в результате снижения активности DNMT3B в hopx−/prosp-c+ клетках приводит к увеличению экспрессии провоспалительных цитокинов, таких как IL-1β и TNF-α, и активации NF-κB p65 сигнального пути\n",
            "   📚 Обоснование: Снижение активности DNMT3B в hopx−/prosp-c+ клетках может привести к гипометилированию CpG-сайта cg09809672, что, в свою очередь, увеличивает экспрессию генов, связанных с воспалением и дифференцировкой\n",
            "   🧬 Эпигенетика: Гипометилирование CpG-сайта cg09809672, снижение активности DNMT3B, увеличение гистоновой метки H3K4me3\n",
            "   🔥 Воспаление: Повышение IL-1β до 12.5±2.8 пг/мл и TNF-α до 6.3±1.9 пг/мл в плазме, активация NF-κB p65 сигнального пути\n",
            "   🛡️  Иммунитет: Увеличение CD4+CD28- сенесцентных T-клеток до 18±4% от CD4+, повышение экспрессии поверхностного маркера CD57\n",
            "   🧪 Эксперимент: In vitro модель: первичные фибробласты человека, методы: qPCR, Western blot, ELISA, временные точки: 24, 48, 72 часа\n",
            "   📊 Биомаркеры: Концентрация IL-1β и TNF-α в плазме (пг/мл), процент метилирования CpG-сайта cg09809672 (%), экспрессия мРНК генов, связанных с воспалением и дифференцировкой (fold change, ΔCt)\n",
            "   🏥 Клиническая валидация: Клиническая когорта: 100 участников в возрасте 65-85 лет, методы: анализ плазмы, биопсия кожи, биомаркеры: концентрация IL-1β и TNF-α, процент метилирования CpG-сайта cg09809672\n",
            "   💊 Терапевтическая мишень: DNMT3B, NF-κB p65 сигнальный путь\n",
            "   📈 Приоритет терминов:\n",
            "      • hopx+/prosp-c− cells: 39.8\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬\n",
            "СПЕЦИАЛИЗИРОВАННЫЙ АНАЛИЗ: HOPX−/PROSP-C+ CELLS\n",
            "🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬\n",
            "Выберите действие:\n",
            "1. 🔍 Анализ специализированных связей\n",
            "2. 💡 Генерация клинических гипотез\n",
            "3. 📚 Показать метаданные и DOI\n",
            "4. 🔄 Выбрать новый термин\n",
            "5. ❌ Выход\n",
            "\n",
            "👆 Выберите действие (1-5): 4\n",
            "\n",
            "🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯\n",
            "ВЫБОР СПЕЦИАЛИЗИРОВАННОГО ТЕРМИНА\n",
            "🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯\n",
            "\n",
            "🎯 СПЕЦИАЛИЗИРОВАННЫЕ КАТЕГОРИИ ПО ВАШИМ КЛЮЧЕВЫМ СЛОВАМ\n",
            "================================================================================\n",
            "\n",
            "1. 🧬 Биомаркеры старения человека (15 терминов)\n",
            "   Топ-3 по специализированному скору:\n",
            "   1. anti-human emilin1 (связей: 6, DOI: 📚, скор: 49.0)\n",
            "   2. human emilin1 (связей: 7, DOI: 📚, скор: 47.3)\n",
            "   3. extracellular protein emilin1 (связей: 2, DOI: 📚, скор: 43.0)\n",
            "\n",
            "2. 🔬 Эпигенетические часы (0 терминов)\n",
            "   Термины не найдены\n",
            "\n",
            "3. ⚡ Воспаление при старении (0 терминов)\n",
            "   Термины не найдены\n",
            "\n",
            "4. 🧪 Гены и белки старения (1 терминов)\n",
            "   Топ-3 по специализированному скору:\n",
            "   1. ecm glycoprotein emilin1 (связей: 8, DOI: 📚, скор: 37.0)\n",
            "\n",
            "5. 🏥 Терапевтические мишени (0 терминов)\n",
            "   Термины не найдены\n",
            "\n",
            "6. 🧠 Иммунное старение (0 терминов)\n",
            "   Термины не найдены\n",
            "\n",
            "7. 🔀 Пути долголетия (0 терминов)\n",
            "   Термины не найдены\n",
            "\n",
            "8. 📊 Клеточные процессы (10 терминов)\n",
            "   Топ-3 по специализированному скору:\n",
            "   1. phospho-vimentin+/hopx+ cells (связей: 1, DOI: 📚, скор: 44.0)\n",
            "   2. coa1δ cells (связей: 3, DOI: 📚, скор: 42.0)\n",
            "   3. hopx-derived alveolar cells (связей: 3, DOI: 📚, скор: 42.0)\n",
            "\n",
            "0. ✍️  Ввести термин вручную\n",
            "99. 🎲 Случайный специализированный термин\n",
            "\n",
            "👆 Выберите категорию (1-8), 0 для ручного ввода, 99 для случайного: human hopx protein\n",
            "⚠️ Неверный выбор\n",
            "\n",
            "🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯\n",
            "ВЫБОР СПЕЦИАЛИЗИРОВАННОГО ТЕРМИНА\n",
            "🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯\n",
            "\n",
            "🎯 СПЕЦИАЛИЗИРОВАННЫЕ КАТЕГОРИИ ПО ВАШИМ КЛЮЧЕВЫМ СЛОВАМ\n",
            "================================================================================\n",
            "\n",
            "1. 🧬 Биомаркеры старения человека (15 терминов)\n",
            "   Топ-3 по специализированному скору:\n",
            "   1. anti-human emilin1 (связей: 6, DOI: 📚, скор: 49.0)\n",
            "   2. human emilin1 (связей: 7, DOI: 📚, скор: 47.3)\n",
            "   3. extracellular protein emilin1 (связей: 2, DOI: 📚, скор: 43.0)\n",
            "\n",
            "2. 🔬 Эпигенетические часы (0 терминов)\n",
            "   Термины не найдены\n",
            "\n",
            "3. ⚡ Воспаление при старении (0 терминов)\n",
            "   Термины не найдены\n",
            "\n",
            "4. 🧪 Гены и белки старения (1 терминов)\n",
            "   Топ-3 по специализированному скору:\n",
            "   1. ecm glycoprotein emilin1 (связей: 8, DOI: 📚, скор: 37.0)\n",
            "\n",
            "5. 🏥 Терапевтические мишени (0 терминов)\n",
            "   Термины не найдены\n",
            "\n",
            "6. 🧠 Иммунное старение (0 терминов)\n",
            "   Термины не найдены\n",
            "\n",
            "7. 🔀 Пути долголетия (0 терминов)\n",
            "   Термины не найдены\n",
            "\n",
            "8. 📊 Клеточные процессы (10 терминов)\n",
            "   Топ-3 по специализированному скору:\n",
            "   1. phospho-vimentin+/hopx+ cells (связей: 1, DOI: 📚, скор: 44.0)\n",
            "   2. coa1δ cells (связей: 3, DOI: 📚, скор: 42.0)\n",
            "   3. hopx-derived alveolar cells (связей: 3, DOI: 📚, скор: 42.0)\n",
            "\n",
            "0. ✍️  Ввести термин вручную\n",
            "99. 🎲 Случайный специализированный термин\n",
            "\n",
            "👆 Выберите категорию (1-8), 0 для ручного ввода, 99 для случайного: 8\n",
            "\n",
            "📋 📊 Клеточные процессы\n",
            "============================================================\n",
            " 1. phospho-vimentin+/hopx+ cells       (связей: 1, DOI: 📚, КС: 1, скор: 44.0)\n",
            " 2. coa1δ cells                         (связей: 3, DOI: 📚, КС: 1, скор: 42.0)\n",
            " 3. hopx-derived alveolar cells         (связей: 3, DOI: 📚, КС: 1, скор: 42.0)\n",
            " 4. coa1δ mitochondria                  (связей: 4, DOI: 📚, КС: 1, скор: 41.0)\n",
            " 5. hopx+spc+ cells                     (связей: 5, DOI: 📚, КС: 1, скор: 40.0)\n",
            " 6. hopx+/prosp-c− cells                (связей: 4, DOI: 📚, КС: 1, скор: 39.8)\n",
            " 7. hopx+/prosp-c+ cells                (связей: 6, DOI: 📚, КС: 1, скор: 39.0)\n",
            " 8. basal hopx+ cells                   (связей: 5, DOI: 📚, КС: 1, скор: 39.0)\n",
            " 9. hopx−/prosp-c+ cells                (связей: 7, DOI: 📚, КС: 1, скор: 37.3)\n",
            "10. hopx+ cells                         (связей: 57, DOI: 📚, КС: 1, скор: 18.9)\n",
            "\n",
            "👆 Выберите термин (1-10): 11\n",
            "\n",
            "🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯\n",
            "ВЫБОР СПЕЦИАЛИЗИРОВАННОГО ТЕРМИНА\n",
            "🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯\n",
            "\n",
            "🎯 СПЕЦИАЛИЗИРОВАННЫЕ КАТЕГОРИИ ПО ВАШИМ КЛЮЧЕВЫМ СЛОВАМ\n",
            "================================================================================\n",
            "\n",
            "1. 🧬 Биомаркеры старения человека (15 терминов)\n",
            "   Топ-3 по специализированному скору:\n",
            "   1. anti-human emilin1 (связей: 6, DOI: 📚, скор: 49.0)\n",
            "   2. human emilin1 (связей: 7, DOI: 📚, скор: 47.3)\n",
            "   3. extracellular protein emilin1 (связей: 2, DOI: 📚, скор: 43.0)\n",
            "\n",
            "2. 🔬 Эпигенетические часы (0 терминов)\n",
            "   Термины не найдены\n",
            "\n",
            "3. ⚡ Воспаление при старении (0 терминов)\n",
            "   Термины не найдены\n",
            "\n",
            "4. 🧪 Гены и белки старения (1 терминов)\n",
            "   Топ-3 по специализированному скору:\n",
            "   1. ecm glycoprotein emilin1 (связей: 8, DOI: 📚, скор: 37.0)\n",
            "\n",
            "5. 🏥 Терапевтические мишени (0 терминов)\n",
            "   Термины не найдены\n",
            "\n",
            "6. 🧠 Иммунное старение (0 терминов)\n",
            "   Термины не найдены\n",
            "\n",
            "7. 🔀 Пути долголетия (0 терминов)\n",
            "   Термины не найдены\n",
            "\n",
            "8. 📊 Клеточные процессы (10 терминов)\n",
            "   Топ-3 по специализированному скору:\n",
            "   1. phospho-vimentin+/hopx+ cells (связей: 1, DOI: 📚, скор: 44.0)\n",
            "   2. coa1δ cells (связей: 3, DOI: 📚, скор: 42.0)\n",
            "   3. hopx-derived alveolar cells (связей: 3, DOI: 📚, скор: 42.0)\n",
            "\n",
            "0. ✍️  Ввести термин вручную\n",
            "99. 🎲 Случайный специализированный термин\n",
            "\n",
            "👆 Выберите категорию (1-8), 0 для ручного ввода, 99 для случайного: 8\n",
            "\n",
            "📋 📊 Клеточные процессы\n",
            "============================================================\n",
            " 1. phospho-vimentin+/hopx+ cells       (связей: 1, DOI: 📚, КС: 1, скор: 44.0)\n",
            " 2. coa1δ cells                         (связей: 3, DOI: 📚, КС: 1, скор: 42.0)\n",
            " 3. hopx-derived alveolar cells         (связей: 3, DOI: 📚, КС: 1, скор: 42.0)\n",
            " 4. coa1δ mitochondria                  (связей: 4, DOI: 📚, КС: 1, скор: 41.0)\n",
            " 5. hopx+spc+ cells                     (связей: 5, DOI: 📚, КС: 1, скор: 40.0)\n",
            " 6. hopx+/prosp-c− cells                (связей: 4, DOI: 📚, КС: 1, скор: 39.8)\n",
            " 7. hopx+/prosp-c+ cells                (связей: 6, DOI: 📚, КС: 1, скор: 39.0)\n",
            " 8. basal hopx+ cells                   (связей: 5, DOI: 📚, КС: 1, скор: 39.0)\n",
            " 9. hopx−/prosp-c+ cells                (связей: 7, DOI: 📚, КС: 1, скор: 37.3)\n",
            "10. hopx+ cells                         (связей: 57, DOI: 📚, КС: 1, скор: 18.9)\n",
            "\n",
            "👆 Выберите термин (1-10): 4\n",
            "\n",
            "📋 ДЕТАЛЬНАЯ ИНФОРМАЦИЯ: coa1δ mitochondria\n",
            "============================================================\n",
            "🔗 Степень узла: 4\n",
            "🎯 Специализированный скор: 41.0\n",
            "👥 Соседей: 4\n",
            "📚 DOI: https://doi.org/10.1083/jcb.201007026\n",
            "🔑 Совпадения с вашими ключевыми словами: coa1\n",
            "📄 Заголовок: Coa3 and Cox14 are essential for negative feedback regulation of COX1 translation in mitochondria...\n",
            "👨‍🔬 Авторы: David U. Mick, Milena Vukotic, Heike Piechura, Helmut E. Meyer, Bettina Warscheid, Markus Deckers, P...\n",
            "📅 Год: 2010\n",
            "📊 Диапазон лет: 2010-2010\n",
            "📚 Всего статей: 1\n",
            "🏷️  Ключевые слова источника: COA1...\n",
            "📝 Описание: Biomedical entity from: Coa3 and Cox14 are essential for negative feedback regulation of COX1 translation in mitochondria...\n",
            "\n",
            "🔗 Топ-5 специализированных соседей:\n",
            "   1. coa1δ (скор: 41.0, связей: 4, DOI: 📚)\n",
            "   2. 250-kd (скор: 23.2, связей: 6, DOI: 📚)\n",
            "   3. wild-type mitochondria (скор: 20.8, связей: 8, DOI: 📚)\n",
            "   4. mitochondria (скор: 2.2, связей: 955, DOI: 📚)\n",
            "\n",
            "🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬\n",
            "СПЕЦИАЛИЗИРОВАННЫЙ АНАЛИЗ: COA1Δ MITOCHONDRIA\n",
            "🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬\n",
            "Выберите действие:\n",
            "1. 🔍 Анализ специализированных связей\n",
            "2. 💡 Генерация клинических гипотез\n",
            "3. 📚 Показать метаданные и DOI\n",
            "4. 🔄 Выбрать новый термин\n",
            "5. ❌ Выход\n",
            "\n",
            "👆 Выберите действие (1-5): 2\n",
            "⏳ Генерация конкретных клинических гипотез...\n",
            "🌡️ Выберите температуру (0.1-1.0) [по умолчанию 0.75]: \n",
            "🌡️ Используем температуру: 0.75\n",
            "💡 Совет: 0.1-0.3=консервативно, 0.4-0.7=сбалансированно, 0.8-1.0=креативно\n",
            "\n",
            "🔍 АНАЛИЗ СПЕЦИАЛИЗИРОВАННЫХ СВЯЗЕЙ: coa1δ mitochondria\n",
            "------------------------------------------------------------\n",
            "🔍 Инициализация для графа: 32,707 узлов, 775,715 рёбер\n",
            "📋 Извлечено 68 уникальных ключевых слов\n",
            "🚀 Построение индексов...\n",
            "🔍 Поиск специализированных терминов по вашим ключевым словам...\n",
            "📚 Построение индекса DOI...\n",
            "🔍 Примеры метаданных для отладки:\n",
            "   muscle biopsies: <class 'dict'>\n",
            "      ключи: ['dois', 'years', 'titles', 'authors', 'keywords']\n",
            "      dois: ['https://doi.org/10.1186/s13072-025-00601-w', nan]\n",
            "   human myoblast: <class 'dict'>\n",
            "      ключи: ['dois', 'years', 'titles', 'authors', 'keywords']\n",
            "      dois: ['https://doi.org/10.1186/s13072-025-00601-w', 'https://doi.org/10.3390/ijms241713181']\n",
            "   tet1-2: <class 'dict'>\n",
            "      ключи: ['dois', 'years', 'titles', 'authors', 'keywords']\n",
            "      dois: ['https://doi.org/10.1186/s13072-025-00601-w']\n",
            "✅ Найдено 23286 узлов с DOI/PMID\n",
            "🔍 Примеры найденных DOI:\n",
            "   muscle biopsies: https://doi.org/10.1186/s13072-025-00601-w\n",
            "   human myoblast: https://doi.org/10.1186/s13072-025-00601-w\n",
            "   tet1-2: https://doi.org/10.1186/s13072-025-00601-w\n",
            "   human cancers [: https://doi.org/10.1186/s13100-024-00339-4\n",
            "   tet2: https://doi.org/10.1038/s42255-023-00855-2\n",
            "✅ Найдено 276 специализированных терминов\n",
            "🔧 Построение специализированных категорий с расширенными ключевыми словами...\n",
            "🎯 Минимальный порог специализированных соседей: 1\n",
            "\n",
            "🔍 ДИАГНОСТИКА С РАСШИРЕННЫМИ КЛЮЧЕВЫМИ СЛОВАМИ:\n",
            "    1. human hopx genome.blue arrowhead    ✅ (всего: 1, спец: 0)\n",
            "    2. human coa1                          ✅ (всего: 3, спец: 0)\n",
            "    3. human col18a1 promoter 2            ✅ (всего: 3, спец: 0)\n",
            "    4. human hopx proteins                 ✅ (всего: 3, спец: 0)\n",
            "    5. anti-human emilin1                  ✅ (всего: 6, спец: 2)\n",
            "    6. high hopx methylation               ✅ (всего: 1, спец: 0)\n",
            "    7. gdf15 human                         ✅ (всего: 6, спец: 0)\n",
            "    8. anti‐human hmgb1 human              ✅ (всего: 6, спец: 0)\n",
            "    9. human hopx protein                  ✅ (всего: 5, спец: 0)\n",
            "   10. human emilin1                       ✅ (всего: 7, спец: 2)\n",
            "   11. human hopx                          ✅ (всего: 7, спец: 0)\n",
            "   12. human pro-longevity                 ✅ (всего: 1, спец: 0)\n",
            "   13. human aging.skin                    ✅ (всего: 1, спец: 0)\n",
            "   14. human col18a1                       ✅ (всего: 8, спец: 0)\n",
            "   15. human longevity.23                  ✅ (всего: 2, спец: 0)\n",
            "   16. human longevity-specific            ✅ (всего: 2, спец: 0)\n",
            "   17. human senescence-associated         ✅ (всего: 1, спец: 0)\n",
            "   18. human senescence-associated β-galactosidase ✅ (всего: 1, спец: 0)\n",
            "   19. human cellular senescence-associated ✅ (всего: 1, спец: 0)\n",
            "   20. coa1/c7orf44/mitrac15               ❌ (всего: 1, спец: 0)\n",
            "\n",
            "📊 СТАТИСТИКА С РАСШИРЕННЫМИ КРИТЕРИЯМИ:\n",
            "   Терминов соответствует расширенным критериям: 178\n",
            "\n",
            "🚫 Отфильтровано 151 из 178 терминов\n",
            "✅ В категории добавлено 26 качественных терминов\n",
            "\n",
            "📊 СТАТИСТИКА ПО КАТЕГОРИЯМ (с расширенными ключевыми словами):\n",
            "   🧬 Биомаркеры старения человека: 15 терминов (средняя степень: 5.0)\n",
            "   🔬 Эпигенетические часы: 0 терминов ⚠️\n",
            "   ⚡ Воспаление при старении: 0 терминов ⚠️\n",
            "   🧪 Гены и белки старения: 1 терминов (средняя степень: 8.0)\n",
            "   🏥 Терапевтические мишени: 0 терминов ⚠️\n",
            "   🧠 Иммунное старение: 0 терминов ⚠️\n",
            "   🔀 Пути долголетия: 0 терминов ⚠️\n",
            "   📊 Клеточные процессы: 10 терминов (средняя степень: 2.3)\n",
            "🎯 Выбрано 1 специализированных соседей из 4\n",
            "\n",
            "💡 ГЕНЕРАЦИЯ СПЕЦИАЛИЗИРОВАННЫХ ГИПОТЕЗ\n",
            "🌡️ Температура: 0.75 (0.1=консервативно, 1.0=креативно)\n",
            "------------------------------------------------------------\n",
            "\n",
            "📚 КОНКРЕТНЫЕ ГИПОТЕЗЫ СТАРЕНИЯ ЧЕЛОВЕКА\n",
            "🌡️ Температура генерации: 0.75\n",
            "================================================================================\n",
            "\n",
            "1. ГИПОТЕЗА:\n",
            "   🧬 Термины: coa1δ mitochondria, DNMT1, H3K4me3\n",
            "   🔬 Механизм: Гипометилирование CpG-сайта cg09809672 в гене COA1δ, обусловленное снижением активности DNMT1 и увеличением гистоновой метки H3K4me3, приводит к увеличению экспрессии COA1δ и улучшению функции митохондрий в стареющих клетках\n",
            "   📚 Обоснование: Снижение активности DNMT1 и увеличение гистоновой метки H3K4me3 могут привести к гипометилированию CpG-сайта cg09809672, что увеличивает экспрессию COA1δ и улучшает функцию митохондрий\n",
            "   🧬 Эпигенетика: Гипометилирование CpG-сайта cg09809672, снижение активности DNMT1, увеличение гистоновой метки H3K4me3\n",
            "   🔥 Воспаление: Снижение уровня IL-1β до 2.1±0.5 пг/мл и TNF-α до 1.2±0.3 пг/мл в плазме\n",
            "   🛡️  Иммунитет: Увеличение CD4+CD25+FOXP3+ Tregs до 15±3% от CD4+\n",
            "   🧪 Эксперимент: In vitro модель: первичные фибробласты человека, методы: qPCR, Western blot, ELISA, временные точки: 24, 48, 72 часа\n",
            "   📊 Биомаркеры: Уровень COA1δ мРНК в фибробластах (fold change: 2.5±0.5), уровень IL-1β в плазме (пг/мл: 2.1±0.5), процент CD4+CD25+FOXP3+ Tregs (15±3%)\n",
            "   🏥 Клиническая валидация: Клиническая когорта: 100 человек в возрасте 60-80 лет, методы: qPCR, ELISA, биомаркеры: уровень COA1δ мРНК в крови, уровень IL-1β в плазме\n",
            "   💊 Терапевтическая мишень: Увеличение экспрессии COA1δ и улучшение функции митохондрий\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "2. ГИПОТЕЗА:\n",
            "   🧬 Термины: coa1δ mitochondria, TET2, H3K27me3\n",
            "   🔬 Механизм: Гипометилирование CpG-сайта cg05575921 в гене COA1δ, обусловленное увеличением активности TET2 и снижением гистоновой метки H3K27me3, приводит к увеличению экспрессии COA1δ и улучшению функции митохондрий в стареющих клетках\n",
            "   📚 Обоснование: Увеличение активности TET2 и снижение гистоновой метки H3K27me3 могут привести к гипометилированию CpG-сайта cg05575921, что увеличивает экспрессию COA1δ и улучшает функцию митохондрий\n",
            "   🧬 Эпигенетика: Гипометилирование CpG-сайта cg05575921, увеличение активности TET2, снижение гистоновой метки H3K27me3\n",
            "   🔥 Воспаление: Снижение уровня IL-6 до 10.5±2.1 пг/мл и TNF-α до 5.6±1.2 пг/мл в плазме\n",
            "   🛡️  Иммунитет: Увеличение CD8+CD57+KLRG1+ сенесцентных T-клеток до 25±5% от CD8+\n",
            "   🧪 Эксперимент: In vivo модель: мыши C57BL/6, методы: qPCR, Western blot, ELISA, временные точки: 1, 3, 6 месяцев\n",
            "   📊 Биомаркеры: Уровень COA1δ мРНК в мышцах (fold change: 3.2±0.6), уровень IL-6 в плазме (пг/мл: 10.5±2.1), процент CD8+CD57+KLRG1+ сенесцентных T-клеток (25±5%)\n",
            "   🏥 Клиническая валидация: Клиническая когорта: 50 человек в возрасте 60-80 лет, методы: qPCR, ELISA, биомаркеры: уровень COA1δ мРНК в крови, уровень IL-6 в плазме\n",
            "   💊 Терапевтическая мишень: Увеличение экспрессии COA1δ и улучшение функции митохондрий\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "3. ГИПОТЕЗА:\n",
            "   🧬 Термины: coa1δ mitochondria, DNMT3A, H4K16ac\n",
            "   🔬 Механизм: Гиперметилирование CpG-сайта cg09809672 в гене COA1δ, обусловленное увеличением активности DNMT3A и снижением гистоновой метки H4K16ac, приводит к снижению экспрессии COA1δ и ухудшению функции митохондрий в стареющих клетках\n",
            "   📚 Обоснование: Увеличение активности DNMT3A и снижение гистоновой метки H4K16ac могут привести к гиперметилированию CpG-сайта cg09809672, что снижает экспрессию COA1δ и ухудшает функцию митохондрий\n",
            "   🧬 Эпигенетика: Гиперметилирование CpG-сайта cg09809672, увеличение активности DNMT3A, снижение гистоновой метки H4K16ac\n",
            "   🔥 Воспаление: Увеличение уровня IL-1β до 5.2±1.1 пг/мл и TNF-α до 3.5±0.7 пг/мл в плазме\n",
            "   🛡️  Иммунитет: Снижение CD4+CD25+FOXP3+ Tregs до 5±2% от CD4+\n",
            "   🧪 Эксперимент: In vitro модель: первичные фибробласты человека, методы: qPCR, Western blot, ELISA, временные точки: 24, 48, 72 часа\n",
            "   📊 Биомаркеры: Уровень COA1δ мРНК в фибробластах (fold change: 0.5±0.1), уровень IL-1β в плазме (пг/мл: 5.2±1.1), процент CD4+CD25+FOXP3+ Tregs (5±2%)\n",
            "   🏥 Клиническая валидация: Клиническая когорта: 100 человек в возрасте 60-80 лет, методы: qPCR, ELISA, биомаркеры: уровень COA1δ мРНК в крови, уровень IL-1β в плазме\n",
            "   💊 Терапевтическая мишень: Снижение экспрессии COA1δ и ухудшение функции митохондрий\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "4. ГИПОТЕЗА:\n",
            "   🧬 Термины: coa1δ mitochondria, TET1, H3K9me3\n",
            "   🔬 Механизм: Гипометилирование CpG-сайта cg05575921 в гене COA1δ, обусловленное увеличением активности TET1 и снижением гистоновой метки H3K9me3, приводит к увеличению экспрессии COA1δ и улучшению функции митохондрий в стареющих клетках\n",
            "   📚 Обоснование: Увеличение активности TET1 и снижение гистоновой метки H3K9me3 могут привести к гипометилированию CpG-сайта cg05575921, что увеличивает экспрессию COA1δ и улучшает функцию митохондрий\n",
            "   🧬 Эпигенетика: Гипометилирование CpG-сайта cg05575921, увеличение активности TET1, снижение гистоновой метки H3K9me3\n",
            "   🔥 Воспаление: Снижение уровня IL-6 до 8.5±1.9 пг/мл и TNF-α до 4.2±0.9 пг/мл в плазме\n",
            "   🛡️  Иммунитет: Увеличение CD8+CD57+KLRG1+ сенесцентных T-клеток до 20±4% от CD8+\n",
            "   🧪 Эксперимент: In vivo модель: мыши C57BL/6, методы: qPCR, Western blot, ELISA, временные точки: 1, 3, 6 месяцев\n",
            "   📊 Биомаркеры: Уровень COA1δ мРНК в мышцах (fold change: 2.8±0.5), уровень IL-6 в плазме (пг/мл: 8.5±1.9), процент CD8+CD57+KLRG1+ сенесцентных T-клеток (20±4%)\n",
            "   🏥 Клиническая валидация: Клиническая когорта: 50 человек в возрасте 60-80 лет, методы: qPCR, ELISA, биомаркеры: уровень COA1δ мРНК в крови, уровень IL-6 в плазме\n",
            "   💊 Терапевтическая мишень: Увеличение экспрессии COA1δ и улучшение функции митохондрий\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "5. ГИПОТЕЗА:\n",
            "   🧬 Термины: coa1δ mitochondria, DNMT3B, H3K4me3\n",
            "   🔬 Механизм: Гиперметилирование CpG-сайта cg09809672 в гене COA1δ, обусловленное увеличением активности DNMT3B и снижением гистоновой метки H3K4me3, приводит к снижению экспрессии COA1δ и ухудшению функции митохондрий в стареющих клетках\n",
            "   📚 Обоснование: Увеличение активности DNMT3B и снижение гистоновой метки H3K4me3 могут привести к гиперметилированию CpG-сайта cg09809672, что снижает экспрессию COA1δ и ухудшает функцию митохондрий\n",
            "   🧬 Эпигенетика: Гиперметилирование CpG-сайта cg09809672, увеличение активности DNMT3B, снижение гистоновой метки H3K4me3\n",
            "   🔥 Воспаление: Увеличение уровня IL-1β до 6.3±1.3 пг/мл и TNF-α до 4.5±0.9 пг/мл в плазме\n",
            "   🛡️  Иммунитет: Снижение CD4+CD25+FOXP3+ Tregs до 10±3% от CD4+\n",
            "   🧪 Эксперимент: In vitro модель: первичные фибробласты человека, методы: qPCR, Western blot, ELISA, временные точки: 24, 48, 72 часа\n",
            "   📊 Биомаркеры: Уровень COA1δ мРНК в фибробластах (fold change: 0.8±0.2), уровень IL-1β в плазме (пг/мл: 6.3±1.3), процент CD4+CD25+FOXP3+ Tregs (10±3%)\n",
            "   🏥 Клиническая валидация: Клиническая когорта: 100 человек в возрасте 60-80 лет, методы: qPCR, ELISA, биомаркеры: уровень COA1δ мРНК в крови, уровень IL-1β в плазме\n",
            "   💊 Терапевтическая мишень: Снижение экспрессии COA1δ и ухудшение функции митохондрий\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬\n",
            "СПЕЦИАЛИЗИРОВАННЫЙ АНАЛИЗ: COA1Δ MITOCHONDRIA\n",
            "🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬\n",
            "Выберите действие:\n",
            "1. 🔍 Анализ специализированных связей\n",
            "2. 💡 Генерация клинических гипотез\n",
            "3. 📚 Показать метаданные и DOI\n",
            "4. 🔄 Выбрать новый термин\n",
            "5. ❌ Выход\n",
            "\n",
            "👆 Выберите действие (1-5): 2\n",
            "⏳ Генерация конкретных клинических гипотез...\n",
            "🌡️ Выберите температуру (0.1-1.0) [по умолчанию 0.75]: \n",
            "🌡️ Используем температуру: 0.75\n",
            "💡 Совет: 0.1-0.3=консервативно, 0.4-0.7=сбалансированно, 0.8-1.0=креативно\n",
            "\n",
            "🔍 АНАЛИЗ СПЕЦИАЛИЗИРОВАННЫХ СВЯЗЕЙ: coa1δ mitochondria\n",
            "------------------------------------------------------------\n",
            "🔍 Инициализация для графа: 32,707 узлов, 775,715 рёбер\n",
            "📋 Извлечено 68 уникальных ключевых слов\n",
            "🚀 Построение индексов...\n",
            "🔍 Поиск специализированных терминов по вашим ключевым словам...\n",
            "📚 Построение индекса DOI...\n",
            "🔍 Примеры метаданных для отладки:\n",
            "   muscle biopsies: <class 'dict'>\n",
            "      ключи: ['dois', 'years', 'titles', 'authors', 'keywords']\n",
            "      dois: ['https://doi.org/10.1186/s13072-025-00601-w', nan]\n",
            "   human myoblast: <class 'dict'>\n",
            "      ключи: ['dois', 'years', 'titles', 'authors', 'keywords']\n",
            "      dois: ['https://doi.org/10.1186/s13072-025-00601-w', 'https://doi.org/10.3390/ijms241713181']\n",
            "   tet1-2: <class 'dict'>\n",
            "      ключи: ['dois', 'years', 'titles', 'authors', 'keywords']\n",
            "      dois: ['https://doi.org/10.1186/s13072-025-00601-w']\n",
            "✅ Найдено 23286 узлов с DOI/PMID\n",
            "🔍 Примеры найденных DOI:\n",
            "   muscle biopsies: https://doi.org/10.1186/s13072-025-00601-w\n",
            "   human myoblast: https://doi.org/10.1186/s13072-025-00601-w\n",
            "   tet1-2: https://doi.org/10.1186/s13072-025-00601-w\n",
            "   human cancers [: https://doi.org/10.1186/s13100-024-00339-4\n",
            "   tet2: https://doi.org/10.1038/s42255-023-00855-2\n",
            "✅ Найдено 276 специализированных терминов\n",
            "🔧 Построение специализированных категорий с расширенными ключевыми словами...\n",
            "🎯 Минимальный порог специализированных соседей: 1\n",
            "\n",
            "🔍 ДИАГНОСТИКА С РАСШИРЕННЫМИ КЛЮЧЕВЫМИ СЛОВАМИ:\n",
            "    1. human hopx genome.blue arrowhead    ✅ (всего: 1, спец: 0)\n",
            "    2. human coa1                          ✅ (всего: 3, спец: 0)\n",
            "    3. human col18a1 promoter 2            ✅ (всего: 3, спец: 0)\n",
            "    4. human hopx proteins                 ✅ (всего: 3, спец: 0)\n",
            "    5. anti-human emilin1                  ✅ (всего: 6, спец: 2)\n",
            "    6. high hopx methylation               ✅ (всего: 1, спец: 0)\n",
            "    7. gdf15 human                         ✅ (всего: 6, спец: 0)\n",
            "    8. anti‐human hmgb1 human              ✅ (всего: 6, спец: 0)\n",
            "    9. human hopx protein                  ✅ (всего: 5, спец: 0)\n",
            "   10. human emilin1                       ✅ (всего: 7, спец: 2)\n",
            "   11. human hopx                          ✅ (всего: 7, спец: 0)\n",
            "   12. human pro-longevity                 ✅ (всего: 1, спец: 0)\n",
            "   13. human aging.skin                    ✅ (всего: 1, спец: 0)\n",
            "   14. human col18a1                       ✅ (всего: 8, спец: 0)\n",
            "   15. human longevity.23                  ✅ (всего: 2, спец: 0)\n",
            "   16. human longevity-specific            ✅ (всего: 2, спец: 0)\n",
            "   17. human senescence-associated         ✅ (всего: 1, спец: 0)\n",
            "   18. human senescence-associated β-galactosidase ✅ (всего: 1, спец: 0)\n",
            "   19. human cellular senescence-associated ✅ (всего: 1, спец: 0)\n",
            "   20. coa1/c7orf44/mitrac15               ❌ (всего: 1, спец: 0)\n",
            "\n",
            "📊 СТАТИСТИКА С РАСШИРЕННЫМИ КРИТЕРИЯМИ:\n",
            "   Терминов соответствует расширенным критериям: 178\n",
            "\n",
            "🚫 Отфильтровано 151 из 178 терминов\n",
            "✅ В категории добавлено 26 качественных терминов\n",
            "\n",
            "📊 СТАТИСТИКА ПО КАТЕГОРИЯМ (с расширенными ключевыми словами):\n",
            "   🧬 Биомаркеры старения человека: 15 терминов (средняя степень: 5.0)\n",
            "   🔬 Эпигенетические часы: 0 терминов ⚠️\n",
            "   ⚡ Воспаление при старении: 0 терминов ⚠️\n",
            "   🧪 Гены и белки старения: 1 терминов (средняя степень: 8.0)\n",
            "   🏥 Терапевтические мишени: 0 терминов ⚠️\n",
            "   🧠 Иммунное старение: 0 терминов ⚠️\n",
            "   🔀 Пути долголетия: 0 терминов ⚠️\n",
            "   📊 Клеточные процессы: 10 терминов (средняя степень: 2.3)\n",
            "🎯 Выбрано 1 специализированных соседей из 4\n",
            "\n",
            "💡 ГЕНЕРАЦИЯ СПЕЦИАЛИЗИРОВАННЫХ ГИПОТЕЗ\n",
            "🌡️ Температура: 0.75 (0.1=консервативно, 1.0=креативно)\n",
            "------------------------------------------------------------\n",
            "\n",
            "📚 КОНКРЕТНЫЕ ГИПОТЕЗЫ СТАРЕНИЯ ЧЕЛОВЕКА\n",
            "🌡️ Температура генерации: 0.75\n",
            "================================================================================\n",
            "\n",
            "1. ГИПОТЕЗА:\n",
            "   🧬 Термины: coa1δ mitochondria, DNMT1, H3K4me3\n",
            "   🔬 Механизм: Гипометилирование CpG-сайта cg09809672 в промотере COA1δ, обусловленное снижением активности DNMT1 и увеличением гистоновой метки H3K4me3, приводит к повышению экспрессии COA1δ и улучшению функции митохондрий в стареющих клетках\n",
            "   📚 Обоснование: Снижение активности DNMT1 и увеличение гистоновой метки H3K4me3 могут привести к гипометилированию промотора COA1δ, что в свою очередь увеличивает экспрессию COA1δ и улучшает функцию митохондрий\n",
            "   🧬 Эпигенетика: Гипометилирование CpG-сайта cg09809672, снижение активности DNMT1, увеличение гистоновой метки H3K4me3\n",
            "   🔥 Воспаление: Снижение уровня IL-1β до 2.1±0.5 пг/мл и TNF-α до 1.2±0.3 пг/мл в плазме\n",
            "   🛡️  Иммунитет: Увеличение CD4+CD25+FOXP3+ Tregs до 18±4% от CD4+\n",
            "   🧪 Эксперимент: In vitro модель: первичные фибробласты человека, обработанные сенолитиками, qPCR праймеры для COA1δ, Western blot для DNMT1 и H3K4me3\n",
            "   📊 Биомаркеры: Уровень COA1δ мРНК (fold change: 2.5±0.5), уровень DNMT1 белка (нг/мл: 150±20), уровень H3K4me3 (пг/мл: 50±10)\n",
            "   🏥 Клиническая валидация: Когорта: 100 человек в возрасте 65-75 лет, методы: qPCR, Western blot, ELISA, биомаркеры: уровень COA1δ мРНК, уровень DNMT1 белка, уровень H3K4me3\n",
            "   💊 Терапевтическая мишень: COA1δ, DNMT1, H3K4me3\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "2. ГИПОТЕЗА:\n",
            "   🧬 Термины: coa1δ mitochondria, TET2, H3K27me3\n",
            "   🔬 Механизм: Гипометилирование CpG-сайта cg05575921 в промотере COA1δ, обусловленное увеличением активности TET2 и снижением гистоновой метки H3K27me3, приводит к повышению экспрессии COA1δ и улучшению функции митохондрий в стареющих клетках\n",
            "   📚 Обоснование: Увеличение активности TET2 и снижение гистоновой метки H3K27me3 могут привести к гипометилированию промотора COA1δ, что в свою очередь увеличивает экспрессию COA1δ и улучшает функцию митохондрий\n",
            "   🧬 Эпигенетика: Гипометилирование CpG-сайта cg05575921, увеличение активности TET2, снижение гистоновой метки H3K27me3\n",
            "   🔥 Воспаление: Снижение уровня IL-6 до 10.5±2.5 пг/мл и TNF-α до 5.5±1.5 пг/мл в плазме\n",
            "   🛡️  Иммунитет: Увеличение CD8+CD57+KLRG1+ сенесцентных T-клеток до 25±5% от CD8+\n",
            "   🧪 Эксперимент: In vivo модель: мыши C57BL/6, обработанные сенолитиками, qPCR праймеры для COA1δ, Western blot для TET2 и H3K27me3\n",
            "   📊 Биомаркеры: Уровень COA1δ мРНК (fold change: 3.2±0.6), уровень TET2 белка (нг/мл: 200±30), уровень H3K27me3 (пг/мл: 30±5)\n",
            "   🏥 Клиническая валидация: Когорта: 50 человек в возрасте 60-70 лет, методы: qPCR, Western blot, ELISA, биомаркеры: уровень COA1δ мРНК, уровень TET2 белка, уровень H3K27me3\n",
            "   💊 Терапевтическая мишень: COA1δ, TET2, H3K27me3\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "3. ГИПОТЕЗА:\n",
            "   🧬 Термины: coa1δ mitochondria, NF-κB p65, IL-1β\n",
            "   🔬 Механизм: Активация NF-κB p65 и увеличение уровня IL-1β приводит к снижению экспрессии COA1δ и ухудшению функции митохондрий в стареющих клетках\n",
            "   📚 Обоснование: Активация NF-κB p65 и увеличение уровня IL-1β могут привести к снижению экспрессии COA1δ и ухудшению функции митохондрий\n",
            "   🧬 Эпигенетика: Гиперметилирование CpG-сайта cg09809672, снижение активности DNMT1\n",
            "   🔥 Воспаление: Увеличение уровня IL-1β до 15.2±3.1 пг/мл и TNF-α до 8.7±2.3 пг/мл в плазме\n",
            "   🛡️  Иммунитет: Увеличение CD4+CD28- сенесцентных T-клеток до 20±4% от CD4+\n",
            "   🧪 Эксперимент: In vitro модель: первичные фибробласты человека, обработанные ЛПС, qPCR праймеры для COA1δ, Western blot для NF-κB p65 и IL-1β\n",
            "   📊 Биомаркеры: Уровень COA1δ мРНК (fold change: 0.5±0.1), уровень NF-κB p65 белка (нг/мл: 100±20), уровень IL-1β (пг/мл: 20±5)\n",
            "   🏥 Клиническая валидация: Когорта: 100 человек в возрасте 65-75 лет, методы: qPCR, Western blot, ELISA, биомаркеры: уровень COA1δ мРНК, уровень NF-κB p65 белка, уровень IL-1β\n",
            "   💊 Терапевтическая мишень: COA1δ, NF-κB p65, IL-1β\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "4. ГИПОТЕЗА:\n",
            "   🧬 Термины: coa1δ mitochondria, STAT3, IL-6\n",
            "   🔬 Механизм: Активация STAT3 и увеличение уровня IL-6 приводит к снижению экспрессии COA1δ и ухудшению функции митохондрий в стареющих клетках\n",
            "   📚 Обоснование: Активация STAT3 и увеличение уровня IL-6 могут привести к снижению экспрессии COA1δ и ухудшению функции митохондрий\n",
            "   🧬 Эпигенетика: Гиперметилирование CpG-сайта cg05575921, снижение активности TET2\n",
            "   🔥 Воспаление: Увеличение уровня IL-6 до 25.5±5.5 пг/мл и TNF-α до 12.5±3.5 пг/мл в плазме\n",
            "   🛡️  Иммунитет: Увеличение CD8+CD57+KLRG1+ сенесцентных T-клеток до 28±6% от CD8+\n",
            "   🧪 Эксперимент: In vivo модель: мыши C57BL/6, обработанные ЛПС, qPCR праймеры для COA1δ, Western blot для STAT3 и IL-6\n",
            "   📊 Биомаркеры: Уровень COA1δ мРНК (fold change: 0.6±0.1), уровень STAT3 белка (нг/мл: 150±30), уровень IL-6 (пг/мл: 30±5)\n",
            "   🏥 Клиническая валидация: Когорта: 50 человек в возрасте 60-70 лет, методы: qPCR, Western blot, ELISA, биомаркеры: уровень COA1δ мРНК, уровень STAT3 белка, уровень IL-6\n",
            "   💊 Терапевтическая мишень: COA1δ, STAT3, IL-6\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "5. ГИПОТЕЗА:\n",
            "   🧬 Термины: coa1δ mitochondria, NLRP3, caspase-1\n",
            "   🔬 Механизм: Активация NLRP3 и увеличение уровня caspase-1 приводит к снижению экспрессии COA1δ и ухудшению функции митохондрий в стареющих клетках\n",
            "   📚 Обоснование: Активация NLRP3 и увеличение уровня caspase-1 могут привести к снижению экспрессии COA1δ и ухудшению функции митохондрий\n",
            "   🧬 Эпигенетика: Гиперметилирование CpG-сайта cg09809672, снижение активности DNMT1\n",
            "   🔥 Воспаление: Увеличение уровня IL-1β до 20.2±4.1 пг/мл и TNF-α до 10.7±2.7 пг/мл в плазме\n",
            "   🛡️  Иммунитет: Увеличение CD4+CD28- сенесцентных T-клеток до 22±5% от CD4+\n",
            "   🧪 Эксперимент: In vitro модель: первичные фибробласты человека, обработанные ЛПС, qPCR праймеры для COA1δ, Western blot для NLRP3 и caspase-1\n",
            "   📊 Биомаркеры: Уровень COA1δ мРНК (fold change: 0.4±0.1), уровень NLRP3 белка (нг/мл: 120±25), уровень caspase-1 (пг/мл: 25±5)\n",
            "   🏥 Клиническая валидация: Когорта: 100 человек в возрасте 65-75 лет, методы: qPCR, Western blot, ELISA, биомаркеры: уровень COA1δ мРНК, уровень NLRP3 белка, уровень caspase-1\n",
            "   💊 Терапевтическая мишень: COA1δ, NLRP3, caspase-1\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬\n",
            "СПЕЦИАЛИЗИРОВАННЫЙ АНАЛИЗ: COA1Δ MITOCHONDRIA\n",
            "🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬\n",
            "Выберите действие:\n",
            "1. 🔍 Анализ специализированных связей\n",
            "2. 💡 Генерация клинических гипотез\n",
            "3. 📚 Показать метаданные и DOI\n",
            "4. 🔄 Выбрать новый термин\n",
            "5. ❌ Выход\n",
            "\n",
            "👆 Выберите действие (1-5): 4\n",
            "\n",
            "🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯\n",
            "ВЫБОР СПЕЦИАЛИЗИРОВАННОГО ТЕРМИНА\n",
            "🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯\n",
            "\n",
            "🎯 СПЕЦИАЛИЗИРОВАННЫЕ КАТЕГОРИИ ПО ВАШИМ КЛЮЧЕВЫМ СЛОВАМ\n",
            "================================================================================\n",
            "\n",
            "1. 🧬 Биомаркеры старения человека (15 терминов)\n",
            "   Топ-3 по специализированному скору:\n",
            "   1. anti-human emilin1 (связей: 6, DOI: 📚, скор: 49.0)\n",
            "   2. human emilin1 (связей: 7, DOI: 📚, скор: 47.3)\n",
            "   3. extracellular protein emilin1 (связей: 2, DOI: 📚, скор: 43.0)\n",
            "\n",
            "2. 🔬 Эпигенетические часы (0 терминов)\n",
            "   Термины не найдены\n",
            "\n",
            "3. ⚡ Воспаление при старении (0 терминов)\n",
            "   Термины не найдены\n",
            "\n",
            "4. 🧪 Гены и белки старения (1 терминов)\n",
            "   Топ-3 по специализированному скору:\n",
            "   1. ecm glycoprotein emilin1 (связей: 8, DOI: 📚, скор: 37.0)\n",
            "\n",
            "5. 🏥 Терапевтические мишени (0 терминов)\n",
            "   Термины не найдены\n",
            "\n",
            "6. 🧠 Иммунное старение (0 терминов)\n",
            "   Термины не найдены\n",
            "\n",
            "7. 🔀 Пути долголетия (0 терминов)\n",
            "   Термины не найдены\n",
            "\n",
            "8. 📊 Клеточные процессы (10 терминов)\n",
            "   Топ-3 по специализированному скору:\n",
            "   1. phospho-vimentin+/hopx+ cells (связей: 1, DOI: 📚, скор: 44.0)\n",
            "   2. coa1δ cells (связей: 3, DOI: 📚, скор: 42.0)\n",
            "   3. hopx-derived alveolar cells (связей: 3, DOI: 📚, скор: 42.0)\n",
            "\n",
            "0. ✍️  Ввести термин вручную\n",
            "99. 🎲 Случайный специализированный термин\n",
            "\n",
            "👆 Выберите категорию (1-8), 0 для ручного ввода, 99 для случайного: 0\n",
            "✍️ Введите термин: gdf15 human\n",
            "\n",
            "🔍 ВАЛИДАЦИЯ ТЕРМИНА 'gdf15 human':\n",
            "❌ Термин не рекомендуется для качественного анализа:\n",
            "   ❌ Мало специализированных соседей: 0 < 3\n",
            "\n",
            "📊 ХАРАКТЕРИСТИКИ:\n",
            "   🔗 Всего связей: 6\n",
            "   🎯 Специализированных соседей: 0\n",
            "   🔑 Соответствует ключевым словам: ✅\n",
            "   📚 DOI: ✅\n",
            "   📈 Специализированный скор: 48.2\n",
            "\n",
            "❓ Продолжить с этим термином несмотря на низкое качество? (y/N): y\n",
            "⚠️ Термин принят с предупреждениями\n",
            "\n",
            "📋 ДЕТАЛЬНАЯ ИНФОРМАЦИЯ: gdf15 human\n",
            "============================================================\n",
            "🔗 Степень узла: 6\n",
            "🎯 Специализированный скор: 48.2\n",
            "👥 Соседей: 6\n",
            "📚 DOI: https://doi.org/10.1073/pnas.1918508117\n",
            "🔑 Совпадения с вашими ключевыми словами: gdf15 human\n",
            "📄 Заголовок: CXCL5-mediated recruitment of neutrophils into the peritoneal cavity of Gdf15-deficient mice protect...\n",
            "👨‍🔬 Авторы: Isa Santos, Henrique G. Colaço, Ana Neves-Costa, Elsa Seixas, Tiago R. Velho, Dora Pedroso, André Ba...\n",
            "📅 Год: 2020\n",
            "📊 Диапазон лет: 2020-2020\n",
            "📚 Всего статей: 1\n",
            "🏷️  Ключевые слова источника: GDF15, human...\n",
            "📝 Описание: Biomedical entity from: CXCL5-mediated recruitment of neutrophils into the peritoneal cavity of Gdf15-deficient mice protects against abdominal sepsis...\n",
            "\n",
            "🔗 Топ-5 специализированных соседей:\n",
            "   1. cardiac puncture) (скор: 23.2, связей: 6, DOI: 📚)\n",
            "   2. catalog (скор: 23.2, связей: 6, DOI: 📚)\n",
            "   3. gdf15 (скор: 16.9, связей: 1004, DOI: 📚)\n",
            "   4. mouse blood (скор: 10.5, связей: 19, DOI: 📚)\n",
            "   5. serum (скор: 1.4, связей: 2038, DOI: 📚)\n",
            "\n",
            "🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬\n",
            "СПЕЦИАЛИЗИРОВАННЫЙ АНАЛИЗ: GDF15 HUMAN\n",
            "🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬\n",
            "Выберите действие:\n",
            "1. 🔍 Анализ специализированных связей\n",
            "2. 💡 Генерация клинических гипотез\n",
            "3. 📚 Показать метаданные и DOI\n",
            "4. 🔄 Выбрать новый термин\n",
            "5. ❌ Выход\n",
            "\n",
            "👆 Выберите действие (1-5): 2\n",
            "⏳ Генерация конкретных клинических гипотез...\n",
            "🌡️ Выберите температуру (0.1-1.0) [по умолчанию 0.75]: \n",
            "🌡️ Используем температуру: 0.75\n",
            "💡 Совет: 0.1-0.3=консервативно, 0.4-0.7=сбалансированно, 0.8-1.0=креативно\n",
            "\n",
            "🔍 АНАЛИЗ СПЕЦИАЛИЗИРОВАННЫХ СВЯЗЕЙ: gdf15 human\n",
            "------------------------------------------------------------\n",
            "🔍 Инициализация для графа: 32,707 узлов, 775,715 рёбер\n",
            "📋 Извлечено 68 уникальных ключевых слов\n",
            "🚀 Построение индексов...\n",
            "🔍 Поиск специализированных терминов по вашим ключевым словам...\n",
            "📚 Построение индекса DOI...\n",
            "🔍 Примеры метаданных для отладки:\n",
            "   muscle biopsies: <class 'dict'>\n",
            "      ключи: ['dois', 'years', 'titles', 'authors', 'keywords']\n",
            "      dois: ['https://doi.org/10.1186/s13072-025-00601-w', nan]\n",
            "   human myoblast: <class 'dict'>\n",
            "      ключи: ['dois', 'years', 'titles', 'authors', 'keywords']\n",
            "      dois: ['https://doi.org/10.1186/s13072-025-00601-w', 'https://doi.org/10.3390/ijms241713181']\n",
            "   tet1-2: <class 'dict'>\n",
            "      ключи: ['dois', 'years', 'titles', 'authors', 'keywords']\n",
            "      dois: ['https://doi.org/10.1186/s13072-025-00601-w']\n",
            "✅ Найдено 23286 узлов с DOI/PMID\n",
            "🔍 Примеры найденных DOI:\n",
            "   muscle biopsies: https://doi.org/10.1186/s13072-025-00601-w\n",
            "   human myoblast: https://doi.org/10.1186/s13072-025-00601-w\n",
            "   tet1-2: https://doi.org/10.1186/s13072-025-00601-w\n",
            "   human cancers [: https://doi.org/10.1186/s13100-024-00339-4\n",
            "   tet2: https://doi.org/10.1038/s42255-023-00855-2\n",
            "✅ Найдено 276 специализированных терминов\n",
            "🔧 Построение специализированных категорий с расширенными ключевыми словами...\n",
            "🎯 Минимальный порог специализированных соседей: 1\n",
            "\n",
            "🔍 ДИАГНОСТИКА С РАСШИРЕННЫМИ КЛЮЧЕВЫМИ СЛОВАМИ:\n",
            "    1. human hopx genome.blue arrowhead    ✅ (всего: 1, спец: 0)\n",
            "    2. human coa1                          ✅ (всего: 3, спец: 0)\n",
            "    3. human col18a1 promoter 2            ✅ (всего: 3, спец: 0)\n",
            "    4. human hopx proteins                 ✅ (всего: 3, спец: 0)\n",
            "    5. anti-human emilin1                  ✅ (всего: 6, спец: 2)\n",
            "    6. high hopx methylation               ✅ (всего: 1, спец: 0)\n",
            "    7. gdf15 human                         ✅ (всего: 6, спец: 0)\n",
            "    8. anti‐human hmgb1 human              ✅ (всего: 6, спец: 0)\n",
            "    9. human hopx protein                  ✅ (всего: 5, спец: 0)\n",
            "   10. human emilin1                       ✅ (всего: 7, спец: 2)\n",
            "   11. human hopx                          ✅ (всего: 7, спец: 0)\n",
            "   12. human pro-longevity                 ✅ (всего: 1, спец: 0)\n",
            "   13. human aging.skin                    ✅ (всего: 1, спец: 0)\n",
            "   14. human col18a1                       ✅ (всего: 8, спец: 0)\n",
            "   15. human longevity.23                  ✅ (всего: 2, спец: 0)\n",
            "   16. human longevity-specific            ✅ (всего: 2, спец: 0)\n",
            "   17. human senescence-associated         ✅ (всего: 1, спец: 0)\n",
            "   18. human senescence-associated β-galactosidase ✅ (всего: 1, спец: 0)\n",
            "   19. human cellular senescence-associated ✅ (всего: 1, спец: 0)\n",
            "   20. coa1/c7orf44/mitrac15               ❌ (всего: 1, спец: 0)\n",
            "\n",
            "📊 СТАТИСТИКА С РАСШИРЕННЫМИ КРИТЕРИЯМИ:\n",
            "   Терминов соответствует расширенным критериям: 178\n",
            "\n",
            "🚫 Отфильтровано 151 из 178 терминов\n",
            "✅ В категории добавлено 26 качественных терминов\n",
            "\n",
            "📊 СТАТИСТИКА ПО КАТЕГОРИЯМ (с расширенными ключевыми словами):\n",
            "   🧬 Биомаркеры старения человека: 15 терминов (средняя степень: 5.0)\n",
            "   🔬 Эпигенетические часы: 0 терминов ⚠️\n",
            "   ⚡ Воспаление при старении: 0 терминов ⚠️\n",
            "   🧪 Гены и белки старения: 1 терминов (средняя степень: 8.0)\n",
            "   🏥 Терапевтические мишени: 0 терминов ⚠️\n",
            "   🧠 Иммунное старение: 0 терминов ⚠️\n",
            "   🔀 Пути долголетия: 0 терминов ⚠️\n",
            "   📊 Клеточные процессы: 10 терминов (средняя степень: 2.3)\n",
            "🎯 Выбрано 0 специализированных соседей из 6\n",
            "\n",
            "💡 ГЕНЕРАЦИЯ СПЕЦИАЛИЗИРОВАННЫХ ГИПОТЕЗ\n",
            "🌡️ Температура: 0.75 (0.1=консервативно, 1.0=креативно)\n",
            "------------------------------------------------------------\n",
            "\n",
            "📚 КОНКРЕТНЫЕ ГИПОТЕЗЫ СТАРЕНИЯ ЧЕЛОВЕКА\n",
            "🌡️ Температура генерации: 0.75\n",
            "================================================================================\n",
            "\n",
            "1. ГИПОТЕЗА:\n",
            "   🧬 Термины: gdf15 human, эпигенетические изменения, воспаление\n",
            "   🔬 Механизм: Увеличение экспрессии GDF15 в стареющих клетках человека происходит за счет гиперметилирования CpG-сайта cg09809672 в промотере GDF15 и активации NF-κB p65, что приводит к повышению уровня IL-6 до 15.2±3.1 пг/мл и TNF-α до 8.7±2.3 пг/мл в плазме\n",
            "   📚 Обоснование: GDF15 является ключевым регулятором старения, и его экспрессия увеличивается с возрастом. Эпигенетические изменения, такие как метилирование ДНК, могут влиять на экспрессию GDF15. Воспаление, индуцированное NF-κB p65, также может стимулировать экспрессию GDF15\n",
            "   🧬 Эпигенетика: Гиперметилирование CpG-сайта cg09809672 в промотере GDF15, увеличение экспрессии DNMT1 и DNMT3B, снижение экспрессии TET1 и TET2\n",
            "   🔥 Воспаление: Повышение IL-6 до 15.2±3.1 пг/мл и TNF-α до 8.7±2.3 пг/мл в плазме, активация NF-κB p65, увеличение экспрессии NLRP3 и ASC\n",
            "   🛡️  Иммунитет: Увеличение CD8+CD57+KLRG1+ сенесцентных T-клеток до 23±5% от CD8+, снижение экспрессии CD28 на CD4+ T-клетках\n",
            "   🧪 Эксперимент: In vitro модели: первичные фибробласты человека, HUVEC, PBMC. In vivo модели: мыши C57BL/6. Клинические когорты: возраст участников 65-80 лет, размер выборки 100 человек. Конкретные методы: qPCR праймеры для GDF15, антитела для Western blot против NF-κB p65, ELISA киты для IL-6 и TNF-α\n",
            "   📊 Биомаркеры: Уровень GDF15 в плазме (нг/мл), метилирование CpG-сайта cg09809672 (%), уровень IL-6 и TNF-α в плазме (пг/мл), количество CD8+CD57+KLRG1+ сенесцентных T-клеток (% от CD8+)\n",
            "   🏥 Клиническая валидация: Клиническая когорта: возраст участников 65-80 лет, размер выборки 100 человек. Методы: qPCR, Western blot, ELISA, ФACS-анализ. Биомаркеры: уровень GDF15, метилирование CpG-сайта cg09809672, уровень IL-6 и TNF-α, количество CD8+CD57+KLRG1+ сенесцентных T-клеток\n",
            "   💊 Терапевтическая мишень: GDF15, NF-κB p65, DNMT1 и DNMT3B\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "2. ГИПОТЕЗА:\n",
            "   🧬 Термины: gdf15 human, иммунный ответ, воспаление\n",
            "   🔬 Механизм: Увеличение экспрессии GDF15 в стареющих клетках человека приводит к активации иммунного ответа и воспалению, что характеризуется увеличением количества CD4+CD25+FOXP3+ Tregs до 12±3% от CD4+ и повышением уровня IL-10 до 10.5±2.5 пг/мл в плазме\n",
            "   📚 Обоснование: GDF15 может влиять на иммунный ответ и воспаление, и его экспрессия увеличивается с возрастом. Иммунный ответ, индуцированный GDF15, может привести к активации Tregs и производству противовоспалительных цитокинов\n",
            "   🧬 Эпигенетика: Гипометилирование CpG-сайта cg05575921 в промотере GDF15, увеличение экспрессии TET1 и TET2, снижение экспрессии DNMT1 и DNMT3B\n",
            "   🔥 Воспаление: Повышение IL-10 до 10.5±2.5 пг/мл в плазме, снижение экспрессии NLRP3 и ASC, активация STAT3\n",
            "   🛡️  Иммунитет: Увеличение CD4+CD25+FOXP3+ Tregs до 12±3% от CD4+, снижение экспрессии CD38 на CD8+ T-клетках\n",
            "   🧪 Эксперимент: In vitro модели: первичные фибробласты человека, HUVEC, PBMC. In vivo модели: мыши C57BL/6. Клинические когорты: возраст участников 65-80 лет, размер выборки 100 человек. Конкретные методы: qPCR праймеры для GDF15, антитела для Western blot против FOXP3, ELISA киты для IL-10\n",
            "   📊 Биомаркеры: Уровень GDF15 в плазме (нг/мл), метилирование CpG-сайта cg05575921 (%), уровень IL-10 в плазме (пг/мл), количество CD4+CD25+FOXP3+ Tregs (% от CD4+)\n",
            "   🏥 Клиническая валидация: Клиническая когорта: возраст участников 65-80 лет, размер выборки 100 человек. Методы: qPCR, Western blot, ELISA, ФACS-анализ. Биомаркеры: уровень GDF15, метилирование CpG-сайта cg05575921, уровень IL-10, количество CD4+CD25+FOXP3+ Tregs\n",
            "   💊 Терапевтическая мишень: GDF15, FOXP3, TET1 и TET2\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "3. ГИПОТЕЗА:\n",
            "   🧬 Термины: gdf15 human, эпигенетические изменения, сенесценция\n",
            "   🔬 Механизм: Увеличение экспрессии GDF15 в стареющих клетках человека происходит за счет гиперметилирования CpG-сайта cg09809672 в промотере GDF15 и активации п53, что приводит к индукции сенесценции и увеличению количества SA-β-Gal+ клеток до 30±5% от общего количества клеток\n",
            "   📚 Обоснование: GDF15 является ключевым регулятором старения, и его экспрессия увеличивается с возрастом. Эпигенетические изменения, такие как метилирование ДНК, могут влиять на экспрессию GDF15. Сенесценция, индуцированная GDF15, может привести к активации п53 и производству сенесцентных клеток\n",
            "   🧬 Эпигенетика: Гиперметилирование CpG-сайта cg09809672 в промотере GDF15, увеличение экспрессии DNMT1 и DNMT3B, снижение экспрессии TET1 и TET2\n",
            "   🔥 Воспаление: Повышение IL-1β до 12.1±2.9 пг/мл в плазме, активация NF-κB p65, увеличение экспрессии NLRP3 и ASC\n",
            "   🛡️  Иммунитет: Увеличение CD8+CD57+KLRG1+ сенесцентных T-клеток до 25±5% от CD8+, снижение экспрессии CD28 на CD4+ T-клетках\n",
            "   🧪 Эксперимент: In vitro модели: первичные фибробласты человека, HUVEC, PBMC. In vivo модели: мыши C57BL/6. Клинические когорты: возраст участников 65-80 лет, размер выборки 100 человек. Конкретные методы: qPCR праймеры для GDF15, антитела для Western blot против п53, ELISA киты для IL-1β\n",
            "   📊 Биомаркеры: Уровень GDF15 в плазме (нг/мл), метилирование CpG-сайта cg09809672 (%), уровень IL-1β в плазме (пг/мл), количество SA-β-Gal+ клеток (% от общего количества клеток)\n",
            "   🏥 Клиническая валидация: Клиническая когорта: возраст участников 65-80 лет, размер выборки 100 человек. Методы: qPCR, Western blot, ELISA, ФACS-анализ. Биомаркеры: уровень GDF15, метилирование CpG-сайта cg09809672, уровень IL-1β, количество SA-β-Gal+ клеток\n",
            "   💊 Терапевтическая мишень: GDF15, п53, DNMT1 и DNMT3B\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "4. ГИПОТЕЗА:\n",
            "   🧬 Термины: gdf15 human, иммунный ответ, воспаление\n",
            "   🔬 Механизм: Увеличение экспрессии GDF15 в стареющих клетках человека приводит к активации иммунного ответа и воспалению, что характеризуется увеличением количества CD4+CD25+FOXP3+ Tregs до 15±3% от CD4+ и повышением уровня IL-10 до 12.5±2.5 пг/мл в плазме\n",
            "   📚 Обоснование: GDF15 может влиять на иммунный ответ и воспаление, и его экспрессия увеличивается с возрастом. Иммунный ответ, индуцированный GDF15, может привести к активации Tregs и производству противовоспалительных цитокинов\n",
            "   🧬 Эпигенетика: Гипометилирование CpG-сайта cg05575921 в промотере GDF15, увеличение экспрессии TET1 и TET2, снижение экспрессии DNMT1 и DNMT3B\n",
            "   🔥 Воспаление: Повышение IL-10 до 12.5±2.5 пг/мл в плазме, снижение экспрессии NLRP3 и ASC, активация STAT3\n",
            "   🛡️  Иммунитет: Увеличение CD4+CD25+FOXP3+ Tregs до 15±3% от CD4+, снижение экспрессии CD38 на CD8+ T-клетках\n",
            "   🧪 Эксперимент: In vitro модели: первичные фибробласты человека, HUVEC, PBMC. In vivo модели: мыши C57BL/6. Клинические когорты: возраст участников 65-80 лет, размер выборки 100 человек. Конкретные методы: qPCR праймеры для GDF15, антитела для Western blot против FOXP3, ELISA киты для IL-10\n",
            "   📊 Биомаркеры: Уровень GDF15 в плазме (нг/мл), метилирование CpG-сайта cg05575921 (%), уровень IL-10 в плазме (пг/мл), количество CD4+CD25+FOXP3+ Tregs (% от CD4+)\n",
            "   🏥 Клиническая валидация: Клиническая когорта: возраст участников 65-80 лет, размер выборки 100 человек. Методы: qPCR, Western blot, ELISA, ФACS-анализ. Биомаркеры: уровень GDF15, метилирование CpG-сайта cg05575921, уровень IL-10, количество CD4+CD25+FOXP3+ Tregs\n",
            "   💊 Терапевтическая мишень: GDF15, FOXP3, TET1 и TET2\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "5. ГИПОТЕЗА:\n",
            "   🧬 Термины: gdf15 human, эпигенетические изменения, сенесценция\n",
            "   🔬 Механизм: Увеличение экспрессии GDF15 в стареющих клетках человека происходит за счет гиперметилирования CpG-сайта cg09809672 в промотере GDF15 и активации п53, что приводит к индукции сенесценции и увеличению количества SA-β-Gal+ клеток до 35±5% от общего количества клеток\n",
            "   📚 Обоснование: GDF15 является ключевым регулятором старения, и его экспрессия увеличивается с возрастом. Эпигенетические изменения, такие как метилирование ДНК, могут влиять на экспрессию GDF15. Сенесценция, индуцированная GDF15, может привести к активации п53 и производству сенесцентных клеток\n",
            "   🧬 Эпигенетика: Гиперметилирование CpG-сайта cg09809672 в промотере GDF15, увеличение экспрессии DNMT1 и DNMT3B, снижение экспрессии TET1 и TET2\n",
            "   🔥 Воспаление: Повышение IL-1β до 15.1±3.1 пг/мл в плазме, активация NF-κB p65, увеличение экспрессии NLRP3 и ASC\n",
            "   🛡️  Иммунитет: Увеличение CD8+CD57+KLRG1+ сенесцентных T-клеток до 28±5% от CD8+, снижение экспрессии CD28 на CD4+ T-клетках\n",
            "   🧪 Эксперимент: In vitro модели: первичные фибробласты человека, HUVEC, PBMC. In vivo модели: мыши C57BL/6. Клинические когорты: возраст участников 65-80 лет, размер выборки 100 человек. Конкретные методы: qPCR праймеры для GDF15, антитела для Western blot против п53, ELISA киты для IL-1β\n",
            "   📊 Биомаркеры: Уровень GDF15 в плазме (нг/мл), метилирование CpG-сайта cg09809672 (%), уровень IL-1β в плазме (пг/мл), количество SA-β-Gal+ клеток (% от общего количества клеток)\n",
            "   🏥 Клиническая валидация: Клиническая когорта: возраст участников 65-80 лет, размер выборки 100 человек. Методы: qPCR, Western blot, ELISA, ФACS-анализ. Биомаркеры: уровень GDF15, метилирование CpG-сайта cg09809672, уровень IL-1β, количество SA-β-Gal+ клеток\n",
            "   💊 Терапевтическая мишень: GDF15, п53, DNMT1 и DNMT3B\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬\n",
            "СПЕЦИАЛИЗИРОВАННЫЙ АНАЛИЗ: GDF15 HUMAN\n",
            "🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬\n",
            "Выберите действие:\n",
            "1. 🔍 Анализ специализированных связей\n",
            "2. 💡 Генерация клинических гипотез\n",
            "3. 📚 Показать метаданные и DOI\n",
            "4. 🔄 Выбрать новый термин\n",
            "5. ❌ Выход\n",
            "\n",
            "👆 Выберите действие (1-5): 4\n",
            "\n",
            "🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯\n",
            "ВЫБОР СПЕЦИАЛИЗИРОВАННОГО ТЕРМИНА\n",
            "🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯\n",
            "\n",
            "🎯 СПЕЦИАЛИЗИРОВАННЫЕ КАТЕГОРИИ ПО ВАШИМ КЛЮЧЕВЫМ СЛОВАМ\n",
            "================================================================================\n",
            "\n",
            "1. 🧬 Биомаркеры старения человека (15 терминов)\n",
            "   Топ-3 по специализированному скору:\n",
            "   1. anti-human emilin1 (связей: 6, DOI: 📚, скор: 49.0)\n",
            "   2. human emilin1 (связей: 7, DOI: 📚, скор: 47.3)\n",
            "   3. extracellular protein emilin1 (связей: 2, DOI: 📚, скор: 43.0)\n",
            "\n",
            "2. 🔬 Эпигенетические часы (0 терминов)\n",
            "   Термины не найдены\n",
            "\n",
            "3. ⚡ Воспаление при старении (0 терминов)\n",
            "   Термины не найдены\n",
            "\n",
            "4. 🧪 Гены и белки старения (1 терминов)\n",
            "   Топ-3 по специализированному скору:\n",
            "   1. ecm glycoprotein emilin1 (связей: 8, DOI: 📚, скор: 37.0)\n",
            "\n",
            "5. 🏥 Терапевтические мишени (0 терминов)\n",
            "   Термины не найдены\n",
            "\n",
            "6. 🧠 Иммунное старение (0 терминов)\n",
            "   Термины не найдены\n",
            "\n",
            "7. 🔀 Пути долголетия (0 терминов)\n",
            "   Термины не найдены\n",
            "\n",
            "8. 📊 Клеточные процессы (10 терминов)\n",
            "   Топ-3 по специализированному скору:\n",
            "   1. phospho-vimentin+/hopx+ cells (связей: 1, DOI: 📚, скор: 44.0)\n",
            "   2. coa1δ cells (связей: 3, DOI: 📚, скор: 42.0)\n",
            "   3. hopx-derived alveolar cells (связей: 3, DOI: 📚, скор: 42.0)\n",
            "\n",
            "0. ✍️  Ввести термин вручную\n",
            "99. 🎲 Случайный специализированный термин\n",
            "\n",
            "👆 Выберите категорию (1-8), 0 для ручного ввода, 99 для случайного: 0\n",
            "✍️ Введите термин: human longevity\n",
            "\n",
            "🔍 ВАЛИДАЦИЯ ТЕРМИНА 'human longevity':\n",
            "Термин 'human longevity' не найден в графе\n",
            "\n",
            "❓ Продолжить с этим термином несмотря на низкое качество? (y/N): human longevity.23\n",
            "❌ Выбор термина отменен\n",
            "\n",
            "🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯\n",
            "ВЫБОР СПЕЦИАЛИЗИРОВАННОГО ТЕРМИНА\n",
            "🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯\n",
            "\n",
            "🎯 СПЕЦИАЛИЗИРОВАННЫЕ КАТЕГОРИИ ПО ВАШИМ КЛЮЧЕВЫМ СЛОВАМ\n",
            "================================================================================\n",
            "\n",
            "1. 🧬 Биомаркеры старения человека (15 терминов)\n",
            "   Топ-3 по специализированному скору:\n",
            "   1. anti-human emilin1 (связей: 6, DOI: 📚, скор: 49.0)\n",
            "   2. human emilin1 (связей: 7, DOI: 📚, скор: 47.3)\n",
            "   3. extracellular protein emilin1 (связей: 2, DOI: 📚, скор: 43.0)\n",
            "\n",
            "2. 🔬 Эпигенетические часы (0 терминов)\n",
            "   Термины не найдены\n",
            "\n",
            "3. ⚡ Воспаление при старении (0 терминов)\n",
            "   Термины не найдены\n",
            "\n",
            "4. 🧪 Гены и белки старения (1 терминов)\n",
            "   Топ-3 по специализированному скору:\n",
            "   1. ecm glycoprotein emilin1 (связей: 8, DOI: 📚, скор: 37.0)\n",
            "\n",
            "5. 🏥 Терапевтические мишени (0 терминов)\n",
            "   Термины не найдены\n",
            "\n",
            "6. 🧠 Иммунное старение (0 терминов)\n",
            "   Термины не найдены\n",
            "\n",
            "7. 🔀 Пути долголетия (0 терминов)\n",
            "   Термины не найдены\n",
            "\n",
            "8. 📊 Клеточные процессы (10 терминов)\n",
            "   Топ-3 по специализированному скору:\n",
            "   1. phospho-vimentin+/hopx+ cells (связей: 1, DOI: 📚, скор: 44.0)\n",
            "   2. coa1δ cells (связей: 3, DOI: 📚, скор: 42.0)\n",
            "   3. hopx-derived alveolar cells (связей: 3, DOI: 📚, скор: 42.0)\n",
            "\n",
            "0. ✍️  Ввести термин вручную\n",
            "99. 🎲 Случайный специализированный термин\n",
            "\n",
            "👆 Выберите категорию (1-8), 0 для ручного ввода, 99 для случайного: 0\n",
            "✍️ Введите термин: 0\n",
            "\n",
            "🔍 ВАЛИДАЦИЯ ТЕРМИНА '0':\n",
            "Термин '0' не найден в графе\n",
            "\n",
            "❓ Продолжить с этим термином несмотря на низкое качество? (y/N): human pro-longevity\n",
            "❌ Выбор термина отменен\n",
            "\n",
            "🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯\n",
            "ВЫБОР СПЕЦИАЛИЗИРОВАННОГО ТЕРМИНА\n",
            "🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯\n",
            "\n",
            "🎯 СПЕЦИАЛИЗИРОВАННЫЕ КАТЕГОРИИ ПО ВАШИМ КЛЮЧЕВЫМ СЛОВАМ\n",
            "================================================================================\n",
            "\n",
            "1. 🧬 Биомаркеры старения человека (15 терминов)\n",
            "   Топ-3 по специализированному скору:\n",
            "   1. anti-human emilin1 (связей: 6, DOI: 📚, скор: 49.0)\n",
            "   2. human emilin1 (связей: 7, DOI: 📚, скор: 47.3)\n",
            "   3. extracellular protein emilin1 (связей: 2, DOI: 📚, скор: 43.0)\n",
            "\n",
            "2. 🔬 Эпигенетические часы (0 терминов)\n",
            "   Термины не найдены\n",
            "\n",
            "3. ⚡ Воспаление при старении (0 терминов)\n",
            "   Термины не найдены\n",
            "\n",
            "4. 🧪 Гены и белки старения (1 терминов)\n",
            "   Топ-3 по специализированному скору:\n",
            "   1. ecm glycoprotein emilin1 (связей: 8, DOI: 📚, скор: 37.0)\n",
            "\n",
            "5. 🏥 Терапевтические мишени (0 терминов)\n",
            "   Термины не найдены\n",
            "\n",
            "6. 🧠 Иммунное старение (0 терминов)\n",
            "   Термины не найдены\n",
            "\n",
            "7. 🔀 Пути долголетия (0 терминов)\n",
            "   Термины не найдены\n",
            "\n",
            "8. 📊 Клеточные процессы (10 терминов)\n",
            "   Топ-3 по специализированному скору:\n",
            "   1. phospho-vimentin+/hopx+ cells (связей: 1, DOI: 📚, скор: 44.0)\n",
            "   2. coa1δ cells (связей: 3, DOI: 📚, скор: 42.0)\n",
            "   3. hopx-derived alveolar cells (связей: 3, DOI: 📚, скор: 42.0)\n",
            "\n",
            "0. ✍️  Ввести термин вручную\n",
            "99. 🎲 Случайный специализированный термин\n",
            "\n",
            "👆 Выберите категорию (1-8), 0 для ручного ввода, 99 для случайного: 0\n",
            "✍️ Введите термин: human pro-longevity\n",
            "\n",
            "🔍 ВАЛИДАЦИЯ ТЕРМИНА 'human pro-longevity':\n",
            "❌ Термин не рекомендуется для качественного анализа:\n",
            "   ❌ Мало специализированных соседей: 0 < 3\n",
            "   ⚠️ Очень мало связей: 1\n",
            "\n",
            "📊 ХАРАКТЕРИСТИКИ:\n",
            "   🔗 Всего связей: 1\n",
            "   🎯 Специализированных соседей: 0\n",
            "   🔑 Соответствует ключевым словам: ✅\n",
            "   📚 DOI: ✅\n",
            "   📈 Специализированный скор: 47.0\n",
            "\n",
            "❓ Продолжить с этим термином несмотря на низкое качество? (y/N): y\n",
            "⚠️ Термин принят с предупреждениями\n",
            "\n",
            "📋 ДЕТАЛЬНАЯ ИНФОРМАЦИЯ: human pro-longevity\n",
            "============================================================\n",
            "🔗 Степень узла: 1\n",
            "🎯 Специализированный скор: 47.0\n",
            "👥 Соседей: 1\n",
            "📚 DOI: https://doi.org/10.1186/s12929-024-01005-w\n",
            "📄 Заголовок: Hesperetin activates CISD2 to attenuate senescence in human keratinocytes from an older person and r...\n",
            "👨‍🔬 Авторы: Zhao-Qing Shen, Cheng-Yen Chang, Chi-Hsiao Yeh, Chung-Kuang Lu, Hao-Chih Hung, Tai-Wen Wang, Kuan-Sh...\n",
            "📅 Год: 2024\n",
            "📊 Диапазон лет: 2024-2024\n",
            "📚 Всего статей: 1\n",
            "🏷️  Ключевые слова источника: aging, genes, human...\n",
            "📝 Описание: Biomedical entity from: Hesperetin activates CISD2 to attenuate senescence in human keratinocytes from an older person and rejuvenates naturally aged skin in mice...\n",
            "\n",
            "🔗 Топ-5 специализированных соседей:\n",
            "   1. foxo3a (скор: 3.4, связей: 168, DOI: 📚)\n",
            "\n",
            "🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬\n",
            "СПЕЦИАЛИЗИРОВАННЫЙ АНАЛИЗ: HUMAN PRO-LONGEVITY\n",
            "🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬\n",
            "Выберите действие:\n",
            "1. 🔍 Анализ специализированных связей\n",
            "2. 💡 Генерация клинических гипотез\n",
            "3. 📚 Показать метаданные и DOI\n",
            "4. 🔄 Выбрать новый термин\n",
            "5. ❌ Выход\n",
            "\n",
            "👆 Выберите действие (1-5): 2\n",
            "⏳ Генерация конкретных клинических гипотез...\n",
            "🌡️ Выберите температуру (0.1-1.0) [по умолчанию 0.75]: 0.3\n",
            "🌡️ Используем температуру: 0.3\n",
            "💡 Совет: 0.1-0.3=консервативно, 0.4-0.7=сбалансированно, 0.8-1.0=креативно\n",
            "\n",
            "🔍 АНАЛИЗ СПЕЦИАЛИЗИРОВАННЫХ СВЯЗЕЙ: human pro-longevity\n",
            "------------------------------------------------------------\n",
            "🔍 Инициализация для графа: 32,707 узлов, 775,715 рёбер\n",
            "📋 Извлечено 68 уникальных ключевых слов\n",
            "🚀 Построение индексов...\n",
            "🔍 Поиск специализированных терминов по вашим ключевым словам...\n",
            "📚 Построение индекса DOI...\n",
            "🔍 Примеры метаданных для отладки:\n",
            "   muscle biopsies: <class 'dict'>\n",
            "      ключи: ['dois', 'years', 'titles', 'authors', 'keywords']\n",
            "      dois: ['https://doi.org/10.1186/s13072-025-00601-w', nan]\n",
            "   human myoblast: <class 'dict'>\n",
            "      ключи: ['dois', 'years', 'titles', 'authors', 'keywords']\n",
            "      dois: ['https://doi.org/10.1186/s13072-025-00601-w', 'https://doi.org/10.3390/ijms241713181']\n",
            "   tet1-2: <class 'dict'>\n",
            "      ключи: ['dois', 'years', 'titles', 'authors', 'keywords']\n",
            "      dois: ['https://doi.org/10.1186/s13072-025-00601-w']\n",
            "✅ Найдено 23286 узлов с DOI/PMID\n",
            "🔍 Примеры найденных DOI:\n",
            "   muscle biopsies: https://doi.org/10.1186/s13072-025-00601-w\n",
            "   human myoblast: https://doi.org/10.1186/s13072-025-00601-w\n",
            "   tet1-2: https://doi.org/10.1186/s13072-025-00601-w\n",
            "   human cancers [: https://doi.org/10.1186/s13100-024-00339-4\n",
            "   tet2: https://doi.org/10.1038/s42255-023-00855-2\n",
            "✅ Найдено 276 специализированных терминов\n",
            "🔧 Построение специализированных категорий с расширенными ключевыми словами...\n",
            "🎯 Минимальный порог специализированных соседей: 1\n",
            "\n",
            "🔍 ДИАГНОСТИКА С РАСШИРЕННЫМИ КЛЮЧЕВЫМИ СЛОВАМИ:\n",
            "    1. human hopx genome.blue arrowhead    ✅ (всего: 1, спец: 0)\n",
            "    2. human coa1                          ✅ (всего: 3, спец: 0)\n",
            "    3. human col18a1 promoter 2            ✅ (всего: 3, спец: 0)\n",
            "    4. human hopx proteins                 ✅ (всего: 3, спец: 0)\n",
            "    5. anti-human emilin1                  ✅ (всего: 6, спец: 2)\n",
            "    6. high hopx methylation               ✅ (всего: 1, спец: 0)\n",
            "    7. gdf15 human                         ✅ (всего: 6, спец: 0)\n",
            "    8. anti‐human hmgb1 human              ✅ (всего: 6, спец: 0)\n",
            "    9. human hopx protein                  ✅ (всего: 5, спец: 0)\n",
            "   10. human emilin1                       ✅ (всего: 7, спец: 2)\n",
            "   11. human hopx                          ✅ (всего: 7, спец: 0)\n",
            "   12. human pro-longevity                 ✅ (всего: 1, спец: 0)\n",
            "   13. human aging.skin                    ✅ (всего: 1, спец: 0)\n",
            "   14. human col18a1                       ✅ (всего: 8, спец: 0)\n",
            "   15. human longevity.23                  ✅ (всего: 2, спец: 0)\n",
            "   16. human longevity-specific            ✅ (всего: 2, спец: 0)\n",
            "   17. human senescence-associated         ✅ (всего: 1, спец: 0)\n",
            "   18. human senescence-associated β-galactosidase ✅ (всего: 1, спец: 0)\n",
            "   19. human cellular senescence-associated ✅ (всего: 1, спец: 0)\n",
            "   20. coa1/c7orf44/mitrac15               ❌ (всего: 1, спец: 0)\n",
            "\n",
            "📊 СТАТИСТИКА С РАСШИРЕННЫМИ КРИТЕРИЯМИ:\n",
            "   Терминов соответствует расширенным критериям: 178\n",
            "\n",
            "🚫 Отфильтровано 151 из 178 терминов\n",
            "✅ В категории добавлено 26 качественных терминов\n",
            "\n",
            "📊 СТАТИСТИКА ПО КАТЕГОРИЯМ (с расширенными ключевыми словами):\n",
            "   🧬 Биомаркеры старения человека: 15 терминов (средняя степень: 5.0)\n",
            "   🔬 Эпигенетические часы: 0 терминов ⚠️\n",
            "   ⚡ Воспаление при старении: 0 терминов ⚠️\n",
            "   🧪 Гены и белки старения: 1 терминов (средняя степень: 8.0)\n",
            "   🏥 Терапевтические мишени: 0 терминов ⚠️\n",
            "   🧠 Иммунное старение: 0 терминов ⚠️\n",
            "   🔀 Пути долголетия: 0 терминов ⚠️\n",
            "   📊 Клеточные процессы: 10 терминов (средняя степень: 2.3)\n",
            "🎯 Выбрано 0 специализированных соседей из 1\n",
            "\n",
            "💡 ГЕНЕРАЦИЯ СПЕЦИАЛИЗИРОВАННЫХ ГИПОТЕЗ\n",
            "🌡️ Температура: 0.3 (0.1=консервативно, 1.0=креативно)\n",
            "------------------------------------------------------------\n",
            "\n",
            "📚 КОНКРЕТНЫЕ ГИПОТЕЗЫ СТАРЕНИЯ ЧЕЛОВЕКА\n",
            "🌡️ Температура генерации: 0.3\n",
            "================================================================================\n",
            "\n",
            "1. ГИПОТЕЗА:\n",
            "   🧬 Термины: human pro-longevity, эпигенетические изменения, иммунный ответ\n",
            "   🔬 Механизм: Гиперметилирование CpG-сайта cg09809672 в промотере гена CD28 приводит к снижению экспрессии CD28 на CD4+ T-клетках и активации про-сенесцентных сигнальных путей, что может быть обратимо с помощью ингибиторов DNMT1\n",
            "   📚 Обоснование: Метилирование CpG-сайтов в промоторных регионах генов может регулировать их экспрессию. Гиперметилирование CpG-сайта cg09809672 может привести к снижению экспрессии CD28, что является критическим ко-stimуляторным молекулом для активации T-клеток. Это может привести к активации про-сенесцентных сигнальных путей и снижению иммунного ответа\n",
            "   🧬 Эпигенетика: Гиперметилирование CpG-сайта cg09809672, повышение активности DNMT1, снижение гистоновой метки H3K4me3 в промотере CD28\n",
            "   🔥 Воспаление: Повышение IL-6 до 15.2±3.1 пг/мл и TNF-α до 8.7±2.3 пг/мл в плазме, активация NF-κB p65 и IRF3 сигнальных путей\n",
            "   🛡️  Иммунитет: Снижение CD4+CD28+ T-клеток до 12±3% от CD4+, увеличение CD8+CD57+KLRG1+ сенесцентных T-клеток до 23±5% от CD8+\n",
            "   🧪 Эксперимент: In vitro модели: первичные фибробласты человека, HUVEC, PBMC. In vivo модели: мыши C57BL/6. Клинические когорты: 100 участников, возраст 65-80 лет. Конкретные методы: qPCR праймеры для CD28, антитела для Western blot против DNMT1, ELISA киты для IL-6 и TNF-α\n",
            "   📊 Биомаркеры: Уровни метилирования CpG-сайта cg09809672 (% methylation), экспрессия CD28 на CD4+ T-клетках (fold change, ΔCt), концентрации IL-6 и TNF-α в плазме (пг/мл)\n",
            "   🏥 Клиническая валидация: Провести клиническое исследование с 100 участниками, возраст 65-80 лет, для оценки эффективности ингибиторов DNMT1 на экспрессию CD28 и про-сенесцентные сигнальные пути\n",
            "   💊 Терапевтическая мишень: DNMT1\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "2. ГИПОТЕЗА:\n",
            "   🧬 Термины: human pro-longevity, воспаление, иммунный ответ\n",
            "   🔬 Механизм: Повышение экспрессии NLRP3 инфламмасомы в макрофагах приводит к активации про-воспалительных сигнальных путей и снижению иммунного ответа, что может быть обратимо с помощью ингибиторов NLRP3\n",
            "   📚 Обоснование: NLRP3 инфламмасома играет критическую роль в активации про-воспалительных сигнальных путей. Повышение экспрессии NLRP3 может привести к активации про-воспалительных сигнальных путей и снижению иммунного ответа\n",
            "   🧬 Эпигенетика: Гиперметилирование CpG-сайта cg05575921 в промотере гена NLRP3, повышение активности DNMT3A\n",
            "   🔥 Воспаление: Повышение IL-1β до 10.5±2.5 пг/мл и IL-18 до 5.2±1.8 пг/мл в плазме, активация NF-κB p65 и IRF3 сигнальных путей\n",
            "   🛡️  Иммунитет: Снижение CD4+CD28+ T-клеток до 10±2% от CD4+, увеличение CD8+CD57+KLRG1+ сенесцентных T-клеток до 25±5% от CD8+\n",
            "   🧪 Эксперимент: In vitro модели: первичные фибробласты человека, HUVEC, PBMC. In vivo модели: мыши C57BL/6. Клинические когорты: 100 участников, возраст 65-80 лет. Конкретные методы: qPCR праймеры для NLRP3, антитела для Western blot против NLRP3, ELISA киты для IL-1β и IL-18\n",
            "   📊 Биомаркеры: Уровни экспрессии NLRP3 (fold change, ΔCt), концентрации IL-1β и IL-18 в плазме (пг/мл)\n",
            "   🏥 Клиническая валидация: Провести клиническое исследование с 100 участниками, возраст 65-80 лет, для оценки эффективности ингибиторов NLRP3 на про-воспалительные сигнальные пути и иммунный ответ\n",
            "   💊 Терапевтическая мишень: NLRP3\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "3. ГИПОТЕЗА:\n",
            "   🧬 Термины: human pro-longevity, иммунный ответ, эпигенетические изменения\n",
            "   🔬 Механизм: Гиперметилирование CpG-сайта cg09809672 в промотере гена CD28 приводит к снижению экспрессии CD28 на CD4+ T-клетках и активации про-сенесцентных сигнальных путей, что может быть обратимо с помощью ингибиторов DNMT1\n",
            "   📚 Обоснование: Метилирование CpG-сайтов в промоторных регионах генов может регулировать их экспрессию. Гиперметилирование CpG-сайта cg09809672 может привести к снижению экспрессии CD28, что является критическим ко-stimуляторным молекулом для активации T-клеток. Это может привести к активации про-сенесцентных сигнальных путей и снижению иммунного ответа\n",
            "   🧬 Эпигенетика: Гиперметилирование CpG-сайта cg09809672, повышение активности DNMT1, снижение гистоновой метки H3K4me3 в промотере CD28\n",
            "   🔥 Воспаление: Повышение IL-6 до 15.2±3.1 пг/мл и TNF-α до 8.7±2.3 пг/мл в плазме, активация NF-κB p65 и IRF3 сигнальных путей\n",
            "   🛡️  Иммунитет: Снижение CD4+CD28+ T-клеток до 12±3% от CD4+, увеличение CD8+CD57+KLRG1+ сенесцентных T-клеток до 23±5% от CD8+\n",
            "   🧪 Эксперимент: In vitro модели: первичные фибробласты человека, HUVEC, PBMC. In vivo модели: мыши C57BL/6. Клинические когорты: 100 участников, возраст 65-80 лет. Конкретные методы: qPCR праймеры для CD28, антитела для Western blot против DNMT1, ELISA киты для IL-6 и TNF-α\n",
            "   📊 Биомаркеры: Уровни метилирования CpG-сайта cg09809672 (% methylation), экспрессия CD28 на CD4+ T-клетках (fold change, ΔCt), концентрации IL-6 и TNF-α в плазме (пг/мл)\n",
            "   🏥 Клиническая валидация: Провести клиническое исследование с 100 участниками, возраст 65-80 лет, для оценки эффективности ингибиторов DNMT1 на экспрессию CD28 и про-сенесцентные сигнальные пути\n",
            "   💊 Терапевтическая мишень: DNMT1\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "4. ГИПОТЕЗА:\n",
            "   🧬 Термины: human pro-longevity, воспаление, иммунный ответ\n",
            "   🔬 Механизм: Повышение экспрессии TLR4 в макрофагах приводит к активации про-воспалительных сигнальных путей и снижению иммунного ответа, что может быть обратимо с помощью ингибиторов TLR4\n",
            "   📚 Обоснование: TLR4 играет критическую роль в активации про-воспалительных сигнальных путей. Повышение экспрессии TLR4 может привести к активации про-воспалительных сигнальных путей и снижению иммунного ответа\n",
            "   🧬 Эпигенетика: Гиперметилирование CpG-сайта cg05575921 в промотере гена TLR4, повышение активности DNMT3A\n",
            "   🔥 Воспаление: Повышение IL-1β до 10.5±2.5 пг/мл и IL-18 до 5.2±1.8 пг/мл в плазме, активация NF-κB p65 и IRF3 сигнальных путей\n",
            "   🛡️  Иммунитет: Снижение CD4+CD28+ T-клеток до 10±2% от CD4+, увеличение CD8+CD57+KLRG1+ сенесцентных T-клеток до 25±5% от CD8+\n",
            "   🧪 Эксперимент: In vitro модели: первичные фибробласты человека, HUVEC, PBMC. In vivo модели: мыши C57BL/6. Клинические когорты: 100 участников, возраст 65-80 лет. Конкретные методы: qPCR праймеры для TLR4, антитела для Western blot против TLR4, ELISA киты для IL-1β и IL-18\n",
            "   📊 Биомаркеры: Уровни экспрессии TLR4 (fold change, ΔCt), концентрации IL-1β и IL-18 в плазме (пг/мл)\n",
            "   🏥 Клиническая валидация: Провести клиническое исследование с 100 участниками, возраст 65-80 лет, для оценки эффективности ингибиторов TLR4 на про-воспалительные сигнальные пути и иммунный ответ\n",
            "   💊 Терапевтическая мишень: TLR4\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "5. ГИПОТЕЗА:\n",
            "   🧬 Термины: human pro-longevity, иммунный ответ, эпигенетические изменения\n",
            "   🔬 Механизм: Гиперметилирование CpG-сайта cg09809672 в промотере гена CD28 приводит к снижению экспрессии CD28 на CD4+ T-клетках и активации про-сенесцентных сигнальных путей, что может быть обратимо с помощью ингибиторов DNMT1\n",
            "   📚 Обоснование: Метилирование CpG-сайтов в промоторных регионах генов может регулировать их экспрессию. Гиперметилирование CpG-сайта cg09809672 может привести к снижению экспрессии CD28, что является критическим ко-stimуляторным молекулом для активации T-клеток. Это может привести к активации про-сенесцентных сигнальных путей и снижению иммунного ответа\n",
            "   🧬 Эпигенетика: Гиперметилирование CpG-сайта cg09809672, повышение активности DNMT1, снижение гистоновой метки H3K4me3 в промотере CD28\n",
            "   🔥 Воспаление: Повышение IL-6 до 15.2±3.1 пг/мл и TNF-α до 8.7±2.3 пг/мл в плазме, активация NF-κB p65 и IRF3 сигнальных путей\n",
            "   🛡️  Иммунитет: Снижение CD4+CD28+ T-клеток до 12±3% от CD4+, увеличение CD8+CD57+KLRG1+ сенесцентных T-клеток до 23±5% от CD8+\n",
            "   🧪 Эксперимент: In vitro модели: первичные фибробласты человека, HUVEC, PBMC. In vivo модели: мыши C57BL/6. Клинические когорты: 100 участников, возраст 65-80 лет. Конкретные методы: qPCR праймеры для CD28, антитела для Western blot против DNMT1, ELISA киты для IL-6 и TNF-α\n",
            "   📊 Биомаркеры: Уровни метилирования CpG-сайта cg09809672 (% methylation), экспрессия CD28 на CD4+ T-клетках (fold change, ΔCt), концентрации IL-6 и TNF-α в плазме (пг/мл)\n",
            "   🏥 Клиническая валидация: Провести клиническое исследование с 100 участниками, возраст 65-80 лет, для оценки эффективности ингибиторов DNMT1 на экспрессию CD28 и про-сенесцентные сигнальные пути\n",
            "   💊 Терапевтическая мишень: DNMT1\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬\n",
            "СПЕЦИАЛИЗИРОВАННЫЙ АНАЛИЗ: HUMAN PRO-LONGEVITY\n",
            "🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬\n",
            "Выберите действие:\n",
            "1. 🔍 Анализ специализированных связей\n",
            "2. 💡 Генерация клинических гипотез\n",
            "3. 📚 Показать метаданные и DOI\n",
            "4. 🔄 Выбрать новый термин\n",
            "5. ❌ Выход\n",
            "\n",
            "👆 Выберите действие (1-5): 4\n",
            "\n",
            "🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯\n",
            "ВЫБОР СПЕЦИАЛИЗИРОВАННОГО ТЕРМИНА\n",
            "🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯\n",
            "\n",
            "🎯 СПЕЦИАЛИЗИРОВАННЫЕ КАТЕГОРИИ ПО ВАШИМ КЛЮЧЕВЫМ СЛОВАМ\n",
            "================================================================================\n",
            "\n",
            "1. 🧬 Биомаркеры старения человека (15 терминов)\n",
            "   Топ-3 по специализированному скору:\n",
            "   1. anti-human emilin1 (связей: 6, DOI: 📚, скор: 49.0)\n",
            "   2. human emilin1 (связей: 7, DOI: 📚, скор: 47.3)\n",
            "   3. extracellular protein emilin1 (связей: 2, DOI: 📚, скор: 43.0)\n",
            "\n",
            "2. 🔬 Эпигенетические часы (0 терминов)\n",
            "   Термины не найдены\n",
            "\n",
            "3. ⚡ Воспаление при старении (0 терминов)\n",
            "   Термины не найдены\n",
            "\n",
            "4. 🧪 Гены и белки старения (1 терминов)\n",
            "   Топ-3 по специализированному скору:\n",
            "   1. ecm glycoprotein emilin1 (связей: 8, DOI: 📚, скор: 37.0)\n",
            "\n",
            "5. 🏥 Терапевтические мишени (0 терминов)\n",
            "   Термины не найдены\n",
            "\n",
            "6. 🧠 Иммунное старение (0 терминов)\n",
            "   Термины не найдены\n",
            "\n",
            "7. 🔀 Пути долголетия (0 терминов)\n",
            "   Термины не найдены\n",
            "\n",
            "8. 📊 Клеточные процессы (10 терминов)\n",
            "   Топ-3 по специализированному скору:\n",
            "   1. phospho-vimentin+/hopx+ cells (связей: 1, DOI: 📚, скор: 44.0)\n",
            "   2. coa1δ cells (связей: 3, DOI: 📚, скор: 42.0)\n",
            "   3. hopx-derived alveolar cells (связей: 3, DOI: 📚, скор: 42.0)\n",
            "\n",
            "0. ✍️  Ввести термин вручную\n",
            "99. 🎲 Случайный специализированный термин\n",
            "\n",
            "👆 Выберите категорию (1-8), 0 для ручного ввода, 99 для случайного: 0\n",
            "✍️ Введите термин: human longevity-specific\n",
            "\n",
            "🔍 ВАЛИДАЦИЯ ТЕРМИНА 'human longevity-specific':\n",
            "❌ Термин не рекомендуется для качественного анализа:\n",
            "   ❌ Мало специализированных соседей: 0 < 3\n",
            "\n",
            "📊 ХАРАКТЕРИСТИКИ:\n",
            "   🔗 Всего связей: 2\n",
            "   🎯 Специализированных соседей: 0\n",
            "   🔑 Соответствует ключевым словам: ✅\n",
            "   📚 DOI: ✅\n",
            "   📈 Специализированный скор: 46.0\n",
            "\n",
            "❓ Продолжить с этим термином несмотря на низкое качество? (y/N): y\n",
            "⚠️ Термин принят с предупреждениями\n",
            "\n",
            "📋 ДЕТАЛЬНАЯ ИНФОРМАЦИЯ: human longevity-specific\n",
            "============================================================\n",
            "🔗 Степень узла: 2\n",
            "🎯 Специализированный скор: 46.0\n",
            "👥 Соседей: 2\n",
            "📚 DOI: https://doi.org/10.18502/ijph.v50i1.5082\n",
            "📄 Заголовок: Specific Differentially Methylated and Expressed Genes in People with Longevity Family History....\n",
            "👨‍🔬 Авторы: Chunhong Li, Qingqing Nong, Bin Guan, Haoyu He, Zhiyong Zhang...\n",
            "📅 Год: 2021\n",
            "📊 Диапазон лет: 2021-2021\n",
            "📚 Всего статей: 1\n",
            "🏷️  Ключевые слова источника: longevity, pathways, human...\n",
            "📝 Описание: Biomedical entity from: Specific Differentially Methylated and Expressed Genes in People with Longevity Family History....\n",
            "\n",
            "🔗 Топ-5 специализированных соседей:\n",
            "   1. people (скор: 2.9, связей: 311, DOI: 📚)\n",
            "   2. cellular (скор: 0.9, связей: 3894, DOI: 📚)\n",
            "\n",
            "🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬\n",
            "СПЕЦИАЛИЗИРОВАННЫЙ АНАЛИЗ: HUMAN LONGEVITY-SPECIFIC\n",
            "🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬\n",
            "Выберите действие:\n",
            "1. 🔍 Анализ специализированных связей\n",
            "2. 💡 Генерация клинических гипотез\n",
            "3. 📚 Показать метаданные и DOI\n",
            "4. 🔄 Выбрать новый термин\n",
            "5. ❌ Выход\n",
            "\n",
            "👆 Выберите действие (1-5): 2\n",
            "⏳ Генерация конкретных клинических гипотез...\n",
            "🌡️ Выберите температуру (0.1-1.0) [по умолчанию 0.75]: 0.4\n",
            "🌡️ Используем температуру: 0.4\n",
            "💡 Совет: 0.1-0.3=консервативно, 0.4-0.7=сбалансированно, 0.8-1.0=креативно\n",
            "\n",
            "🔍 АНАЛИЗ СПЕЦИАЛИЗИРОВАННЫХ СВЯЗЕЙ: human longevity-specific\n",
            "------------------------------------------------------------\n",
            "🔍 Инициализация для графа: 32,707 узлов, 775,715 рёбер\n",
            "📋 Извлечено 68 уникальных ключевых слов\n",
            "🚀 Построение индексов...\n",
            "🔍 Поиск специализированных терминов по вашим ключевым словам...\n",
            "📚 Построение индекса DOI...\n",
            "🔍 Примеры метаданных для отладки:\n",
            "   muscle biopsies: <class 'dict'>\n",
            "      ключи: ['dois', 'years', 'titles', 'authors', 'keywords']\n",
            "      dois: ['https://doi.org/10.1186/s13072-025-00601-w', nan]\n",
            "   human myoblast: <class 'dict'>\n",
            "      ключи: ['dois', 'years', 'titles', 'authors', 'keywords']\n",
            "      dois: ['https://doi.org/10.1186/s13072-025-00601-w', 'https://doi.org/10.3390/ijms241713181']\n",
            "   tet1-2: <class 'dict'>\n",
            "      ключи: ['dois', 'years', 'titles', 'authors', 'keywords']\n",
            "      dois: ['https://doi.org/10.1186/s13072-025-00601-w']\n",
            "✅ Найдено 23286 узлов с DOI/PMID\n",
            "🔍 Примеры найденных DOI:\n",
            "   muscle biopsies: https://doi.org/10.1186/s13072-025-00601-w\n",
            "   human myoblast: https://doi.org/10.1186/s13072-025-00601-w\n",
            "   tet1-2: https://doi.org/10.1186/s13072-025-00601-w\n",
            "   human cancers [: https://doi.org/10.1186/s13100-024-00339-4\n",
            "   tet2: https://doi.org/10.1038/s42255-023-00855-2\n",
            "✅ Найдено 276 специализированных терминов\n",
            "🔧 Построение специализированных категорий с расширенными ключевыми словами...\n",
            "🎯 Минимальный порог специализированных соседей: 1\n",
            "\n",
            "🔍 ДИАГНОСТИКА С РАСШИРЕННЫМИ КЛЮЧЕВЫМИ СЛОВАМИ:\n",
            "    1. human hopx genome.blue arrowhead    ✅ (всего: 1, спец: 0)\n",
            "    2. human coa1                          ✅ (всего: 3, спец: 0)\n",
            "    3. human col18a1 promoter 2            ✅ (всего: 3, спец: 0)\n",
            "    4. human hopx proteins                 ✅ (всего: 3, спец: 0)\n",
            "    5. anti-human emilin1                  ✅ (всего: 6, спец: 2)\n",
            "    6. high hopx methylation               ✅ (всего: 1, спец: 0)\n",
            "    7. gdf15 human                         ✅ (всего: 6, спец: 0)\n",
            "    8. anti‐human hmgb1 human              ✅ (всего: 6, спец: 0)\n",
            "    9. human hopx protein                  ✅ (всего: 5, спец: 0)\n",
            "   10. human emilin1                       ✅ (всего: 7, спец: 2)\n",
            "   11. human hopx                          ✅ (всего: 7, спец: 0)\n",
            "   12. human pro-longevity                 ✅ (всего: 1, спец: 0)\n",
            "   13. human aging.skin                    ✅ (всего: 1, спец: 0)\n",
            "   14. human col18a1                       ✅ (всего: 8, спец: 0)\n",
            "   15. human longevity.23                  ✅ (всего: 2, спец: 0)\n",
            "   16. human longevity-specific            ✅ (всего: 2, спец: 0)\n",
            "   17. human senescence-associated         ✅ (всего: 1, спец: 0)\n",
            "   18. human senescence-associated β-galactosidase ✅ (всего: 1, спец: 0)\n",
            "   19. human cellular senescence-associated ✅ (всего: 1, спец: 0)\n",
            "   20. coa1/c7orf44/mitrac15               ❌ (всего: 1, спец: 0)\n",
            "\n",
            "📊 СТАТИСТИКА С РАСШИРЕННЫМИ КРИТЕРИЯМИ:\n",
            "   Терминов соответствует расширенным критериям: 178\n",
            "\n",
            "🚫 Отфильтровано 151 из 178 терминов\n",
            "✅ В категории добавлено 26 качественных терминов\n",
            "\n",
            "📊 СТАТИСТИКА ПО КАТЕГОРИЯМ (с расширенными ключевыми словами):\n",
            "   🧬 Биомаркеры старения человека: 15 терминов (средняя степень: 5.0)\n",
            "   🔬 Эпигенетические часы: 0 терминов ⚠️\n",
            "   ⚡ Воспаление при старении: 0 терминов ⚠️\n",
            "   🧪 Гены и белки старения: 1 терминов (средняя степень: 8.0)\n",
            "   🏥 Терапевтические мишени: 0 терминов ⚠️\n",
            "   🧠 Иммунное старение: 0 терминов ⚠️\n",
            "   🔀 Пути долголетия: 0 терминов ⚠️\n",
            "   📊 Клеточные процессы: 10 терминов (средняя степень: 2.3)\n",
            "🎯 Выбрано 0 специализированных соседей из 2\n",
            "\n",
            "💡 ГЕНЕРАЦИЯ СПЕЦИАЛИЗИРОВАННЫХ ГИПОТЕЗ\n",
            "🌡️ Температура: 0.4 (0.1=консервативно, 1.0=креативно)\n",
            "------------------------------------------------------------\n",
            "\n",
            "📚 КОНКРЕТНЫЕ ГИПОТЕЗЫ СТАРЕНИЯ ЧЕЛОВЕКА\n",
            "🌡️ Температура генерации: 0.4\n",
            "================================================================================\n",
            "\n",
            "1. ГИПОТЕЗА:\n",
            "   🧬 Термины: human longevity-specific, эпигенетические изменения, метилирование ДНК\n",
            "   🔬 Механизм: Гиперметилирование CpG-сайта cg09809672 в промотере гена SIRT1 является ключевым фактором в регуляции человеческой долголетия\n",
            "   📚 Обоснование: SIRT1 играет важную роль в регуляции клеточного старения и метаболизма, а гиперметилирование его промотера может привести к снижению его экспрессии\n",
            "   🧬 Эпигенетика: Гиперметилирование CpG-сайта cg09809672, снижение гистоновой метки H3K4me3 и увеличение DNMT1 и DNMT3B\n",
            "   🔥 Воспаление: Повышение IL-6 до 15.2±3.1 пг/мл и TNF-α до 8.7±2.3 пг/мл в плазме, активация NF-κB p65 и IRF3\n",
            "   🛡️  Иммунитет: Увеличение CD8+CD57+KLRG1+ сенесцентных T-клеток до 23±5% от CD8+, снижение Tregs CD4+CD25+FOXP3+ до 5±2% от CD4+\n",
            "   🧪 Эксперимент: In vitro модели: первичные фибробласты человека, HUVEC, PBMC; in vivo модели: мыши C57BL/6; клинические когорты: 100 участников 65-85 лет, размер выборки 50\n",
            "   📊 Биомаркеры: Уровни SIRT1 в плазме (нг/мл), метилирование CpG-сайта cg09809672 (%), IL-6 и TNF-α в плазме (пг/мл), CD8+CD57+KLRG1+ T-клетки (% от CD8+)\n",
            "   🏥 Клиническая валидация: Проверка у человека: измерение уровней SIRT1, IL-6 и TNF-α в плазме, анализ метилирования CpG-сайта cg09809672 и иммунных клеток в крови\n",
            "   💊 Терапевтическая мишень: SIRT1, DNMT1 и DNMT3B\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "2. ГИПОТЕЗА:\n",
            "   🧬 Термины: human longevity-specific, воспаление, иммунный ответ\n",
            "   🔬 Механизм: Активация NLRP3 инфламмасома является ключевым фактором в развитии хронического воспаления и снижении человеческой долголетия\n",
            "   📚 Обоснование: NLRP3 инфламмасома играет важную роль в регуляции воспаления и иммунного ответа, а его активация может привести к хроническому воспалению и снижению долголетия\n",
            "   🧬 Эпигенетика: Гиперметилирование CpG-сайта cg05575921 в промотере гена NLRP3, снижение гистоновой метки H3K4me3 и увеличение DNMT1 и DNMT3B\n",
            "   🔥 Воспаление: Повышение IL-1β до 10.5±2.5 пг/мл и IL-18 до 5.2±1.8 пг/мл в плазме, активация NF-κB p65 и IRF3\n",
            "   🛡️  Иммунитет: Увеличение CD4+CD28- сенесцентных T-клеток до 18±4% от CD4+, снижение Tregs CD4+CD25+FOXP3+ до 4±2% от CD4+\n",
            "   🧪 Эксперимент: In vitro модели: первичные фибробласты человека, HUVEC, PBMC; in vivo модели: мыши C57BL/6; клинические когорты: 100 участников 65-85 лет, размер выборки 50\n",
            "   📊 Биомаркеры: Уровни NLRP3 в плазме (нг/мл), метилирование CpG-сайта cg05575921 (%), IL-1β и IL-18 в плазме (пг/мл), CD4+CD28- T-клетки (% от CD4+)\n",
            "   🏥 Клиническая валидация: Проверка у человека: измерение уровней NLRP3, IL-1β и IL-18 в плазме, анализ метилирования CpG-сайта cg05575921 и иммунных клеток в крови\n",
            "   💊 Терапевтическая мишень: NLRP3, DNMT1 и DNMT3B\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "3. ГИПОТЕЗА:\n",
            "   🧬 Термины: human longevity-specific, иммунный ответ, сенесцентные клетки\n",
            "   🔬 Механизм: Увеличение сенесцентных T-клеток CD8+CD57+KLRG1+ является ключевым фактором в снижении человеческой долголетия\n",
            "   📚 Обоснование: Сенесцентные T-клетки играют важную роль в регуляции иммунного ответа и могут способствовать снижению долголетия\n",
            "   🧬 Эпигенетика: Гиперметилирование CpG-сайта cg09809672 в промотере гена CDKN2A, снижение гистоновой метки H3K4me3 и увеличение DNMT1 и DNMT3B\n",
            "   🔥 Воспаление: Повышение IL-6 до 15.2±3.1 пг/мл и TNF-α до 8.7±2.3 пг/мл в плазме, активация NF-κB p65 и IRF3\n",
            "   🛡️  Иммунитет: Увеличение CD8+CD57+KLRG1+ сенесцентных T-клеток до 23±5% от CD8+, снижение Tregs CD4+CD25+FOXP3+ до 5±2% от CD4+\n",
            "   🧪 Эксперимент: In vitro модели: первичные фибробласты человека, HUVEC, PBMC; in vivo модели: мыши C57BL/6; клинические когорты: 100 участников 65-85 лет, размер выборки 50\n",
            "   📊 Биомаркеры: Уровни CD8+CD57+KLRG1+ T-клеток (% от CD8+), метилирование CpG-сайта cg09809672 (%), IL-6 и TNF-α в плазме (пг/мл)\n",
            "   🏥 Клиническая валидация: Проверка у человека: измерение уровней CD8+CD57+KLRG1+ T-клеток, анализ метилирования CpG-сайта cg09809672 и иммунных клеток в крови\n",
            "   💊 Терапевтическая мишень: CD8+CD57+KLRG1+ T-клетки, DNMT1 и DNMT3B\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "4. ГИПОТЕЗА:\n",
            "   🧬 Термины: human longevity-specific, эпигенетические изменения, метилирование ДНК\n",
            "   🔬 Механизм: Гиперметилирование CpG-сайта cg05575921 в промотере гена TERT является ключевым фактором в регуляции человеческой долголетия\n",
            "   📚 Обоснование: TERT играет важную роль в регуляции теломерного старения и метаболизма, а гиперметилирование его промотера может привести к снижению его экспрессии\n",
            "   🧬 Эпигенетика: Гиперметилирование CpG-сайта cg05575921, снижение гистоновой метки H3K4me3 и увеличение DNMT1 и DNMT3B\n",
            "   🔥 Воспаление: Повышение IL-6 до 15.2±3.1 пг/мл и TNF-α до 8.7±2.3 пг/мл в плазме, активация NF-κB p65 и IRF3\n",
            "   🛡️  Иммунитет: Увеличение CD4+CD28- сенесцентных T-клеток до 18±4% от CD4+, снижение Tregs CD4+CD25+FOXP3+ до 4±2% от CD4+\n",
            "   🧪 Эксперимент: In vitro модели: первичные фибробласты человека, HUVEC, PBMC; in vivo модели: мыши C57BL/6; клинические когорты: 100 участников 65-85 лет, размер выборки 50\n",
            "   📊 Биомаркеры: Уровни TERT в плазме (нг/мл), метилирование CpG-сайта cg05575921 (%), IL-6 и TNF-α в плазме (пг/мл), CD4+CD28- T-клетки (% от CD4+)\n",
            "   🏥 Клиническая валидация: Проверка у человека: измерение уровней TERT, IL-6 и TNF-α в плазме, анализ метилирования CpG-сайта cg05575921 и иммунных клеток в крови\n",
            "   💊 Терапевтическая мишень: TERT, DNMT1 и DNMT3B\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "5. ГИПОТЕЗА:\n",
            "   🧬 Термины: human longevity-specific, иммунный ответ, сенесцентные клетки\n",
            "   🔬 Механизм: Увеличение сенесцентных B-клеток CD19+CD27-CD43+ является ключевым фактором в снижении человеческой долголетия\n",
            "   📚 Обоснование: Сенесцентные B-клетки играют важную роль в регуляции иммунного ответа и могут способствовать снижению долголетия\n",
            "   🧬 Эпигенетика: Гиперметилирование CpG-сайта cg09809672 в промотере гена CDKN2A, снижение гистоновой метки H3K4me3 и увеличение DNMT1 и DNMT3B\n",
            "   🔥 Воспаление: Повышение IL-6 до 15.2±3.1 пг/мл и TNF-α до 8.7±2.3 пг/мл в плазме, активация NF-κB p65 и IRF3\n",
            "   🛡️  Иммунитет: Увеличение CD19+CD27-CD43+ сенесцентных B-клеток до 20±5% от CD19+, снижение Tregs CD4+CD25+FOXP3+ до 5±2% от CD4+\n",
            "   🧪 Эксперимент: In vitro модели: первичные фибробласты человека, HUVEC, PBMC; in vivo модели: мыши C57BL/6; клинические когорты: 100 участников 65-85 лет, размер выборки 50\n",
            "   📊 Биомаркеры: Уровни CD19+CD27-CD43+ B-клеток (% от CD19+), метилирование CpG-сайта cg09809672 (%), IL-6 и TNF-α в плазме (пг/мл)\n",
            "   🏥 Клиническая валидация: Проверка у человека: измерение уровней CD19+CD27-CD43+ B-клеток, анализ метилирования CpG-сайта cg09809672 и иммунных клеток в крови\n",
            "   💊 Терапевтическая мишень: CD19+CD27-CD43+ B-клетки, DNMT1 и DNMT3B\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬\n",
            "СПЕЦИАЛИЗИРОВАННЫЙ АНАЛИЗ: HUMAN LONGEVITY-SPECIFIC\n",
            "🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬\n",
            "Выберите действие:\n",
            "1. 🔍 Анализ специализированных связей\n",
            "2. 💡 Генерация клинических гипотез\n",
            "3. 📚 Показать метаданные и DOI\n",
            "4. 🔄 Выбрать новый термин\n",
            "5. ❌ Выход\n",
            "\n",
            "👆 Выберите действие (1-5): 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-qwf0ax1LDb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZG-GJ6UQZf6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zle-SPafQZkT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KV7sXxNWQZox"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tk9Izyb8SNCu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ya6vtBZYQZt1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UK5evI-LQZya"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFRlraaHQZ4B"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxxppZ32QZ8e"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oxn6uZr9QaAx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_A5rC9MQaFI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1LsNJ8L7cZl"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}